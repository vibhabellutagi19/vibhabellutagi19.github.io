"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[1472],{2968:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>p});var a=n(3493),i=n(4848),t=n(8453);n(4512);const o={slug:"columns-and-expressions",title:"Columns and Expressions",authors:["me"],tags:["de","apache-spark"],keywords:["data engineering","apache spark","columns and expressions"],hide_table_of_contents:!1,image:"assets/col_n_exps.png"},l=void 0,r={authorsImageUrls:[void 0]},p=[];function c(e){const s={code:"code",p:"p",...(0,t.R)(),...e.components};return(0,i.jsxs)(s.p,{children:["Apache Spark's ",(0,i.jsx)(s.code,{children:"Column and Expression"})," play a big\u2002role in making your pipeline more efficient. In this\u2002blog we will look into ALL the possible ways to select columns, use built-in functions and perform calculations with column objects and expressions in PySpark. So, whether you build an ETL pipeline or doing exploratory data analysis, these techniques methods will come in handy."]})}function u(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},3493:e=>{e.exports=JSON.parse('{"permalink":"/blog/columns-and-expressions","source":"@site/blog/2025-01-10-columns-and-experssions/index.md","title":"Columns and Expressions","description":"Apache Spark\'s Column and Expression play a big\u2002role in making your pipeline more efficient. In this\u2002blog we will look into ALL the possible ways to select columns, use built-in functions and perform calculations with column objects and expressions in PySpark. So, whether you build an ETL pipeline or doing exploratory data analysis, these techniques methods will come in handy.","date":"2025-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":3.8,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"columns-and-expressions","title":"Columns and Expressions","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","columns and expressions"],"hide_table_of_contents":false,"image":"assets/col_n_exps.png"},"unlisted":false,"prevItem":{"title":"Handling Nulls in Spark","permalink":"/blog/handling-nulls-in-spark"},"nextItem":{"title":"Introduction to Apache Spark","permalink":"/blog/spark-basics"}}')}}]);