"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"de-bootcamp-dimensional-modelling-scd","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling-scd","source":"@site/blog/2024-12-01-de-bootcamp-dimensional-modelling-scd/index.mdx","title":"Dimensional Modelling - SCD","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.","date":"2024-12-01T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"slowly-changing-dimension","permalink":"/blog/tags/scd","description":"Slowly Changing Dimension tag description"}],"readingTime":5.085,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling-scd","title":"Dimensional Modelling - SCD","authors":["me"],"tags":["de","dim","de-bootcamp","scd"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","slowly changing dimensions"],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-2: Dimensional Modelling - Slowly Changing Dimensions (Theory)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is an Idempotent Pipeline?\\n> Idempotency: An idempotent pipeline ensures that processing the same data multiple times (whether it\'s live production data or backfill data) yields the same result without any unintended side effects. This consistency is crucial for reliability and simplicity in data workflows.\\n\\nPipelines should produce the same results:\\n1. The day they are run\\n2. How many times they are run\\n3. What hour they are run\\n\\n### Why is hard to troubleshoot non-idempotent pipelines?\\n1. Silent Failure: The non-idempotent pipelines can produce different results every time they are run, it doesnt fail. This results in <Highlight color=\\"#3e6980\\">non reproduceable results</Highlight>.\\n2. This causes inconsistency in the data to the downstream systems too.\\n\\n### What can make a pipline non-idempotent?\\n1. `INSERT INTO` without `TRUNCATE`:\\n    1. If you dont `TRUNCATE` data before INSERT, it will keep on adding the data to the table, creating duplicates. \\n    2. Thus your pipeline will not be idempotent, as it will keep on adding the data to the table.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">MERGE</Highlight> or <Highlight color=\\"#3e6980\\">INSERT OVERWRITE</Highlight> always.\\n2. Using `start_date >` without a corresponding `end_date <`:\\n    1. If you dont have `end_date` in your SCD, you will not be able to track the changes in the data.\\n    2. This can create a out of memory exceptions, when you backfill the data.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">end_date</Highlight> in your SCD always.\\n3. Not using a full set of partition sensors:\\n    1. Your pipeline may run with partial set of inputs.\\n    2. pipeline might run when there is no/partial data\\n4. Not using `depends_on_past` for cumulative pipelines:\\n    1. When you process the data in parallel ( lets say processing parallel days ), pipeline may end up processing yesterday\'s data, which hasnt been created yet.\\n\\n:::info\\nThe beauty of an idempotent pipeline lies in its ability to produce consistent results for both production and backfill data.\\n:::\\n\\n**Example:**\\nImagine a pipeline that:\\n1. Reads data (e.g., transactions).\\n2. Cleans and deduplicates it.\\n3. Stores the cleaned data in a database.\\n\\n- **Production:** As new transactions come in, they\u2019re cleaned, deduplicated, and stored.\\n- **Backfill:** When historical transactions are added, they go through the same cleaning and deduplication process, ensuring no duplicate records and consistent results.\\n\\nBecause the pipeline is idempotent:\\n- Processing the same data again won\u2019t alter the results or create issues.\\n- Whether it\'s production or backfill data, the behavior remains identical.\\n\\n### Few more problems that can make a pipeline non-idempotent\\n\\n1. Relying on the **latest** partition of a not properly modeled SCD table.\\n    - Cumulative table design AMPLIFIES this bug.\\n2. One exception relying on latest partition is when you have properly modeled SCD table and <Highlight color=\\"#3e8045\\">you are backfilling not in production</Highlight>.\\n\\n### The pains of not having an idempotent pipeline\\n\\n- Backfilling causes inconsistencies between the old and restated data.\\n- Hard to troubleshoot bugs and fix the issues.\\n- Unit testing cannot replicate the production behavior.\\n- The pipeline doesnt fail, but the data is inconsistent, which is silent failures.\\n\\n## Slowly Changing Dimensions (SCD)\\n\\nThe Slowly Changing Dimensions (SCD) are dimensions that change slowly over time, which would have time frame with them. For example,\\n    1. Customer Address\\n    2. Product Description\\n    3. Employee Details\\n    4. etc.\\n\\n:::warning\\nNot modelling the SCDs properly can impact the idempotency.\\n:::\\n\\n:::tip\\nThe slower the changes in a Slowly Changing Dimension (SCD), the more effectively it can be modeled.\\n:::\\n\\n### How can you model dimensions that change over time?\\n\\n- Singular snapshot: The dimension table only contains the most recent data.\\n    - Be Careful: This pipelines are not idempotent.\\n- Daily partitioned snapshots: The dimension table contains a snapshot of the data for each day.\\n    - Very simple way of implementing SCD.\\n- SCD Types 1,2,3\\n\\n### Types of Slowly Changing Dimensions\\n\\nHere\u2019s a Markdown table summarizing the content provided:\\n\\n| **Type**  | **Description**                                                                                             | **Key Features**                                                                                      | **Limitations**                                                                                   |\\n|-----------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\\n| **Type 0** | Static dimensions, not actually slowly changing.                                                           | - No temporal columns. <br /> - Table contains only identifier and value.                              | - No changes tracked or maintained.                                                              |\\n| **Type 1** | Overwrites old data with new data, only latest data is retained.                                           | - Simple and efficient. <br /> - No history maintained.                                                | - Cannot retrieve historical data.                                                               |\\n| **Type 2** | Gold standard of Slowly Changing Dimensions, maintains history using temporal columns.                     | - Uses `start_date` and `end_date`. <br /> - Current rows have `end_date = NULL` or `9999-12-31`. <br /> - Optionally uses `is_current` column. | - More than one row per dimension can complicate filtering and querying.                         |\\n| **Type 3** | Maintains only two versions of the data: `original` and `current`.                                         | - One row per dimension.                                                                             | - Loses history of changes between `original` and `current`. <br /> - Partially idempotent.        |\\n\\n\\n### Which types are idempotent?\\n\\n| Type      | Idempotent | Reason                                                                                           |\\n|-----------|------------|--------------------------------------------------------------------------------------------------|\\n| Type 0 | Yes        | The values are unchanging.                                                                       |\\n| Type 2 | Yes        | It is idempotent, but you need to be careful with how you use the `start_date` and `end_date`.   |\\n| Type 1 | No         | Backfilling with this dataset gives the dimension as it is now, not as it was before.           |\\n| Type 3 | No         | Backfilling makes it impossible to determine whether to pick \\"original\\" or \\"current.\\"           |\\n\\n### Loading SCD2\\n\\n- Load the entire history in one query\\n    - Inefficient but nimble\\n- Incremnetally load the data after the previoud SCD is generated\\n    - Has the same `depends_on_past`. its efficient but cumbersome."},{"id":"de-bootcamp-dimensional-modelling","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling","source":"@site/blog/2024-11-29-de-bootcamp-dimensional-modelling/index.md","title":"Dimensional Modelling - Cumulative Table","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Cumulative Table.","date":"2024-11-29T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"cumulative-table","permalink":"/blog/tags/cumulativetable","description":"Cumulative Table tag description"}],"readingTime":8.82,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling","title":"Dimensional Modelling - Cumulative Table","authors":["me"],"tags":["de","dim","de-bootcamp","cumulativetable"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","cumulative table"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome-to-my-blog"}},"content":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Cumulative Table.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-1: Dimensional Modelling (Theory)\\n\\n## What is a Dimension?\\n\\nDimension are `attributes` of an entity ( example: user\'s name, age etc)\\n    - Dimensions may be `IDENTIFY` an entity, which would be a unique identifier for the entity (example: user_id, device_id etc)\\n    - Dimensions may be `DESCRIPTIVE` which would be the attributes of the entity (example: user\'s name,age etc) but wont be unique.\\n\\n\x3c!-- truncate --\x3e\\n\\nDimensions comes in two flavors:\\n    - `Slowly Changing Dimensions` : Dimensions that change over time like user\'s address, phone number etc. This attribute is time dependent. Can be painful deal with these attributes.\\n    - `Fixed`: Dimensions that do not change over time like user\'s birthdate. These are easy to deal with.\\n\\n:::info\\nKnowing whether the attributes are slowly changing or fixed is important to design the data model.\\n:::\\n## Knowing your data consumer\\n\\n`Who is using your data?` is the most important question to ask before designing the data model. The data model should be designed in such a way that it is easy for the data consumer to understand the data.\\n\\nWhether the data is getting delivered to:\\n1. Data Analyst / Data Scientist: \\n    - They would be interested in the data that is `easy to understand and analyze`.\\n    - They wouldnt be interested in dealing with `complex data types` like JSON, XML etc.\\n2. Data Engineers:\\n    - Sometimes downstream systems would be handled by Data Engineers, to take your data sets and join with other data to achieve the buisness logic, so the data model be compact and probably harder to query.\\n    - Nested types are fine to model the data.\\n    - Usually, as we get further up in ladder of data engineering, there would be `Master Data` which would be used by other data engineers or teams, to join the master data with other data sets.\\n3. ML models:\\n    - Generally machine learning models would need `identifier` and bunch of flattened decimal columns like `feature value columns` etc.\\n    - Sometimes it depends on the model, the model can sometime handle the complex data structures, similiar to what Data analyst and data scientist would need.\\n4. Customers\\n    - You would never give the data directly ( like JSON, CSVs etc) to the customers, you would always give them the `dashboards` or `reports` which would be easy to understand and analyze.\\n\\n> Not knowing your data consumer, there is a high chance that the data model would be designed in a wrong way, can lead to business loss.\\n\\n## OLTP Vs OLAP Vs Master Data\\n\\n1. OLTP(Online Transaction Processing):\\n    - OLTP is the system that is used to record the transactions that are happening in the system.\\n    - The data model is optimized for `low latency` and `low-volume` queries, usually dealing with `single record` at a time.\\n    - Usually Software Engineers model the data in OLTP.\\n    - The data model in OLTP is `3NF` (Third Normal Form) which means the data is broken down into multiple tables and the data is not repeated.\\n    - Databases: MySQL, Postgres etc.\\n2. OLAP(Online Analytical Processing):\\n    - OLAP is the system that is used to analyze the data that is stored in the OLTP system.\\n    - The data model is optimized for `high volume` and `complex queries`, using `aggregations` and `joins`.\\n    - Usually `Data Engineers` model the data in OLAP.\\n    - Analytical Databases: Redshift, BigQuery etc.\\n3. Master Data:\\n    - Master Data is optimied for completeness of entity definition and deduped.\\n    - Master Data is usually a middle layer between OLTP and OLAP.\\n\\n## Impacts of Mismatched Data Model\\n:::info\\n Mismatching needs of the data consumer == Less business value\\n:::\\n\\n- When you model transaction system as analytical system, like transactional system usually need only one record at a time, but if you model it as analytical system, you would retrieve all the unnecessary data which would be a performance hit.\\n- When you model analytical system as transactional system, like analytical system usually need to aggregate the data, but if you model it as transactional system, you would have to do the aggregation/joins on the fly which would be a performance hit.\\n- Having the Master Data as middle layer, would give you the flexibility whichever data model you want to use.\\n\\n## OLTP and OLAP is a continuum\\n\\n![data-flow](data-flow.png)\\n\\n- `Production Database Snapshot`: Usually the data in Production databases are transactional data, which is used to record the transactions that are happening in the system/web application/mobile application etc.\\n- `Master Data`: Taking the production datasets and creating the master data, for easy to understanding. Instead of querying \\"many\\" transactional databases, merging them into master data would be much efficient.\\n- `OLAP Cubes`: \\n    - You would flatten the data, like you might have multiple rows per entity, where you can aggregate the data.\\n    - OLAP Cubes space is also create the slice and dice the data.\\n- `Metrics`: You would aggregate even further to get the one number which would be like `KPIs` of the business.\\n\\n## Cumulative Table Design\\n\\n- Some days not every user would be active, but still master data should hold the details of the user.\\n- Cumulative table is the table that holds the `history of the data`.\\n\\n*The way cumulative table is designed is*:\\n- You would have 2 datasets like `today` and `yesterday`.\\n- `Full Outer Join` is done on the datasets, to get all the data, pulling the history data every day.\\n- `Coalesce` is used to get the latest data.\\n\\n*Where is it used?*\\n- `Growth Analysis`: To see how the data is growing over time.\\n- `State transition tracking`: To see how the data is changing over time.\\n\\n:::warning\\n- Cumulative table gets bigger and bigger over time, so applying the filtering on the data is important.\\n:::\\n\\n\\n*Cumulative Table: Strengths and Drawbacks*\\n\\n| **Category**  | **Strengths**                                                                 | **Drawbacks**                                                                                           |\\n|----------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\\n| **Analytics** | - Enables historical analysis without shuffling or reprocessing the data.    | - Sequential backfilling can limit flexibility when correcting or reprocessing older data.             |\\n| **Tracking**  | - Easy to track changes and transitions over time.                           | - Incremental growth of data size over time, leading to storage and performance challenges.             |\\n| **Data**      | - Provides a complete snapshot of trends and transitions.                   | - Retains potentially outdated or deleted PII data, complicating compliance with data privacy laws.     |\\n\\n## The compactness vs usability tradeoff\\n\\n- The most usable tables usually are\\n    - They are straightforward, they would have identifier for dimensional model.\\n    - Have no complex data types.\\n    - Easily can be manipulated with `WHERE` and `GROUP BY` clauses.\\n- The most compact tables usually are\\n    - Are compressed to be as small as possible, which would be harder to query directly until they\'re decoded.\\n\\n- The middle-ground table:\\n    - Where you would use complex data types (e.g. ARRAY, STRUCT, MAP) to model the data, which would be harder to query directly but can be used to join with other data sets.\\n\\n*When would you each type of table?*\\n- Most compact:\\n    - Online systems where latency and data volume are critical. \\n    - Consumers are usually hihgly technical and can handle complex data types.\\n- Most usable:\\n    - When analytics is the main consumer\\n    - The majority of the consumer are less technical and need the data to be easy to understand and analyze.\\n- Middle-ground:\\n    - Upstream staging/master data are used by majority of consumers who are other data engineers.\\n\\n## Struct Vs Array Vs Map\\n\\n| **Feature**           | **Struct**                                        | **Map**                                      | **Array**                                    |\\n|------------------------|--------------------------------------------------|---------------------------------------------|---------------------------------------------|\\n| **Definition**         | A table within a table.                          | A key-value pair structure.                 | A list of values.                           |\\n| **Keys**              | Rigidly defined, improving compression.          | Flexible, less compression efficiency.      | Not applicable (index-based).              |\\n| **Values**            | Can be of any type.                              | Must all be of the same type.               | Must all be of the same type.               |\\n| **Usage**             | Suitable for structured data with predefined keys.| Best for flexible key-value relationships.  | Ideal for ordered collections of data.     |\\n| **Complexity**         | Higher due to nested structure.                  | Simpler, dynamic key-value management.      | Simplest among the three; index-based access. |\\n| **Nesting**           | Supports nesting within itself or arrays.         | Can be nested inside structs or arrays.     | Can contain structs or maps as elements.   |\\n\\n## Impact of adding a temporal (time-based) dimension\\n\\n1. **Cardinality Explosion**:\\n   - Adding a time dimension (e.g., days, nights, or timestamps) increases the number of unique combinations in the dataset. This often grows by at least **one order of magnitude** (10x or more).\\n   - Example: Airbnb has ~6 million listings. If we want to track the nightly availability and pricing for each listing for a year, it results in:\\n     - **365 days \xd7 6 million listings = ~2 billion rows** of data.\\n\\n2. **Challenges with Dataset Design**:\\n   - **Option 1**: You could store this data as a single listing with an array (list) of 365 nights. This approach is compact but may be harder to query efficiently.\\n   - **Option 2**: Alternatively, store it as 2 billion rows, where each row represents one listing-night combination. While this structure is easier to query, it increases the dataset size significantly.\\n\\n3. **Using the Parquet**:\\n   - Using efficient file formats like **Parquet**, which compresses data well, helps to reduce the size of both approaches. With proper sorting and organization, you can minimize storage overhead.\\n\\n:::warning\\nBadness of denormolized temporal dimension: if you have to join the data with other dimensions, spark shuffle will ruin the compression.\\n:::\\n\\n## Run length encoding compression\\n\\n- [Run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)  is a form of lossless data compression in which runs of data (consecutive occurrences of the same data value) are stored as a single occurrence of that data value and a count of its consecutive occurrences, rather than as the original run. <sup>[1]</sup>\\n- RLE is one of reasons why Parquet is so efficient for storing data.\\n- However, **shuffle can ruin this efficiency**. Since shuffle happens in a distributed environment during JOIN and GROUP BY operations, data is redistributed across nodes, and compression benefits like RLE are often lost.\\n\\n---\\n[1]: source from wikipedia"},{"id":"welcome-to-my-blog","metadata":{"permalink":"/blog/welcome-to-my-blog","source":"@site/blog/2024-11-26-welcome-to-my-blog/index.md","title":"Welcome","description":"Welcome dear readers...","date":"2024-11-26T00:00:00.000Z","tags":[{"inline":false,"label":"Welcome post","permalink":"/blog/tags/welcome","description":"Welcome post"}],"readingTime":1.095,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"welcome-to-my-blog","title":"Welcome","authors":["me"],"tags":["hola"],"keywords":["blog","welcome"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"Welcome dear readers...\\n\\nIm Vibhavari Bellutagi, from India, currently staying in France. I\'m a Data Engineer and I got sometime to channelise my thoughts and experiences in the form of blog. I\'m a big fan of `Learn by Doing`, so you will see most of the posts are practical oriented.\\n\\n\x3c!-- truncate --\x3e\\n\\nI always wanted to have a blog to share my thoughts and experiences with the world. Im beginner in the world of blogging, so Im still learning how to write a good and effective content. I hope you enjoy my blog and find it useful.\\n\\nI have three different sections in my website:\\n\\n1. Blogs: Where I would long format technical blogs that I have experienced or learned recently.\\n2. Tech Bytes: Where I would post small technical tips and tricks that I have learned.\\n3. Projects: Where I would post the projects that I have worked on, I would like to start with [CodeChallenge](https://codingchallenges.fyi/) by [John Crickett](https://www.linkedin.com/in/johncrickett/).\\n4. From First Principles: Where I would post the concepts that I have learned from the scratch going to the depth of it.\\n\\nHope I will be able to keep up with the content and make it useful for you, im striving to make it better everyday.\\n\\nThanks for visiting my blog. Hope you enjoy it.\\n\\nCheers,\\nVibhavari"}]}}')}}]);