"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[5114],{5864:(a,e,t)=>{t.r(e),t.d(e,{assets:()=>p,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var s=t(5019),i=t(4848),n=t(8453);t(4512);const o={slug:"spark-job-anatomy",title:"Under the hood of a Spark job",authors:["me"],tags:["de","apache-spark"],keywords:["data engineering","apache spark","spark job anatomy","spark stages","spark tasks","spark jobs"],hide_table_of_contents:!1,image:"assets/spark-basics/spark-basics.png"},r=void 0,p={authorsImageUrls:[void 0]},l=[];function d(a){const e={code:"code",p:"p",...(0,n.R)(),...a.components};return(0,i.jsxs)(e.p,{children:["Understanding the internal execution flow of a Spark application is key to optimizing performance and debugging. This blog dives into the details of ",(0,i.jsx)(e.code,{children:"Spark jobs"}),", ",(0,i.jsx)(e.code,{children:"stages"}),", and ",(0,i.jsx)(e.code,{children:"tasks"}),", providing a thorough exploration of how Spark handles distributed execution."]})}function g(a={}){const{wrapper:e}={...(0,n.R)(),...a.components};return e?(0,i.jsx)(e,{...a,children:(0,i.jsx)(d,{...a})}):d(a)}},5019:a=>{a.exports=JSON.parse('{"permalink":"/blog/spark-job-anatomy","source":"@site/blog/2025-01-21-spark-job-anatomy/index.md","title":"Under the hood of a Spark job","description":"Understanding the internal execution flow of a Spark application is key to optimizing performance and debugging. This blog dives into the details of Spark jobs, stages, and tasks, providing a thorough exploration of how Spark handles distributed execution.","date":"2025-01-21T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":4.655,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-job-anatomy","title":"Under the hood of a Spark job","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","spark job anatomy","spark stages","spark tasks","spark jobs"],"hide_table_of_contents":false,"image":"assets/spark-basics/spark-basics.png"},"unlisted":false,"prevItem":{"title":"Spark Execution Modes","permalink":"/blog/spark-execution-modes"},"nextItem":{"title":"Handling Nulls in Spark","permalink":"/blog/handling-nulls-in-spark"}}')}}]);