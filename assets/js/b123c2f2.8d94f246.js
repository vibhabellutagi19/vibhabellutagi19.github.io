"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[5306],{4284:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>u,contentTitle:()=>h,default:()=>g,frontMatter:()=>d,metadata:()=>t,toc:()=>p});var t=n(4604),i=n(4848),r=n(8453),a=n(4512),o=n(552),l=n(1807),c=n(3760);const d={slug:"spark-execution-modes",title:"Spark Execution Modes",authors:["me"],tags:["de","apache-spark"],keywords:["data engineering","apache spark","spark application architecture","spark execution modes"],hide_table_of_contents:!1},h=void 0,u={authorsImageUrls:[void 0]},p=[{value:"Analogy",id:"analogy",level:3},{value:"Execution Modes",id:"execution-modes",level:2},{value:"Cluster Mode",id:"cluster-mode",level:3},{value:"Client Mode",id:"client-mode",level:3},{value:"Local Mode",id:"local-mode",level:3},{value:"Conclusion",id:"conclusion",level:2}];function x(e){const s={a:"a",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Highlight:n}=s;return n||function(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Highlight",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.p,{children:"In this post, we will discuss the different execution modes available in Apache Spark. Apache Spark provides three execution modes to run Spark applications. These execution modes are: Cluster mode, Client mode, and Local mode. Each of these modes has its own use case and is suitable for different scenarios."}),"\n",(0,i.jsx)(s.p,{children:"To have an understand of the execution mode, we will need to re-visit the high-level components of a Spark application, those are:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"The Spark Driver"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Driver is the ",(0,i.jsx)(n,{color:"#3e6980",children:"controller of the execution"})," of the spark application, it maintains the state of the spark cluster (the state and tasks of the executors)."]}),"\n",(0,i.jsx)(s.li,{children:"It will interact with the cluster manager to allocate resources for the executors and schedule tasks on the executors."}),"\n",(0,i.jsx)(s.li,{children:"The driver is just a process on the physical machine, which is responsible for maintaining the state of the application running on the cluster."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"The Spark Executors"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Executors are ",(0,i.jsx)(n,{color:"#3e6980",children:"the processes that run the actual tasks"})," of the spark application, assigned by the driver."]}),"\n",(0,i.jsxs)(s.li,{children:["The main responsibility of the executor is to","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"take the tasks from the driver"}),"\n",(0,i.jsx)(s.li,{children:"run the tasks"}),"\n",(0,i.jsx)(s.li,{children:"report back the status of the tasks to the driver and results."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Cluster Manager"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["The cluster manager is responsible for ",(0,i.jsx)(n,{color:"#3e6980",children:"maintaining a cluster of machines"})," that will run your Spark Application(s)."]}),"\n",(0,i.jsx)(s.li,{children:'Somewhat confusingly, a cluster manager has its own "driver" (sometimes referred to as master) and "worker" abstractions.'}),"\n",(0,i.jsx)(s.li,{children:"The key difference is that these abstractions are tied to physical machines rather than processes, as they are in Spark."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)("div",{class:"text--center",children:(0,i.jsx)("img",{src:o.A,width:"550",height:"450"})}),"\n",(0,i.jsx)(s.h3,{id:"analogy",children:"Analogy"}),"\n",(0,i.jsx)(s.p,{children:"Running a Spark application is like managing a busy restaurant where each component plays a distinct role to ensure everything runs smoothly and efficiently:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Spark Driver (Head Chef)"}),": The head chef manages the kitchen, assigns tasks to the cooks (executors), and makes sure every dish is prepared correctly."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Spark Executors (Cooks)"}),": The cooks prepare the dishes following the head chef\u2019s instructions and report back when each dish is ready."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Cluster Manager (Restaurant Manager)"}),": The restaurant manager ensures the kitchen has enough staff, ingredients, and equipment to run smoothly, handling multiple orders at once."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"execution-modes",children:"Execution Modes"}),"\n",(0,i.jsx)(s.p,{children:"An execution mode gives you the power to determine where the aforementioned resources are physically located when you go to run your application."}),"\n",(0,i.jsx)(s.h3,{id:"cluster-mode",children:"Cluster Mode"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"The user submits a JAR, Python script to the cluster manager."}),"\n",(0,i.jsx)(s.li,{children:"The cluster manager launches both the driver and executors on worker nodes within the cluster."}),"\n",(0,i.jsx)(s.li,{children:"The driver process runs on one worker node, while executors run on other worker nodes."}),"\n",(0,i.jsx)(s.li,{children:"The cluster manager manages and monitors all Spark application processes."}),"\n"]}),"\n",(0,i.jsx)("div",{class:"text--center",children:(0,i.jsx)("img",{src:l.A,width:"600",height:"400"})}),"\n",(0,i.jsx)(s.h3,{id:"client-mode",children:"Client Mode"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"Client Mode is similar to Cluster Mode, but the Spark Driver runs on the client machine (the machine that submits the application)."}),"\n",(0,i.jsx)(s.li,{children:"The client machine is responsible for managing the driver process."}),"\n",(0,i.jsx)(s.li,{children:"The cluster manager handles the executor processes, which run on worker nodes in the cluster."}),"\n",(0,i.jsxs)(s.li,{children:["In Client Mode, the application is submitted from a machine ",(0,i.jsx)(s.strong,{children:"outside the cluster"}),", often called a ",(0,i.jsx)(s.strong,{children:"gateway machine"})," or ",(0,i.jsx)(s.strong,{children:"edge node"}),"."]}),"\n",(0,i.jsx)(s.li,{children:"The driver stays on the client machine, while executors run inside the cluster on worker nodes."}),"\n"]}),"\n",(0,i.jsx)("div",{class:"text--center",children:(0,i.jsx)("img",{src:c.A,width:"600",height:"400"})}),"\n",(0,i.jsx)(s.h3,{id:"local-mode",children:"Local Mode"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"Local Mode runs the entire Spark application on a single machine."}),"\n",(0,i.jsx)(s.li,{children:"It achieves parallelism using threads on the same machine, not multiple worker nodes."}),"\n",(0,i.jsx)(s.li,{children:"Used for learning Spark, testing applications, and iterative development."}),"\n",(0,i.jsx)(s.li,{children:"No cluster manager is required; Spark manages everything locally."}),"\n",(0,i.jsx)(s.li,{children:"Ideal for small datasets and quick experiments but not suitable for production or large-scale jobs."}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{}),(0,i.jsx)(s.th,{children:(0,i.jsx)(s.strong,{children:"Client Mode"})}),(0,i.jsx)(s.th,{children:(0,i.jsx)(s.strong,{children:"Cluster Mode"})})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.strong,{children:"Best for"})}),(0,i.jsx)(s.td,{children:"Interactive use (development, debugging)"}),(0,i.jsx)(s.td,{children:"Production jobs, large-scale deployments"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.strong,{children:"Latency"})}),(0,i.jsx)(s.td,{children:"Lower latency for small tasks"}),(0,i.jsx)(s.td,{children:"Slightly higher due to internal resource allocation"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.strong,{children:"Example Use Case"})}),(0,i.jsx)(s.td,{children:"Testing and interactive Spark shells"}),(0,i.jsx)(s.td,{children:"Scheduled batch jobs or long-running Spark applications"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.strong,{children:"Advantages"})}),(0,i.jsx)(s.td,{children:"Easier to debug and monitor from client machine"}),(0,i.jsx)(s.td,{children:"More reliable and scalable for production workloads"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.strong,{children:"Disadvantages"})}),(0,i.jsx)(s.td,{children:"Unreliable for long-running tasks"}),(0,i.jsx)(s.td,{children:"More setup needed, harder to debug directly"})]})]})]}),"\n",(0,i.jsxs)(s.p,{children:["I hope this blog helped you understand the different execution modes. To learn more about the Spark application architecture, you will find it ",(0,i.jsx)(s.a,{href:"/blog/spark-application-lifecycle-outside",children:"here"}),". If you are interested in reading more about Spark, check out the other posts in this ",(0,i.jsx)(s.a,{href:"/blog/tags/apache-spark",children:"series"}),".\nIf you have any questions or feedback, feel free to reach out to me on ",(0,i.jsx)(a.A,{})]})]})}function g(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(x,{...e})}):x(e)}},3760:(e,s,n)=>{n.d(s,{A:()=>t});const t=n.p+"assets/images/client_mode-425a36cd961a4be2fddf5ef5536c5f84.gif"},552:(e,s,n)=>{n.d(s,{A:()=>t});const t=n.p+"assets/images/cluster_manager-ee69ff7b4553bc54b04ae088f7250cf9.png"},1807:(e,s,n)=>{n.d(s,{A:()=>t});const t=n.p+"assets/images/cluster_mode-e0aa25aea4b420a85c48b967f0b1e514.gif"},4604:e=>{e.exports=JSON.parse('{"permalink":"/blog/spark-execution-modes","source":"@site/blog/2025-02-06-spark-execution-modes/index.md","title":"Spark Execution Modes","description":"In this post, we will discuss the different execution modes available in Apache Spark. Apache Spark provides three execution modes to run Spark applications. These execution modes are: Cluster mode, Client mode, and Local mode. Each of these modes has its own use case and is suitable for different scenarios.","date":"2025-02-06T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":3.975,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-execution-modes","title":"Spark Execution Modes","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","spark application architecture","spark execution modes"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"The Life Cycle of a Spark Application ( Outside )","permalink":"/blog/spark-application-lifecycle-outside"},"nextItem":{"title":"Under the hood of a Spark job","permalink":"/blog/spark-job-anatomy"}}')}}]);