"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"spark-application-lifecycle-outside","metadata":{"permalink":"/blog/spark-application-lifecycle-outside","source":"@site/blog/2025-02-07-spark-application-architecture-outside/index.md","title":"The Life Cycle of a Spark Application ( Outside )","description":"In this blog, we will go in-depth on the overall life cycle of Spark Applications from outside the actual Spark code. Before going ahead, I recommend reading the Execution Modes of the Spark application.","date":"2025-02-07T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":2.59,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-application-lifecycle-outside","title":"The Life Cycle of a Spark Application ( Outside )","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","spark application architecture"],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Spark Execution Modes","permalink":"/blog/spark-execution-modes"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\nimport Completion from \'./assets/completion.gif\';\\nimport RequestingResources from \'./assets/requesting_resources.gif\';\\nimport LaunchApp from \'./assets/launch.gif\';\\nimport Execution from \'./assets/execution.gif\';\\n\\n\\nIn this blog, we will go in-depth on the overall life cycle of Spark Applications from outside the actual Spark code. Before going ahead, I recommend reading the [Execution Modes](/blog/spark-execution-modes) of the Spark application.\\n\x3c!-- truncate --\x3e\\n\\n## Client Request\\n\\n- The first step is the client request. This is the request that is made by the user to the Spark Application. \\n- This request can be made in a variety of ways: Compiled Jars or a library. The most common way is through the `Spark Submit` command Using compiled Jars. \\n- At this point, the client is executing code on the local machine and is going to make a request to the cluster manager driver node.\\n\\n<div class=\\"text--center\\"><img src={RequestingResources} width=\\"500\\" height=\\"400\\" /></div>\\n\\n\\n```\\n./bin/spark-submit \\\\\\n  --class <main-class> \\\\\\n  --master <master-url> \\\\\\n  --deploy-mode cluster \\\\\\n  --conf <key>=<value> \\\\\\n  ... # other options\\n  <application-jar> \\\\\\n  [application-arguments]\\n```\\n\\nThe Spark Submit command takes number of arguments, for example:\\n- the main class of the Application\\n- the master URL ( local, yarn, Mesos etc )\\n- the deploy mode (local, cluster, client)\\n- the configuration of the application\\n- Other arguments like the number of executors to use, the amount of memory to allocate to each executor, the location of the input data.\\n- the location of the application jar file.\\n\\n## Launch\\n\\n- After the spark-submit command is executed, the driver process has been placed on the cluster, begins running user code\\n- The user code should contain `SparkSession`, which is responsible for initializing the Spark Cluster(eg: driver and executors).\\n- Subsequently, the `SparkSession` will communicate with cluster manager for launching the spark executors processors across the cluster.\\n- The relevant configurations like number of executors, memory, cores etc are passed to the cluster manager by user via `spark-submit` command.\\n\\n<div class=\\"text--center\\"><img src={LaunchApp} width=\\"500\\" height=\\"400\\" /></div>\\n\\n\\n## Execution\\n\\n- Spark Context is created now, spark goes about executing the code.\\n- The driver process and executors communicate with each other, executing code and moving data between each other.\\n- The driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure. \\n\\n<div class=\\"text--center\\"><img src={Execution} width=\\"550\\" height=\\"400\\" /></div>\\n\\n## Completion\\n\\n- Upon completion, the driver process exits with either success or failure.\\n- The cluster manager shuts down the executors in the Spark cluster for the driver.\\n- The success or failure of the Spark Application can be checked by querying the cluster manager.\\n\\n<div class=\\"text--center\\"><img src={Completion} width=\\"550\\" height=\\"400\\" /></div>\\n\\n## Conclusion\\n\\nIn this blog, we have explored the overall life cycle of a Spark Application from outside the actual Spark code. We have detailed how the client request is made, how the Spark Application is launched, how the code is executed, and how the Spark Application reaches its completion. If you are interested in more such content, do check out [Apache Spark](/blog/tags/apache-spark) series.\\n\\nIf you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"spark-execution-modes","metadata":{"permalink":"/blog/spark-execution-modes","source":"@site/blog/2025-02-06-spark-execution-modes/index.md","title":"Spark Execution Modes","description":"In this post, we will discuss the different execution modes available in Apache Spark. Apache Spark provides three execution modes to run Spark applications. These execution modes are: Cluster mode, Client mode, and Local mode. Each of these modes has its own use case and is suitable for different scenarios.","date":"2025-02-06T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":3.975,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-execution-modes","title":"Spark Execution Modes","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","spark application architecture","spark execution modes"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"The Life Cycle of a Spark Application ( Outside )","permalink":"/blog/spark-application-lifecycle-outside"},"nextItem":{"title":"Under the hood of a Spark job","permalink":"/blog/spark-job-anatomy"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\nimport ClusterManager from \'./assets/cluster_manager.png\'\\nimport ClusterMode from \'./assets/cluster_mode.gif\'\\nimport ClientMode from \'./assets/client_mode.gif\'\\n\\nIn this post, we will discuss the different execution modes available in Apache Spark. Apache Spark provides three execution modes to run Spark applications. These execution modes are: Cluster mode, Client mode, and Local mode. Each of these modes has its own use case and is suitable for different scenarios.\\n \x3c!--truncate--\x3e\\n\\n To have an understand of the execution mode, we will need to re-visit the high-level components of a Spark application, those are:\\n\\n 1. **The Spark Driver**: \\n    - Driver is the <Highlight color=\\"#3e6980\\">controller of the execution</Highlight> of the spark application, it maintains the state of the spark cluster (the state and tasks of the executors). \\n    - It will interact with the cluster manager to allocate resources for the executors and schedule tasks on the executors. \\n    - The driver is just a process on the physical machine, which is responsible for maintaining the state of the application running on the cluster.\\n 2. **The Spark Executors**:\\n    - Executors are <Highlight color=\\"#3e6980\\">the processes that run the actual tasks</Highlight> of the spark application, assigned by the driver.\\n    - The main responsibility of the executor is to \\n        - take the tasks from the driver\\n        - run the tasks\\n        - report back the status of the tasks to the driver and results.\\n 3. **Cluster Manager**:\\n    - The cluster manager is responsible for <Highlight color=\\"#3e6980\\">maintaining a cluster of machines</Highlight> that will run your Spark Application(s). \\n    - Somewhat confusingly, a cluster manager has its own \\"driver\\" (sometimes referred to as master) and \\"worker\\" abstractions.\\n    - The key difference is that these abstractions are tied to physical machines rather than processes, as they are in Spark.\\n\\n<div class=\\"text--center\\"><img src={ClusterManager} width=\\"550\\" height=\\"450\\" /></div>\\n\\n### Analogy\\nRunning a Spark application is like managing a busy restaurant where each component plays a distinct role to ensure everything runs smoothly and efficiently:\\n\\n1. **Spark Driver (Head Chef)**: The head chef manages the kitchen, assigns tasks to the cooks (executors), and makes sure every dish is prepared correctly.  \\n2. **Spark Executors (Cooks)**: The cooks prepare the dishes following the head chef\u2019s instructions and report back when each dish is ready.  \\n3. **Cluster Manager (Restaurant Manager)**: The restaurant manager ensures the kitchen has enough staff, ingredients, and equipment to run smoothly, handling multiple orders at once.\\n\\n## Execution Modes\\n\\nAn execution mode gives you the power to determine where the aforementioned resources are physically located when you go to run your application.\\n\\n### Cluster Mode\\n\\n1. The user submits a JAR, Python script to the cluster manager.  \\n2. The cluster manager launches both the driver and executors on worker nodes within the cluster.  \\n3. The driver process runs on one worker node, while executors run on other worker nodes.  \\n4. The cluster manager manages and monitors all Spark application processes.  \\n\\n<div class=\\"text--center\\"><img src={ClusterMode} width=\\"600\\" height=\\"400\\" /></div>\\n\\n### Client Mode\\n\\n1. Client Mode is similar to Cluster Mode, but the Spark Driver runs on the client machine (the machine that submits the application).  \\n2. The client machine is responsible for managing the driver process.  \\n3. The cluster manager handles the executor processes, which run on worker nodes in the cluster.  \\n4. In Client Mode, the application is submitted from a machine **outside the cluster**, often called a **gateway machine** or **edge node**.  \\n5. The driver stays on the client machine, while executors run inside the cluster on worker nodes.  \\n\\n<div class=\\"text--center\\"><img src={ClientMode} width=\\"600\\" height=\\"400\\" /></div>\\n\\n### Local Mode\\n\\n1. Local Mode runs the entire Spark application on a single machine.  \\n2. It achieves parallelism using threads on the same machine, not multiple worker nodes.  \\n3. Used for learning Spark, testing applications, and iterative development.  \\n4. No cluster manager is required; Spark manages everything locally.  \\n5. Ideal for small datasets and quick experiments but not suitable for production or large-scale jobs.\\n\\n## Conclusion\\n\\n|                            | **Client Mode**                          | **Cluster Mode**                       |\\n|-----------------------------|------------------------------------------|----------------------------------------|\\n| **Best for**                | Interactive use (development, debugging) | Production jobs, large-scale deployments |\\n| **Latency**                 | Lower latency for small tasks            | Slightly higher due to internal resource allocation |\\n| **Example Use Case**        | Testing and interactive Spark shells     | Scheduled batch jobs or long-running Spark applications |\\n| **Advantages**              | Easier to debug and monitor from client machine | More reliable and scalable for production workloads |\\n| **Disadvantages**           | Unreliable for long-running tasks        | More setup needed, harder to debug directly |\\n\\nI hope this blog helped you understand the different execution modes. To learn more about the Spark application architecture, you will find it [here](/blog/spark-application-lifecycle-outside). If you are interested in reading more about Spark, check out the other posts in this [series](/blog/tags/apache-spark).\\nIf you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"spark-job-anatomy","metadata":{"permalink":"/blog/spark-job-anatomy","source":"@site/blog/2025-01-21-spark-job-anatomy/index.md","title":"Under the hood of a Spark job","description":"Understanding the internal execution flow of a Spark application is key to optimizing performance and debugging. This blog dives into the details of Spark jobs, stages, and tasks, providing a thorough exploration of how Spark handles distributed execution.","date":"2025-01-21T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":4.655,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-job-anatomy","title":"Under the hood of a Spark job","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","spark job anatomy","spark stages","spark tasks","spark jobs"],"hide_table_of_contents":false,"image":"assets/spark-basics/spark-basics.png"},"unlisted":false,"prevItem":{"title":"Spark Execution Modes","permalink":"/blog/spark-execution-modes"},"nextItem":{"title":"Handling Nulls in Spark","permalink":"/blog/handling-nulls-in-spark"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nUnderstanding the internal execution flow of a Spark application is key to optimizing performance and debugging. This blog dives into the details of `Spark jobs`, `stages`, and `tasks`, providing a thorough exploration of how Spark handles distributed execution.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Spark Job Anatomy\\n### Jobs\\nEach application is made up of one or more Spark jobs. Spark jobs within an application are executed serially. In general, a job is created for <Highlight color=\\"#3e6980\\">each action operation</Highlight> in the spark application.\\n\\nEach job breaks down into a series of stages, the number of which depends on how many shuffle operations need to take place.\\n\\nSince Spark [evaluates lazily](https://en.wikipedia.org/wiki/Lazy_evaluation), the job is not executed until an action is called. If there are no actions, there are no jobs.\\n\\n![no-action-no-job](assets/no-actions-no-jobs.png)\\n\\nWhen an action is called on a RDD/DF, a job is created.\\n\\n![action-job](assets/job-created.png)\\n\\nKey Features of Spark Jobs:\\n- Each job is composed of multiple stages.\\n- Spark generates one job per action.\\n- Jobs are independent and can run in parallel if no dependencies exist between them.\\n\\n### Stages\\n\\nA stage is a group of tasks that Spark can run together to perform the same operation across multiple machines. While Spark tries to include as much work as possible in one stage, it creates a new stage whenever there\u2019s a shuffle operation.\\n\\nA shuffle happens when <Highlight color=\\"#3e6980\\">Spark redistributes data across partitions or nodes in the cluster</Highlight> to complete certain operations. It involves a physical rearrangement of the data to meet the needs of the computation.\\n\\n<u>For example</u>:\\nSorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node). \\n\\nThis type of repartitioning requires coordinating across executors to move data around. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result\\n\\n```\\nscala> val df1 = spark.range(2, 10000000, 2) \\nval df1: org.apache.spark.sql.Dataset[Long] = [id: bigint]\\n\\nscala> val step1 = df1.repartition(5)\\nval step1: org.apache.spark.sql.Dataset[Long] = [id: bigint]\\n\\nscala> step1.count()\\nval res0: Long = 4999999\\n```\\n\\n![stages](assets/shuffles.png)\\n\\nIn the above example:\\n- The the 1st stage (stage-0) correspond to the `range` that you perform in order to create your DataFrames. By default when you create a DataFrame with range, it has `8 partitions`.\\n- The next stage (stage-1) is the `repartitioning`. This changes the number of partitions by shuffling the data. These DataFrames are shuffled into `5 partitions`.\\n- The final stage (stage-2) is the `count action`. This is the final stage that is executed.\\n\\n\\n``` scala\\nval df1 = spark.range(2, 10000000, 2)\\nval df2 = spark.range(2, 10000000, 4)\\nval step1 = df1.repartition(5)\\nval step12 = df2.repartition(6)\\nval step2 = step1.selectExpr(\\"id * 5 as id\\")\\nval step3 = step2.join(step12, [\\"id\\"])\\nval step4 = step3.selectExpr(\\"sum(id)\\")\\n```\\n\\n![shuffle-partitions](assets/shuffle-partitions.png)\\n\\n![shuffle-partition-viz](assets/shuffle-partition-viz.png)\\n\\n- Stages 3 and 4 perform on each of those DataFrames and the end of the stage represents the join (a shuffle). \\n- Suddenly, why we have 200 tasks? This is because of a Spark SQL configuration. The `spark.sql.shuffle.partitions` default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. However you can change this value, and the number of output partitions will change.\\n\\n:::note\\nA good rule of thumb: The number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. \\n:::\\n\\n### Tasks\\n\\nA task is just a <Highlight color=\\"#3e6980\\">unit of computation applied to a unit of data (the partition)</Highlight>. Stages in Spark consists of tasks. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor.\\n\\nPartitioning your data into a greatest number of partitions means that more can be executed in parallel.\\n\\n## Summary\\n\\nEasy way to remember how Spark organizes work:\\n\\n| **Topic**                 | **Key Points**                                                                                                                                                                          |\\n|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| **App Decomposition**     | - 1 Spark Application can have 1 or more Jobs. <br/> - Each Job is broken into 1 or more Stages.                                                                                       |\\n| **Stages & Tasks**        | - Each Stage is subdivided into Tasks. <br/> - Shuffle boundaries (data rearrangements) define when Spark moves from one Stage to the next.                                            |\\n| **Tasks & Executors**     | - One Task runs on one Executor (it can\u2019t move mid-task). <br/> - An Executor can run multiple Tasks (depending on its available cores).                                                |\\n| **Partitions & Tasks**    | - Processing one partition = one Task. <br/> - The number of Tasks in a Stage often equals the number of partitions for that data.                                                     |\\n| **Partitions & Executors**| - A partition stays on one Executor while it\u2019s being processed. <br/> - Each Executor can hold 0 or more partitions in memory or on disk.                                              |\\n| **Executors & Nodes**     | - 1 Executor corresponds to 1 JVM running on 1 physical/virtual Node. <br/> - Each Node can host 0 or more Executors.                                                                   |\\n\\nUse this table as a handy to keep the Spark \u201cbig picture\u201d in mind:\\n\\n1. Spark **Applications** \u2192 **Jobs** \u2192 **Stages** \u2192 **Tasks**.  \\n2. A **Task** processes a **Partition**.  \\n3. **Executors** (JVMs) on cluster **Nodes** do the actual work.\\n\\n## References\\n\\n1. [Spark: The Definitive Guide](https://www.amazon.com/Spark-Definitive-Guide-Processing-Simple/dp/1491912219)\\n2. [Apache Spark Documentation](https://spark.apache.org/docs/latest/index.html)\\n\\nI hope you enjoyed reading this blog, if you are interested in learning other topics related to Apache Spark, feel free to check [Apache Spark](/blog/tags/apache-spark) series.\\n\\nIf you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"handling-nulls-in-spark","metadata":{"permalink":"/blog/handling-nulls-in-spark","source":"@site/blog/2025-01-13-handling-nulls-in-spark/index.md","title":"Handling Nulls in Spark","description":"In SQL null or Null is a special marker used to indicate that a data value does not exist in the database. A null should not be confused with a value of 0. A null indicates a lack of a value, which is not the same as a zero value.","date":"2025-01-13T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":10.055,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"handling-nulls-in-spark","title":"Handling Nulls in Spark","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","nulls in spark"],"hide_table_of_contents":false,"image":"assets/logo.png"},"unlisted":false,"prevItem":{"title":"Under the hood of a Spark job","permalink":"/blog/spark-job-anatomy"},"nextItem":{"title":"Columns and Expressions","permalink":"/blog/columns-and-expressions"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIn SQL `null` or `Null` is a special marker used to indicate that a data value does not exist in the database. A null should not be confused with a value of 0. A null indicates a lack of a value, which is not the same as a zero value.\\n\\nFor example:\\nConsider the question \\"How many books does Krishna own?\\" \\nThe answer may be `zero` (we know that he owns none) or `null` (we do not know how many he owns).\\n\\nLet\'s deep dive into handling nulls in Spark.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Handling nulls in Spark\\n\\nIn Spark, a null value represents the absence of any value: it is not zero, an empty string, or false. Spark treats nulls in a way similar to SQL. When performing operations on null values, you need to be explicit about how to handle them, as any operation with a null value generally yields a null result (unless specifically handled).\\n\\nKey Points about Nulls in Spark:\\n1. **Null propagation**: If you do an arithmetic operation (e.g., `colA + colB`) and either `colA` or `colB` is null, the result is null unless there is a function to handle null explicitly (like coalesce).\\n\\n2. **Comparison**: Comparisons with null always yield false or unknown. For example, df.filter(`df(\\"age\\") == null`) might not work as intended. Instead, you need to use methods like isNull or isNotNull.\\n\\n3. **Equi-Null safe join**: Spark SQL provides nullSafeEq or eqNullSafe (`<=>` in SQL) to compare two columns including nulls. This means if both sides are null, it returns true.\\n\\n:::note\\nIn SQL, null is a marker, not a value.\\n:::\\n\\n### Data Preparation \\n\\n```python\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, isnull, when, coalesce\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\\"NullHandlingExample\\") \\\\\\n    .getOrCreate()\\n\\ndata = [\\n    (6.1,   1071,  \\"12-Jun-98\\",  None,  None,                \\"The Land Girls\\"),\\n    (6.9,   207,   \\"7-Aug-98\\",   None,  None,                \\"First Love, Last Rites\\"),\\n    (6.8,   865,   \\"28-Aug-98\\",  None,  None,                \\"I Married a Strange Person\\"),\\n    (5.8,   3275,  \\"1-Jul-86\\",  \\"13\\",  None,                \\"Pirates\\"),\\n    (3.4,   165,   \\"9-Oct-98\\",   \\"62\\",  \\"Original Screenplay\\",\\"Slam\\"),\\n    (None,  None,  \\"15-Jan-99\\",  None,  None,                \\"Mississippi Mermaid\\"),\\n    (7.7,   15133, \\"4-Apr-99\\",   None,  None,                \\"Following\\"),\\n    (3.8,   353,   \\"9-Apr-99\\",   None,  \\"Original Screenplay\\",\\"Foolish\\"),\\n    (5.8,   3275,  \\"1-Jul-86\\",   \\"25\\",  None,                \\"Pirates\\"),\\n    (7.0,   2906,  \\"31-Dec-46\\",  \\"86\\",  None,                \\"Duel in the Sun\\")\\n]\\n\\ncolumns = [\\"IMDB_Rating\\", \\"IMDB_Votes\\", \\"Release_Date\\", \\"Rotten_Tomatoes_Rating\\", \\"Source\\", \\"Title\\"]\\ndf = spark.createDataFrame(data, columns)\\n\\ndf.show()\\n```\\n\\n<details>\\n<summary>Output: You will see the movies data with some null values.</summary>\\n```\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|Source             |Title                     |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|6.1        |1071      |12-Jun-98   |NULL                  |NULL               |The Land Girls            |\\n|6.9        |207       |7-Aug-98    |NULL                  |NULL               |First Love, Last Rites    |\\n|6.8        |865       |28-Aug-98   |NULL                  |NULL               |I Married a Strange Person|\\n|5.8        |3275      |1-Jul-86    |13                    |NULL               |Pirates                   |\\n|3.4        |165       |9-Oct-98    |62                    |Original Screenplay|Slam                      |\\n|NULL       |NULL      |15-Jan-99   |NULL                  |NULL               |Mississippi Mermaid       |\\n|7.7        |15133     |4-Apr-99    |NULL                  |NULL               |Following                 |\\n|3.8        |353       |9-Apr-99    |NULL                  |Original Screenplay|Foolish                   |\\n|5.8        |3275      |1-Jul-86    |25                    |NULL               |Pirates                   |\\n|7.0        |2906      |31-Dec-46   |86                    |NULL               |Duel in the Sun           |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n```\\n</details>\\n\\n### Coalesce\\n- In Spark SQL and DataFrame APIs, `coalesce()` is used to return the first non-null value among its arguments.\\n- If you have multiple columns and want a single column which picks up a non-null value from one of them in order, or you want to replace a single column\u2019s null value with a fallback.\\n\\n```python\\nrating_df = df.select(col(\\"Title\\"),  \\n              col(\\"Rotten_Tomatoes_Rating\\"),  \\n              col(\\"IMDB_Rating\\"),  \\n              coalesce(col(\\"Rotten_Tomatoes_Rating\\"), col(\\"IMDB_Rating\\") * 10).alias(\\"Rating\\"))\\n\\nrating_df.show(10, False)\\n```\\n<details>\\n<summary>Output: You will see when `Rotten_Tomatoes_Rating` is null, `Rating` is calculated as `IMDB_Rating * 10`.</summary>\\n```\\n+--------------------------+----------------------+-----------+------+\\n|Title                     |Rotten_Tomatoes_Rating|IMDB_Rating|Rating|\\n+--------------------------+----------------------+-----------+------+\\n|The Land Girls            |NULL                  |6.1        |61.0  |\\n|First Love, Last Rites    |NULL                  |6.9        |69.0  |\\n|I Married a Strange Person|NULL                  |6.8        |68.0  |\\n|Pirates                   |13                    |5.8        |13    |\\n|Slam                      |62                    |3.4        |62    |\\n|Mississippi Mermaid       |NULL                  |NULL       |NULL  |\\n|Following                 |NULL                  |7.7        |77.0  |\\n|Foolish                   |NULL                  |3.8        |38.0  |\\n|Pirates                   |25                    |5.8        |25    |\\n|Duel in the Sun           |86                    |7.0        |86    |\\n+--------------------------+----------------------+-----------+------+\\n```\\n</details>\\n\\n### Checking for nulls\\nIn Spark, null indicates missing or unknown data. To check if a column has null values, you can use:\\n\\n- `isNull()`\\n- `isNotNull()`\\n- `eqNullSafe()`: compares two columns, treating null values as equal.\\n\\n```python\\n# Check if Rotten_Tomatoes_Rating is null\\ncheck_nulls = df.select(col(\\"Title\\")\\n               ,col(\\"Rotten_Tomatoes_Rating\\"),col(\\"IMDB_Rating\\")\\n               ,coalesce(col(\\"Rotten_Tomatoes_Rating\\")\\n               , col(\\"IMDB_Rating\\") * 10).alias(\\"Rating\\")).where(col(\\"Rating\\").isNotNull())\\ncheck_nulls.show(10, False)\\n```\\n<details>\\n<summary>Output: You will see only rows where `Rating` is not null.</summary>\\n```\\n+--------------------------+----------------------+-----------+------+\\n|Title                     |Rotten_Tomatoes_Rating|IMDB_Rating|Rating|\\n+--------------------------+----------------------+-----------+------+\\n|The Land Girls            |NULL                  |6.1        |61.0  |\\n|First Love, Last Rites    |NULL                  |6.9        |69.0  |\\n|I Married a Strange Person|NULL                  |6.8        |68.0  |\\n|Pirates                   |13                    |5.8        |13    |\\n|Slam                      |62                    |3.4        |62    |\\n|Following                 |NULL                  |7.7        |77.0  |\\n|Foolish                   |NULL                  |3.8        |38.0  |\\n|Pirates                   |25                    |5.8        |25    |\\n|Duel in the Sun           |86                    |7.0        |86    |\\n+--------------------------+----------------------+-----------+------+\\n```\\n</details>\\n---\\n\\n```python\\n# Check for equi-null safe join\\n\\ndf_eq_null_safe = df.filter(col(\\"IMDB_Rating\\").eqNullSafe(\\"Rotten_Tomatoes_Rating\\"))\\ndf_eq_null_safe.show()\\n```\\n\\n<details>\\n<summary>Output: You will see only rows where `IMDB_Rating` is equal to `Rotten_Tomatoes_Rating`.</summary>\\n```python\\n+-----------+----------+------------+----------------------+------+-------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|Source|              Title|\\n+-----------+----------+------------+----------------------+------+-------------------+\\n|       NULL|      NULL|   15-Jan-99|                  NULL|  NULL|Mississippi Mermaid|\\n+-----------+----------+------------+----------------------+------+-------------------+\\n```\\n</details>\\n\\n### Nulls when ordering columns\\n\\nWhen you order a DataFrame in Spark, null handling can change your result. By default, Spark sorts `nulls first in ascending order` and `last in descending order`.\\n\\n1. `asc_nulls_first`: Ascending order, placing nulls at the top.\\n2. `asc_nulls_last`: Ascending order, placing nulls at the bottom.\\n3. `desc_nulls_first`: Descending order, placing nulls at the top.\\n4. `desc_nulls_last`: Descending order, placing nulls at the bottom\\n\\n\\n```python\\n# Move nulls to the end in descending order\\n\\nmove_nulls_to_end = df.orderBy(col(\\"Rotten_Tomatoes_Rating\\").desc_nulls_last())\\nmove_nulls_to_end.show()\\n```\\n<details>\\n<summary>Output: You will see the rows ordered by `Rotten_Tomatoes_Rating` with nulls at the end.</summary>\\n```\\n+-----------+----------+------------+----------------------+-------------------+--------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|             Source|               Title|\\n+-----------+----------+------------+----------------------+-------------------+--------------------+\\n|        7.0|      2906|   31-Dec-46|                    86|               NULL|     Duel in the Sun|\\n|        3.4|       165|    9-Oct-98|                    62|Original Screenplay|                Slam|\\n|        5.8|      3275|    1-Jul-86|                    25|               NULL|             Pirates|\\n|        5.8|      3275|    1-Jul-86|                    13|               NULL|             Pirates|\\n|        6.9|       207|    7-Aug-98|                  NULL|               NULL|First Love, Last ...|\\n|       NULL|      NULL|   15-Jan-99|                  NULL|               NULL| Mississippi Mermaid|\\n|        7.7|     15133|    4-Apr-99|                  NULL|               NULL|           Following|\\n|        6.8|       865|   28-Aug-98|                  NULL|               NULL|I Married a Stran...|\\n|        3.8|       353|    9-Apr-99|                  NULL|Original Screenplay|             Foolish|\\n|        6.1|      1071|   12-Jun-98|                  NULL|               NULL|      The Land Girls|\\n+-----------+----------+------------+----------------------+-------------------+--------------------+\\n```\\n</details>\\n\\n\\n### Replace nulls with a value\\nSpark provides multiple ways to replace null values:\\n\\n- `DataFrame.na.fill(value, subset=None)` \u2013 Fill null values in specified columns.\\n- `DataFrame.fillna(value, subset=None)` \u2013 Same as na.fill, a common alias.\\n\\n\\n```python\\n# Replace nulls with a value for Source column\\nreplace_nulls = df.na.fill(\\"Un Source\\",[\\"Source\\"])\\nreplace_nulls.show(5, False)\\n```\\n<details>\\n<summary>Output: You will see the rows with nulls in the `Source` column replaced with `Un Source`.</summary>\\n```\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|Source             |Title                     |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|6.1        |1071      |12-Jun-98   |NULL                  |Un Source          |The Land Girls            |\\n|6.9        |207       |7-Aug-98    |NULL                  |Un Source          |First Love, Last Rites    |\\n|6.8        |865       |28-Aug-98   |NULL                  |Un Source          |I Married a Strange Person|\\n|5.8        |3275      |1-Jul-86    |13                    |Un Source          |Pirates                   |\\n|3.4        |165       |9-Oct-98    |62                    |Original Screenplay|Slam                      |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n```\\n</details>\\n\\n```python\\n# Replacing multiple columns with nulls\\n\\nfill_values = {\\n    \\"IMDB_Rating\\": 0.0,\\n    \\"IMDB_Votes\\":  0,\\n    \\"Source\\":      \\"Unknown Source\\"\\n}\\n\\ndf_fill_multiple = df.na.fill(fill_values)\\ndf_fill_multiple.show(5, False)\\n```\\n<details>\\n<summary>Output: You will see the rows with nulls in the specified columns replaced with the provided values.</summary>\\n```\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|Source             |Title                     |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|6.1        |1071      |12-Jun-98   |NULL                  |Unknown Source     |The Land Girls            |\\n|6.9        |207       |7-Aug-98    |NULL                  |Unknown Source     |First Love, Last Rites    |\\n|6.8        |865       |28-Aug-98   |NULL                  |Unknown Source     |I Married a Strange Person|\\n|5.8        |3275      |1-Jul-86    |13                    |Unknown Source     |Pirates                   |\\n|3.4        |165       |9-Oct-98    |62                    |Original Screenplay|Slam                      |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n```\\n</details>\\n\\n### Remove nulls\\nSometimes you might want to drop rows that contain any null or all nulls, or conditionally drop rows with null in specific columns.\\n\\nExample 1:\\n```python\\n# Drop rows with all null values\\n\\ndf_drop_any = df.na.drop(\\"any\\") # \\"any\\" means if any column in the row has null, that row is dropped.\\ndf_drop_any.show(5, False)\\n```\\n\\n<details>\\n<summary>Output: You will see the rows with any null values dropped.</summary>\\n```\\n+-----------+----------+------------+----------------------+-------------------+-----+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|             Source|Title|\\n+-----------+----------+------------+----------------------+-------------------+-----+\\n|        3.4|       165|    9-Oct-98|                    62|Original Screenplay| Slam|\\n+-----------+----------+------------+----------------------+-------------------+-----+\\n```\\n</details>\\n\\nExample 2:\\n```python\\n# Drop rows with all null values\\ndf_drop_all = df.na.drop(\\"all\\") # \\"all\\" means a row must have all columns as null to be dropped.\\ndf_drop_all.show()\\n```\\n<details>\\n<summary>Output: You will see the rows with all null values dropped.</summary>\\n```\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|Source             |Title                     |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n|6.1        |1071      |12-Jun-98   |NULL                  |NULL               |The Land Girls            |\\n|6.9        |207       |7-Aug-98    |NULL                  |NULL               |First Love, Last Rites    |\\n|6.8        |865       |28-Aug-98   |NULL                  |NULL               |I Married a Strange Person|\\n|5.8        |3275      |1-Jul-86    |13                    |NULL               |Pirates                   |\\n|3.4        |165       |9-Oct-98    |62                    |Original Screenplay|Slam                      |\\n+-----------+----------+------------+----------------------+-------------------+--------------------------+\\n```\\n</details>\\n\\nExample 3:\\n```python\\n# Drop rows if \\"IMDB_Rating\\" or \\"IMDB_Votes\\" is null\\n\\ndf_drop_subset = df.na.drop(\\"any\\", subset=[\\"IMDB_Rating\\", \\"IMDB_Votes\\"])\\ndf_drop_subset.show()\\n```\\n<details>\\n<summary>Output: You will see the rows with null values in `IMDB_Rating` or `IMDB_Votes` dropped.</summary>\\n```python\\n+-----------+----------+------------+----------------------+-------------------+--------------------+\\n|IMDB_Rating|IMDB_Votes|Release_Date|Rotten_Tomatoes_Rating|             Source|               Title|\\n+-----------+----------+------------+----------------------+-------------------+--------------------+\\n|        6.1|      1071|   12-Jun-98|                  NULL|               NULL|      The Land Girls|\\n|        6.9|       207|    7-Aug-98|                  NULL|               NULL|First Love, Last ...|\\n|        6.8|       865|   28-Aug-98|                  NULL|               NULL|I Married a Stran...|\\n|        5.8|      3275|    1-Jul-86|                    13|               NULL|             Pirates|\\n|        3.4|       165|    9-Oct-98|                    62|Original Screenplay|                Slam|\\n|        7.7|     15133|    4-Apr-99|                  NULL|               NULL|           Following|\\n|        3.8|       353|    9-Apr-99|                  NULL|Original Screenplay|             Foolish|\\n|        5.8|      3275|    1-Jul-86|                    25|               NULL|             Pirates|\\n|        7.0|      2906|   31-Dec-46|                    86|               NULL|     Duel in the Sun|\\n+-----------+----------+------------+----------------------+-------------------+--------------------+\\n```\\n</details>\\n\\n### Special Null functions\\n\\n1. **equal_null**: Returns `true` if both expressions are equal (including both being `NULL`), else `false`.  \\n2. **ifnull**: Returns the second expression if the first is `NULL`, otherwise returns the first. Same as `coalesce`. \\n3. **nvl**: Same as `ifnull`\u2014substitutes a `NULL` value with a provided fallback. Same as `coalesce`.\\n4. **nullif**: Returns `NULL` if both expressions match, otherwise returns the first expression.  \\n5. **nvl2**: Returns the second expression if the first is not `NULL`, otherwise returns the third.\\n\\n```python\\nspecial_nulls = df.selectExpr(\\n    \\"Title\\",\\n    \\"Rotten_Tomatoes_Rating\\",\\n    \\"IMDB_Rating\\",\\n    \\"equal_null(Rotten_Tomatoes_Rating, IMDB_Rating) as equal_null\\",\\n    \\"ifnull(Rotten_Tomatoes_Rating, IMDB_Rating * 10) as if_null\\", \\n    \\"nvl(Rotten_Tomatoes_Rating, IMDB_Rating * 10) as nvl\\",\\n    \\"nullif(Rotten_Tomatoes_Rating, IMDB_Rating * 10) as nullif\\",\\n    \\"nvl2(Rotten_Tomatoes_Rating, IMDB_Rating * 10, 0.0)\\" # if (first != null) second else third\\n)\\n\\nspecial_nulls.show(10, False)\\n```\\n<details>\\n<summary>Output: You will see the special null functions applied to the columns.</summary>\\n```\\n+--------------------------+----------------------+-----------+----------+-------+----+------+----+\\n|Title                     |Rotten_Tomatoes_Rating|IMDB_Rating|equal_null|if_null|nvl |nullif|nvl2|\\n+--------------------------+----------------------+-----------+----------+-------+----+------+----+\\n|The Land Girls            |NULL                  |6.1        |false     |61.0   |61.0|NULL  |0.0 |\\n|First Love, Last Rites    |NULL                  |6.9        |false     |69.0   |69.0|NULL  |0.0 |\\n|I Married a Strange Person|NULL                  |6.8        |false     |68.0   |68.0|NULL  |0.0 |\\n|Pirates                   |13                    |5.8        |false     |13     |13  |13    |58.0|\\n|Slam                      |62                    |3.4        |false     |62     |62  |62    |34.0|\\n|Mississippi Mermaid       |NULL                  |NULL       |true      |NULL   |NULL|NULL  |0.0 |\\n|Following                 |NULL                  |7.7        |false     |77.0   |77.0|NULL  |0.0 |\\n|Foolish                   |NULL                  |3.8        |false     |38.0   |38.0|NULL  |0.0 |\\n|Pirates                   |25                    |5.8        |false     |25     |25  |25    |58.0|\\n|Duel in the Sun           |86                    |7.0        |false     |86     |86  |86    |70.0|\\n+--------------------------+----------------------+-----------+----------+-------+----+------+----+\\n```\\n</details>\\n\\n## Conclusion\\n\\nPutting all together - \\n\\n| **Function / Operation**                            | **One-Line Explanation**                                                                                       |\\n|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\\n| **coalesce(expr1, expr2, \u2026)**                       | Returns the first non-null expression in the list.                                                             |\\n| **isNull(column)**                                   | Checks if the column\u2019s value is null, returning a boolean.                                                     |\\n| **isNotNull(column)**                                | Checks if the column\u2019s value is not null, returning a boolean.                                                 |\\n| **eqNullSafe(col1, col2)** or **col1 `<=>` col2**      | Returns `true` if `col1` equals `col2` or both are null, otherwise `false`.                                    |\\n| **ifnull(expr1, expr2)**                            | Returns `expr2` if `expr1` is null, otherwise returns `expr1`.                                                |\\n| **nvl(expr1, expr2)**                               | Same as `ifnull`: substitutes `expr2` when `expr1` is null.                                                    |\\n| **nullif(expr1, expr2)**                            | Returns `NULL` if `expr1` equals `expr2`, otherwise returns `expr1`.                                           |\\n| **nvl2(expr1, expr2, expr3)**                       | Returns `expr2` if `expr1` is not null, otherwise returns `expr3`.                                             |\\n| **fillna / na.fill(value[, subset])**               | Replaces null values in specified columns (or all) with a given value.                                         |\\n| **dropna / na.drop([how, subset])**                 | Removes rows containing null values (based on any/all columns or a subset).                                    |\\n| **asc_nulls_first / asc_nulls_last** (when sorting) | Orders rows ascending, placing nulls either first or last.                                                     |\\n| **desc_nulls_first / desc_nulls_last** (when sorting)| Orders rows descending, placing nulls either first or last.                                                    |\\n\\nI hope you enjoyed reading this blog on handling nulls in Spark, if you are interested in more such content, do check out [Apache Spark](/blog/tags/apache-spark). series.\\n\\nIf you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"columns-and-expressions","metadata":{"permalink":"/blog/columns-and-expressions","source":"@site/blog/2025-01-10-columns-and-experssions/index.md","title":"Columns and Expressions","description":"Apache Spark\'s Column and Expression play a big\u2002role in making your pipeline more efficient. In this\u2002blog we will look into ALL the possible ways to select columns, use built-in functions and perform calculations with column objects and expressions in PySpark. So, whether you build an ETL pipeline or doing exploratory data analysis, these techniques methods will come in handy.","date":"2025-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":3.8,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"columns-and-expressions","title":"Columns and Expressions","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","columns and expressions"],"hide_table_of_contents":false,"image":"assets/col_n_exps.png"},"unlisted":false,"prevItem":{"title":"Handling Nulls in Spark","permalink":"/blog/handling-nulls-in-spark"},"nextItem":{"title":"Introduction to Apache Spark","permalink":"/blog/spark-basics"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nApache Spark\'s `Column and Expression` play a big\u2002role in making your pipeline more efficient. In this\u2002blog we will look into ALL the possible ways to select columns, use built-in functions and perform calculations with column objects and expressions in PySpark. So, whether you build an ETL pipeline or doing exploratory data analysis, these techniques methods will come in handy.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Columns and Experssions\\n\\nIn Spark, columns are not actual data; they are logical constructs\u2014formulas, basically\u2014that say how to calculate a value for every row in\u2002a table. These formulas by themselves\u2002don\u2019t contain any real data, just return real values when they are applied to rows in a DataFrame. For this reason,\u2002columns can\u2019t exist in isolation \u2014 you need a DataFrame and some of its rows for a column\u2019s formula to be evaluated. Therefore, the creation or manipulation\u2002of columns should always occur by transforming on a DataFrame instead of on the column alone.\\n\\n### Ways of Constructing the Column\\n\\nThere are many ways to construct and refer to columns but the simplest ways are by using the `col, column functions or expr`. `col(), column() or expr` are from package `pyspark.sql.functions`. Find the syntax below:\\n\\n```python\\nfrom pyspark.sql.functions import col, column, expr\\ncol(\\"columnName\\")\\ncolumn(\\"columnName\\")\\nexpr(\\"columnName\\")\\n```\\n\\n### Selecting Columns\\n\\nYou can select columns from a DataFrame using the `select()` method. the `select()` takes a list of column objects or expressions.\\n\\n#### Setting up the Spark Session and dummy data\\n\\n```python\\n\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, column, expr\\n\\nspark_session = (\\n    SparkSession\\n    .builder\\n    .appName(\\"ColumnsAndExpressionsDemo\\")\\n    .config(\\"spark.master\\", \\"local\\")\\n    .getOrCreate()\\n)\\n\\ndata = [\\n    {\\"Name\\": \\"car1\\", \\"Horsepower\\": 130, \\"Acceleration\\": 12.0, \\"Weight_in_lbs\\": 3504, \\"Year\\": 1970},\\n    {\\"Name\\": \\"car2\\", \\"Horsepower\\": 165, \\"Acceleration\\": 11.5, \\"Weight_in_lbs\\": 3693, \\"Year\\": 1970},\\n    {\\"Name\\": \\"car3\\", \\"Horsepower\\": 150, \\"Acceleration\\": 10.5, \\"Weight_in_lbs\\": 3436, \\"Year\\": 1970},\\n    {\\"Name\\": \\"car4\\", \\"Horsepower\\": 150, \\"Acceleration\\": 10.0, \\"Weight_in_lbs\\": 3761, \\"Year\\": 1970},\\n    {\\"Name\\": \\"car5\\", \\"Horsepower\\": 140, \\"Acceleration\\": 9.0,  \\"Weight_in_lbs\\": 3200, \\"Year\\": 1971},\\n    {\\"Name\\": \\"car6\\", \\"Horsepower\\": 198, \\"Acceleration\\": 8.5,  \\"Weight_in_lbs\\": 4341, \\"Year\\": 1971},\\n    {\\"Name\\": \\"car7\\", \\"Horsepower\\": 220, \\"Acceleration\\": 8.0,  \\"Weight_in_lbs\\": 4354, \\"Year\\": 1971},\\n    {\\"Name\\": \\"car8\\", \\"Horsepower\\": 215, \\"Acceleration\\": 7.5,  \\"Weight_in_lbs\\": 4312, \\"Year\\": 1972},\\n    {\\"Name\\": \\"car9\\", \\"Horsepower\\": 225, \\"Acceleration\\": 7.7,  \\"Weight_in_lbs\\": 4425, \\"Year\\": 1972},\\n    {\\"Name\\": \\"car10\\",\\"Horsepower\\": 190, \\"Acceleration\\": 9.5,  \\"Weight_in_lbs\\": 3850, \\"Year\\": 1972},\\n]\\n\\ncars_df = spark_session.createDataFrame(data)\\ncars_df.show()\\n```\\n\\n#### Selecting Columns using `col` and `select`\\n\\n```python\\nfirst_column = col(\\"Name\\")  # return Column object\\n\\ncars_df.select(\\n    first_column, # using Column object\\n    col(\\"Acceleration\\"),\\n    column(\\"Weight_in_lbs\\"),\\n    cars_df.Weight_in_lbs, # using dot (.) notation\\n    \'Horsepower\',       # using string-based column reference\\n    expr(\'Year\')        # using Spark SQL expression\\n).show(10)\\n```\\n\\n- <Highlight color=\\"#3e6980\\">col(\\"columnName\\") and column(\\"columnName\\")</Highlight> are functionally equivalent. They create a Column object that you can pass to DataFrame transformations.\\n- You can also refer to columns directly by their <Highlight color=\\"#3e6980\\">string name (\'Horsepower\')</Highlight>.\\n- <Highlight color=\\"#3e6980\\">expr(\\"Year\\")</Highlight> showcases how you can mix SQL expressions right within the select statement.\\n\\n#### Performing Calculations with Columns\\n\\n```python\\nfrom pyspark.sql.column import Column\\n\\nsimple_expression: Column = col(\\"Weight_in_lbs\\")\\nweight_in_kgs_expression: Column = col(\\"Weight_in_lbs\\") / 2.2 # performing arithmetic on columns\\n\\ncars_df.select(\\n    col(\\"Name\\"),\\n    col(\\"Weight_in_lbs\\"),\\n    weight_in_kgs_expression.alias(\\"Weight_in_kgs\\")\\n).show(5)\\n\\n```\\n\\n1. Spark allows you to perform arithmetic on columns just like regular Python variables.\\n2. alias(\\"Weight_in_kgs\\") labels your computed column for clarity in the output.\\n\\n#### Using `expr` to perform calculations\\n\\nAn expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset.\\n\\nIn the simplest case, an expression, created via the `expr function`, is just a DataFrame column reference. In the simplest case, expr(\\"someCol\\") is equivalent to col(\\"someCol\\").\\n\\n```python\\n\\nprint(\\"Select with expr...\\")\\ncars_df.select(\\n    col(\\"Name\\"),\\n    col(\\"Weight_in_lbs\\"),\\n    expr(\\"Weight_in_lbs / 2.2\\") # using expr to perform arithmetic\\n).show(5)\\n\\n```\\n\\n#### Using `selectExpr` \\n\\nThe `select method` when you\u2019re working with columns or expressions, and the `selectExpr method` when you\u2019re working with expressions in strings. \\n\\n```python\\n\\ncars_df.selectExpr(\\n    \'Name\',\\n    \'Weight_in_lbs\',\\n    \'Weight_in_lbs / 2.2\'\\n).show(5)\\n\\n```\\n\\n## Conclusion\\n\\nChoosing between `col()`, `column()`, `expr()`, or even raw string references comes down to personal preference, readability, and complexity of your transformations. For simple column references, col() is often the most straightforward. However, if you prefer writing SQL-like expressions directly in your code or need complex SQL functions, `expr()` and `selectExpr()` provide the flexibility you need.\\n\\n---\\nI hope this blog helped you understand the use of `Columns and Expressions` in Apache Spark. If you are interested in more such content, do check out [Apache Spark](/blog/tags/apache-spark). series.\\n\\nIf you have any questions or feedback, feel free to reach out to me on \\n<SocialLinks />"},{"id":"spark-basics","metadata":{"permalink":"/blog/spark-basics","source":"@site/blog/2025-01-01-spark-basics/index.md","title":"Introduction to Apache Spark","description":"Welcome to my Apache Spark series! I\u2019ll dive deep into Apache Spark, from basics to advanced concepts. This series is about learning, exploring, and sharing\u2014documenting my journey to mastering Apache Spark ( again ) while sharing insights, challenges, and tips.","date":"2025-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":4.42,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-basics","title":"Introduction to Apache Spark","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","Apache spark basics"],"hide_table_of_contents":false,"image":"assets/spark-basics/spark-basics.png"},"unlisted":false,"prevItem":{"title":"Columns and Expressions","permalink":"/blog/columns-and-expressions"},"nextItem":{"title":"Data Modelling - Fact vs Dimension","permalink":"/blog/de-bootcamp-fact-vs-dimension"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nWelcome to my [Apache Spark](/blog/tags/apache-spark) series! I\u2019ll dive deep into Apache Spark, from basics to advanced concepts. This series is about learning, exploring, and sharing\u2014documenting my journey to mastering Apache Spark ( again ) while sharing insights, challenges, and tips.\\n\\nIn this first post, we\u2019ll cover the fundamentals of Apache Spark, its history, and why it\u2019s a game-changer in data engineering.\\n\\nFind all the blogs in the series [here](/blog/tags/apache-spark).\\n\x3c!-- truncate --\x3e\\n\\n## Spark History\\nApache Spark started as a research project by [Matei Zaharia](https://www.linkedin.com/in/mateizaharia/) at the University of California, Berkeley\'s AMPLab in 2009 which focuses on big data analytics. The project was open-sourced in 2010, initially focused on MapReduce computing with a faster API.\\n\\nThe aim was to design a programming model that supports a much wider class of applications than MapReduce, while maintaining its automatic fault tolerance.\\n\\n### Motivation for Apache Spark\\nAt the time, MapReduce was the de facto standard for big data processing, but it had notable limitations:\\n\\n- **Inefficiency for Complex Applications**: MapReduce required multiple disk I/O operations, as each intermediate computation step wrote data to disk, significantly slowing down iterative tasks like machine learning algorithms or graph processing.\\n- **Rigid Programming Model**: Developers were constrained to writing programs in the Map and Reduce paradigm, making it less intuitive for complex workflows or real-time analytics.\\n- **Lack of In-Memory Processing**: MapReduce did not leverage in-memory computation effectively, leading to unnecessary overhead in cases where intermediate data could be reused.\\n\\nThese shortcomings paved the way for **Apache Spark**, designed to address these inefficiencies with its distributed in-memory computation framework. Spark introduced a more flexible programming model and faster execution capabilities, making it ideal for iterative algorithms and large-scale data processing.\\n\\n## What is Apache Spark?\\n\\nApache Spark is an <Highlight color=\\"#3e6980\\">open-source, unified computing engine and libraries for distributed data processing</Highlight>.\\n\\n- **Unified Computing Engine**\\n    - Spark provides a variety of data processing tasks like data loading, SQL queries, machine learning, and graph processing in a single framework etc.\\n    - It is consistent and composable API in multiple languages like Scala, Java, Python, and R.\\n    - It can optimizations across different libraries and workloads.\\n    - Computing Engine: Spark is completely detached from where the data resides and data is being fetched\\n\\n- **Distributed Data Processing**: \\n    - Spark is designed to process large volumes of data in parallel across a cluster of machines.\\n\\n:::warning\\n\\nSpark is not a database, it is a distributed computing engine that can process large volumes of data in parallel across a cluster of machines.\\n\\n:::\\n\\n## Why Apache Spark?\\n\\nSpark is designed to enhance the capabilities of Hadoop, particularly for applications involving <Highlight color=\\"#3e6980\\">iterative jobs and interactive analytics </Highlight> where data reuse is crucial. \\n\\n* **Superior Performance for Iterative Tasks:** Spark excels in iterative jobs common in machine learning. It <Highlight color=\\"#3e6980\\">caches data in memory across iterations</Highlight>, eliminating the need to reload from disk for each cycle, resulting in substantial performance gains over Hadoop.\\n\\n* **Enabling Interactive Data Exploration:** Spark empowers users with <Highlight color=\\"#3e6980\\">sub-second response times for interactive queries on large datasets</Highlight>. Unlike Hadoop, which incurs significant latency for each query, Spark provides an experience comparable to working with data locally. \\n\\n* **Resilience and Fault Tolerance:** Spark leverages the concept of Resilient Distributed Datasets (RDDs). RDDs track data lineage, allowing for <Highlight color=\\"#3e6980\\">efficient reconstruction of lost partitions in case of node failures</Highlight> without relying on resource-intensive checkpointing.\\n\\n* **Efficient Data Sharing:** Spark offers broadcast variables, enabling the distribution of large read-only datasets across worker nodes only once. This contrasts with Hadoop\'s distributed cache, which is limited to single jobs, resulting in reduced data transfer overheads. \\n\\n* **Simplified Programming Model:** Spark\'s abstractions like RDDs, shared variables, and parallel operations offer a <Highlight color=\\"#3e6980\\">higher-level, more user-friendly programming model compared to Hadoop</Highlight>. Its integration with languages like Scala promotes interactive development and simplifies the expression of complex computations. \\n\\n:::info\\n\\nSpark is not a part of the Hadoop ecosystem, but it can run on top of Hadoop YARN, providing a more efficient and flexible alternative to MapReduce for data processing.\\n\\n:::\\n\\n## Spark Ecosystem\\n\\n| **Component**       | **Purpose**                                | **Key Features**                                                                 | **Use Cases**                                   |\\n|----------------------|--------------------------------------------|----------------------------------------------------------------------------------|------------------------------------------------|\\n| **Spark Core**       | General-purpose distributed computing.     | Task scheduling, memory management, fault recovery, and interaction with storage.| Foundation for all other Spark components.     |\\n| **Spark SQL**        | Structured data processing.                | SQL queries, DataFrames/Datasets, Hive integration, and schema management.       | Batch processing, data warehousing, ETL tasks. |\\n| **Spark Streaming**  | Real-time data processing.                 | Processes data in micro-batches, supports Kafka, Flume, and file streams.        | Real-time dashboards, log monitoring.          |\\n| **MLlib**            | Machine learning library.                  | Algorithms for classification, regression, clustering, and recommendation.       | Predictive analytics, recommendation engines.  |\\n| **GraphX**           | Graph processing and computation.          | API for graph-parallel computations, pre-built graph algorithms.                 | Social network analysis, PageRank.             |\\n| **SparkR**           | R-based analytics in Spark.                | R frontend for statistical analysis and machine learning using Spark.            | Data exploration, statistical modeling.        |\\n\\n![Spark Ecosystem](assets/spark-ecosystem.png)\\n\\n## References\\n\\n1. [Spark + AI Summit 2020 - Matei Zaharia Keynote](https://youtu.be/OLJKIogf2nU?si=m0l_73WADiwmL4I0)\\n2. [Apache Spark Documentation](https://spark.apache.org/docs/latest/index.html)\\n3. [Wiki - Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark)\\n\\nI hope you enjoyed reading this blog on Apache Spark introduction . If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-fact-vs-dimension","metadata":{"permalink":"/blog/de-bootcamp-fact-vs-dimension","source":"@site/blog/2024-12-19-de-bootcamp-fact-vs-dimension/index.md","title":"Data Modelling - Fact vs Dimension","description":"I\'m sharing my learnings from the Data Engineering Bootcamp, where we are currently focusing on Fact vs Dimension.","date":"2024-12-19T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"fact-dimensional-modeling","permalink":"/blog/tags/fact","description":"Fact Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"}],"readingTime":2.595,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-fact-vs-dimension","title":"Data Modelling - Fact vs Dimension","authors":["me"],"tags":["de","fact","de-bootcamp"],"keywords":["data engineering","data engineering bootcamp","data modelling","fact data modelling"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Introduction to Apache Spark","permalink":"/blog/spark-basics"},"nextItem":{"title":"Data Modelling - Fact Modelling","permalink":"/blog/de-bootcamp-fact-modelling"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nI\'m sharing my learnings from the Data Engineering Bootcamp, where we are currently focusing on Fact vs Dimension.\\n\\n| Resource | Link |\\n|----------|------|\\n| DataExpert.io | [DataExpert.io](https://bootcamp.techcreator.io/lessons) |\\n| Zach Wilson on LinkedIn | [LinkedIn](https://www.linkedin.com/in/eczachly/) |\\n\\nThank you, Zach, for your invaluable guidance and this comprehensive bootcamp!\\n\\n---\\n\\nWeek-2, Day-2: Fact vs Dimension\\n\\n\x3c!-- truncate --\x3e\\n## Fact vs Dimension\\n\\n- The examples of `dim is active` and `dim is activated`, both user dimensions on Facebook. \\n- `Dim is active` is based on whether a user has engaged with the app in a given timeframe. This could be considered an aggregation of facts (e.g. likes, comments, shares), potentially making it difficult to categorise as purely a dimension. \\n- In contrast, `dim is activated` indicates whether a user has explicitly deactivated their account. This is a pure dimension, as it\'s an attribute of the user object, independent of their actions within the app.\\n\\n### Bucketisation\\n*   **Necessity for Meaningful Analysis:** When creating dimensions based on user activity (e.g., number of likes), bucketisation becomes crucial to avoid excessively high cardinality that would lead to groups of one, rendering analysis less meaningful.\\n*   **Data Distribution Awareness:** The choice of buckets shouldn\'t be arbitrary. Instead, it should be informed by the data\'s statistical distribution. Examining percentiles, quartiles, or quintiles can help define meaningful bucket ranges.\\n*   **Impact on Compression and Flexibility:** Bucketisation can improve data compression by reducing the number of unique values. However, it comes at the expense of flexibility, as pre-defined buckets may limit the types of analyses possible.\\n*   **Avoiding Arbitrary Buckets:** The author cautions against arbitrarily choosing bucket ranges without considering the underlying data distribution, as it can lead to misleading analyses and weaken the credibility of the derived insights.\\n*   **Stakeholder Involvement:** When defining buckets for dimensions that could impact business decisions, it\'s crucial to involve relevant stakeholders to ensure alignment and minimise the need for future changes, which can be costly and time-consuming.\\n\\nZach also provides examples where bucketisation plays a significant role:\\n\\n*   **Facebook\'s \\"Dim is active\\":** While not explicitly bucketised, this dimension demonstrates the aggregation of user actions (facts) into a broader category. A more refined approach could involve bucketising users into activity levels (e.g., low, medium, high) based on their engagement metrics.\\n*   **Airbnb superhosts:** Determining superhost status involves evaluating multiple criteria and potentially bucketising hosts based on their performance across these dimensions. This illustrates how bucketisation can create a meaningful dimension that reflects a collection of behaviours.\\n\\n## Properties of Fact and Dimension tables\\n\\n### Dimensions\\n- Usually show up in `GROUP BY` clause when doing analytics.\\n- Can be \\"high cardinality\\" (e.g., user_id) or \\"low cardinality\\" (e.g., region).\\n- Generally come from a snapshot of state.\\n\\n### Facts\\n- Usually aggregated (e.g., SUM, COUNT, AVG) in analytics.\\n- Almost always higher volume that dimensions, although some fact sources are low-volume, think \\"rare events\\".\\n- Generally are events and logs.\\n\\nI hope you enjoyed reading this blog on Facts vs Dimensions. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-fact-modelling","metadata":{"permalink":"/blog/de-bootcamp-fact-modelling","source":"@site/blog/2024-12-14-de-bootcamp-fact-modelling/index.md","title":"Data Modelling - Fact Modelling","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering.","date":"2024-12-14T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"fact-dimensional-modeling","permalink":"/blog/tags/fact","description":"Fact Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"}],"readingTime":6,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-fact-modelling","title":"Data Modelling - Fact Modelling","authors":["me"],"tags":["de","fact","de-bootcamp"],"keywords":["data engineering","data engineering bootcamp","data modelling","fact data modelling"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Fact vs Dimension","permalink":"/blog/de-bootcamp-fact-vs-dimension"},"nextItem":{"title":"Data Modelling - Graph Databases and Additve Dimensions","permalink":"/blog/de-bootcamp-graph-databases"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. \\nToday we are learning about Fact Modelling.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nWeek-2, Day-1: Fact Data Modeling\\n\\n\x3c!-- truncate --\x3e\\n:::caution\\nFact data is the biggest data you work as data engineer. Zach shares, he worked 2PB of data in a day at Netflix !!\\n:::\\n\\n## Fact Data Modelling\\n\\n### What is a fact?\\n\\nFact can be thought as a record of an event that happened or occured. \\n- A user logs in to an app ( an action )\\n- A transaction is made\\n- You run a mile with your smartwatch - ( a mile can be a aggregated, considering each step in that mile as a granular)\\n\\n:::info\\nFact cannot be broken down further. It is the most granular data you have. ( this is way you think about fact )\\n:::\\n\\n<Highlight color=\\"#3e6980\\">Facts are not slowly changing</Highlight>, which makes them easier to model than dimensions.\\n\\n### Hardest part of modelling facts\\n\\n- Fact data is usually 10-100x bigger than dimension data.\\n- Fact data can need a lot of context for effective analysis like which dimension is it related to etc.\\n- Facts would have duplicate data which is way more common than dimensions.\\n\\n### How does fact data work?\\n\\n- Normalized facts dont have any dimesnional attributes, just IDs to join to get that information.\\n- Denormalized facts bring in some dimension attributes to make it quicket analysis at the cost of storage.\\n\\n:::info\\nNormalised facts works well for a small scale, you would remove the duplicate facts and data integrity is achieved.\\n:::\\n\\n### How does fact modelling work?\\n\\nHere\u2019s a concise table highlighting the differences between **fact data** and **raw log data**, as described in the slide:\\n\\n| **Aspect**           | **Raw Logs**                                                                                  | **Fact Data**                                   |\\n|-----------------------|-----------------------------------------------------------------------------------------------|------------------------------------------------|\\n| **Schema Design**     | Ugly schemas designed for online systems, making data analysis difficult.                     | Well-structured with nice column names.        |\\n| **Data Quality**      | May contain duplicates and other quality issues.                                              | Includes quality guarantees like uniqueness, non-null constraints, etc. |\\n| **Retention**         | Usually has shorter retention.                                                                | Retained for longer periods.                   |\\n\\nThough Fact data and Raw logs are dependent, they are not the same. Fact data is a subset of raw logs, and it is the data that is used for analysis.\\n\\nFact can be identified as `Who`, `What`, `When`, `Where`, `How` of the data.\\n\\n- **Who** - User ID ( user who did the action )\\n- **Where** - Location ID ( where the action happened ), most likely modeled out like who with \\"IDs\\" to join, but more likely to bring in dimensions.\\n- **How** - Similiar to where, \\"He used an iphone to make this click\\"\\n- **When** - Timestamp ( when the action happened ). fundamentally part of the nature of the fact.\\n    - Mostly an \\"event_timestamp\\" field or \\"event_date\\" field. ( always to be not null )\\n- **What** - The action that happened. ( what is the fact ) ( always to be not null )\\n    - In notification world, it could be \\"notification_sent\\", \\"notification_delivered\\", \\"notification_clicked\\"\\n\\n- Fact datasets should have quality guarantees like uniqueness, non-null constraints, etc.\\n- Fact data < raw logs\\n- Fact data should parse out hard-to-understand columns. \\n- Expected to have the simple data types, in some cases, there can be complex data types \\n\\n### How logging fit into fact data?\\n\\n- It brings in all the crtical context for your fact data\\n- Do not log everything, log what you need\\n- Logging should conform to values specified by the online teams, define the standard schema for logging\\n    - Thrift was used at Netflix and Airbnb\\n\\n### Potential options when working with high volume fact data\\n\\n- **Sampling**: This involves analysing a subset of the data, which can be significantly faster and require less storage, especially for gauging trends or directionality. However, sampling is unsuitable for situations like security analysis, where capturing rare events is crucial.\\n\\n- **Bucketing**: This involves dividing the data into smaller partitions based on a key, like user ID. Bucketing can speed up joins, especially when employing techniques like bucket joins or sorted merge bucket joins (SMB joins) that minimise or eliminate shuffle.\\n\\n### Retention of Fact Data\\n\\n- High volumes make fact data much more costly to store.\\n- Any fact tables < 10 TBs, Retention is not a big deal.\\n    - Anonymisation of facts usually happens after 60-90 days, the data would be moved to a new table the PII data would be removed.\\n- Fact tables > 100 TBs, very short retention is common. (~ 14 days)\\n\\n### Deduplication of Fact Data\\n\\nAs duplicate records are much more common in fact datasets compared to dimensional data. These duplicates can arise from various sources, such as:\\n\\n*   **Data quality errors:** Software bugs in logging systems can lead to duplicate entries every time an event occurs.\\n*   **Genuine duplicate actions:** Users might perform the same action multiple times within a given timeframe, resulting in multiple legitimate entries that need to be accounted for without inflating metrics. For example, a user might click on a notification multiple times, or a step-tracking app might record multiple steps in quick succession. \\n\\n**Deduplication is crucial for accurate analysis**, as failing to address duplicates can distort metrics like click-through rates or user engagement. For example, if duplicates aren\'t removed from notification click data, the click-through rate might appear artificially inflated. \\n\\nThe suggestion here is to consider the **timeframe** for deduplication.  While it\'s essential to remove duplicates within a specific period where they significantly impact analysis, duplicates occurring over a longer timeframe might be less critical. For instance, a user clicking on a notification a year after initially clicking on it might not be relevant for measuring short-term engagement. \\n\\nTwo approaches to efficiently handle deduplication for high-volume fact data:\\n\\n*   **Streaming:** \\n    - This method processes data in real time, deduplicating records as they arrive.\\n    - Windowing matters in streaming, you need to have a window to deduplicate the data.\\n    - Entire day deduplication is not possible in streaming, because it needs to hold onto such a big window of memory.\\n    - Large number of duplicates happens within a short time of first event.\\n    - **Deduplication window** - 15 minutes, a sweet spot\\n*   **Microbatch processing:** \\n    - This technique involves processing data in small batches, such as hourly, to deduplicate records within each batch and subsequently merge the deduplicated batches. \\n    - There is a specific microbatch deduplication pattern involving hourly aggregation followed by a series of full outer joins to merge deduplicated data from different hours.\\n\\nThe choice between streaming and microbatch processing depends on factors like latency requirements and the complexity of the deduplication logic. \\n\\nI hope you enjoyed reading this blog on Fact Data Modelling. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-graph-databases","metadata":{"permalink":"/blog/de-bootcamp-graph-databases","source":"@site/blog/2024-12-05-de-bootcamp-graph-databases/index.md","title":"Data Modelling - Graph Databases and Additve Dimensions","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering.","date":"2024-12-05T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"graph-database","permalink":"/blog/tags/graph-database","description":"Graph Database tag description"}],"readingTime":4.955,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-graph-databases","title":"Data Modelling - Graph Databases and Additve Dimensions","authors":["me"],"tags":["de","dim","de-bootcamp","graph-database"],"keywords":["data engineering","data engineering bootcamp","data modelling","graph database"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Fact Modelling","permalink":"/blog/de-bootcamp-fact-modelling"},"nextItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. \\nToday we are learning about Data Modelling - Graph Databases\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay - 3: Data Modeling: Graph Databases\\n\\n\x3c!-- truncate --\x3e\\n\\n## Additive vs non-additive dimensions\\n\\n### What makes a dimension additive\\n\\n- A additive dimension is a dimension that can be summed up across all the dimensions in the fact table.\\nFor Example: You manage a sales database that tracks daily transactions. \\nYou want to analyze the <Highlight color=\\"#3e6980\\">total revenue generated</Highlight> by your business.\\nThe Revenue measure is an additive dimension because it can be summed across all levels of the data (e.g., by day, month, region, product, etc.).\\n\\n- Non-additive dimensions are dimensions that cannot be summed up across all the dimensions in the fact table.\\nFor Example: Application interface is not additive.\\nThe number of active users != # of active users on web + # of active users on mobile (Android + iOS).\\n\\n    Why is this?\\n    Overlapping Users: A single user can be active on multiple platforms (e.g., logging in on both web and mobile during the same time period). Simply summing active users across platforms would `double-count` those users.\\n\\n:::important\\nA deminsion is additive over a specific window of time, if and only if, the grain of data over that window can\\nonly ever be one value at a time.\\n:::\\n\\n### How additivity help?\\n\\n1. **No need for COUNT(DISTINCT)**: When a measure is additive, you can aggregate it (e.g., SUM) across dimensions without needing a COUNT(DISTINCT) to avoid duplicates.\\n\\n2. **Additivity Applies Mostly to SUM:**\\n   - Non-additive dimensions (like active users) are usually problematic only when using `COUNT` because you might double-count. However, **SUM** aggregations typically remain additive.\\n\\n:::note\\nCan the dimension be 2 things at once? If yes, it\'s non-additive.\\n:::\\n\\n## All about ENUMS\\n\\n### What is an ENUM type?\\nENUM is a data type that contains a fixed set of values.\\n\\n### When should you use ENUMs?\\n- Enums are great for low-to-medium cardinality columns.\\n- Country is a great example of where Enums start to struggle.\\n\\n:::info\\nEnums are great if the set of value count is < 50. \\n:::\\n\\n### Why use ENUMs?\\n\\n- Built in data quality:\\n    - IF you model a column as ENUM, you can be sure that the data in that column is one of the values in the ENUM. if the data is not in the ENUM, the pipeline will fail.\\n- Built in static fields:\\n    - If you have a column that is not going to change, you can use ENUMs.\\n    - Example: Employee Type (Full Time, Part Time, Contractor)\\n- Built in documentation:\\n    - You would know already what the values are, it is self documented.\\n\\nIn Postgres you can create ENUMs like this:\\n```sql\\nCREATE TYPE employee_type AS ENUM (\'Full Time\', \'Part Time\', \'Contractor\');\\n```\\n\\n### Enumerations and sup partitions\\n\\n- Enumerations make it easy to create sup partitions, because:\\n    - You can be sure that the data in the column is one of the values in the ENUM.\\n    - They chunk up the big data problem into manageable pieces.\\n\\n**For example:** Deduping the Notification channel (Email, SMS, Push) is defined as ENUM. You can process the data with date and notification channel in parallel.\\n\\n- The [little book of pipelines](https://github.com/EcZachly/little-book-of-pipelines/tree/master) by Zach:\\n    - The **Little Book of Enums** is a design pattern for organizing and managing complex data pipelines with multiple sources. It groups similar data sources together, defines a shared schema for consistency, and uses enums to document metadata, data quality rules, and constants for each group. \\n    - How is this little book generated?\\n        - The **ENUMs** are typically defined in Python or Scala to represent the metadata.  \\n        - A dedicated job is used to convert the enumerated list into a table.  \\n        - The resulting table is small, as it only contains as many entries as there are ENUMs.  \\n        - This table can then be used to share metadata between data quality (DQ) checks (passed as a table) and source functions (passed as Python objects) by joining them as needed.\\n    - What type of use cases is this ENUM pattern useful for?\\n        - Whenever you have many sources mapping to a shared schema.\\n        - Airbnb\\n            - Unit Economics (fees, coupons, credits, insurance, infrastructure cost, taxes etc)\\n    - How do you model data from disparate sources into a shared schema?\\n        - Flexible schema: Using map\\n\\n### Flexible schema\\n\\n- Benefits\\n    - No need to run ALTER TABLE each time a new column is added.\\n    - Easier management of numerous columns.\\n    - Schemas are not cluttered with many \\"NULL\\" columns.\\n    - \\"Other_properties\\" columns are excellent for \\"rarely-used-but-needed\\" data.\\n- Drawbacks\\n    - Typically results in poorer compression.\\n    - Can affect readability and queryability.\\n\\n## Graph data modeling\\n\\n### What is a graph database?\\nGraph databases are a type of NoSQL database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data.\\n\\nGraph modelling is RELATIONSHIP focused, not entity focused. There would be chances of poor job at modelling the entities.\\n\\n\\nUsually the model look like this for entities(vertices):\\n\\n```\\n- Identifier: STRING\\n- Type: STRING\\n- Properties: MAP<STRING,STRING>\\n```\\n:::warning\\nWe dont care about the entities, we care about the relationships.\\n:::\\n\\nThe schema for edges would look like this:\\n```\\n- subject_identifier: STRING\\n- subject_type: VertexType\\n- object_identifier: STRING\\n- object_type: VertexType\\n- edge_type: EdgeType (always a verb: is_a, place_with, has_a etc)\\n- Properties: MAP<STRING,STRING>\\n``` \\n\\nI hope you enjoyed reading this blog on Graph Data Modelling. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-dimensional-modelling-scd","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling-scd","source":"@site/blog/2024-12-01-de-bootcamp-dimensional-modelling-scd/index.md","title":"Dimensional Modelling - SCD","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.","date":"2024-12-01T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"slowly-changing-dimension","permalink":"/blog/tags/scd","description":"Slowly Changing Dimension tag description"}],"readingTime":5.245,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling-scd","title":"Dimensional Modelling - SCD","authors":["me"],"tags":["de","dim","de-bootcamp","scd"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","slowly changing dimensions"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Graph Databases and Additve Dimensions","permalink":"/blog/de-bootcamp-graph-databases"},"nextItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-2: Dimensional Modelling - Slowly Changing Dimensions (Theory)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is an Idempotent Pipeline?\\n> Idempotency: An idempotent pipeline ensures that processing the same data multiple times (whether it\'s live production data or backfill data) yields the same result without any unintended side effects. This consistency is crucial for reliability and simplicity in data workflows.\\n\\nPipelines should produce the same results:\\n1. The day they are run\\n2. How many times they are run\\n3. What hour they are run\\n\\n### Why is hard to troubleshoot non-idempotent pipelines?\\n1. Silent Failure: The non-idempotent pipelines can produce different results every time they are run, it doesnt fail. This results in <Highlight color=\\"#3e6980\\">non reproduceable results</Highlight>.\\n2. This causes inconsistency in the data to the downstream systems too.\\n\\n### What can make a pipline non-idempotent?\\n1. `INSERT INTO` without `TRUNCATE`:\\n    1. If you dont `TRUNCATE` data before INSERT, it will keep on adding the data to the table, creating duplicates. \\n    2. Thus your pipeline will not be idempotent, as it will keep on adding the data to the table.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">MERGE</Highlight> or <Highlight color=\\"#3e6980\\">INSERT OVERWRITE</Highlight> always.\\n2. Using `start_date >` without a corresponding `end_date <`:\\n    1. If you dont have `end_date` in your SCD, you will not be able to track the changes in the data.\\n    2. This can create a out of memory exceptions, when you backfill the data.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">end_date</Highlight> in your SCD always.\\n3. Not using a full set of partition sensors:\\n    1. Your pipeline may run with partial set of inputs.\\n    2. pipeline might run when there is no/partial data\\n4. Not using `depends_on_past` for cumulative pipelines:\\n    1. When you process the data in parallel ( lets say processing parallel days ), pipeline may end up processing yesterday\'s data, which hasnt been created yet.\\n\\n:::info\\nThe beauty of an idempotent pipeline lies in its ability to produce consistent results for both production and backfill data.\\n:::\\n\\n**Example:**\\nImagine a pipeline that:\\n1. Reads data (e.g., transactions).\\n2. Cleans and deduplicates it.\\n3. Stores the cleaned data in a database.\\n\\n- **Production:** As new transactions come in, they\u2019re cleaned, deduplicated, and stored.\\n- **Backfill:** When historical transactions are added, they go through the same cleaning and deduplication process, ensuring no duplicate records and consistent results.\\n\\nBecause the pipeline is idempotent:\\n- Processing the same data again won\u2019t alter the results or create issues.\\n- Whether it\'s production or backfill data, the behavior remains identical.\\n\\n### Few more problems that can make a pipeline non-idempotent\\n\\n1. Relying on the **latest** partition of a not properly modeled SCD table.\\n    - Cumulative table design AMPLIFIES this bug.\\n2. One exception relying on latest partition is when you have properly modeled SCD table and <Highlight color=\\"#3e8045\\">you are backfilling not in production</Highlight>.\\n\\n### The pains of not having an idempotent pipeline\\n\\n- Backfilling causes inconsistencies between the old and restated data.\\n- Hard to troubleshoot bugs and fix the issues.\\n- Unit testing cannot replicate the production behavior.\\n- The pipeline doesnt fail, but the data is inconsistent, which is silent failures.\\n\\n## Slowly Changing Dimensions (SCD)\\n\\nThe Slowly Changing Dimensions (SCD) are dimensions that change slowly over time, which would have time frame with them. For example,\\n    1. Customer Address\\n    2. Product Description\\n    3. Employee Details\\n    4. etc.\\n\\n:::warning\\nNot modelling the SCDs properly can impact the idempotency.\\n:::\\n\\n:::tip\\nThe slower the changes in a Slowly Changing Dimension (SCD), the more effectively it can be modeled.\\n:::\\n\\n### How can you model dimensions that change over time?\\n\\n- Singular snapshot: The dimension table only contains the most recent data.\\n    - Be Careful: This pipelines are not idempotent.\\n- Daily partitioned snapshots: The dimension table contains a snapshot of the data for each day.\\n    - Very simple way of implementing SCD.\\n- SCD Types 1,2,3\\n\\n### Types of Slowly Changing Dimensions\\n\\nHere\u2019s a Markdown table summarizing the content provided:\\n\\n| **Type**  | **Description**                                                                                             | **Key Features**                                                                                      | **Limitations**                                                                                   |\\n|-----------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\\n| **Type 0** | Static dimensions, not actually slowly changing.                                                           | - No temporal columns. <br /> - Table contains only identifier and value.                              | - No changes tracked or maintained.                                                              |\\n| **Type 1** | Overwrites old data with new data, only latest data is retained.                                           | - Simple and efficient. <br /> - No history maintained.                                                | - Cannot retrieve historical data.                                                               |\\n| **Type 2** | Gold standard of Slowly Changing Dimensions, maintains history using temporal columns.                     | - Uses `start_date` and `end_date`. <br /> - Current rows have `end_date = NULL` or `9999-12-31`. <br /> - Optionally uses `is_current` column. | - More than one row per dimension can complicate filtering and querying.                         |\\n| **Type 3** | Maintains only two versions of the data: `original` and `current`.                                         | - One row per dimension.                                                                             | - Loses history of changes between `original` and `current`. <br /> - Partially idempotent.        |\\n\\n\\n### Which types are idempotent?\\n\\n| Type      | Idempotent | Reason                                                                                           |\\n|-----------|------------|--------------------------------------------------------------------------------------------------|\\n| Type 0 | Yes        | The values are unchanging.                                                                       |\\n| Type 2 | Yes        | It is idempotent, but you need to be careful with how you use the `start_date` and `end_date`.   |\\n| Type 1 | No         | Backfilling with this dataset gives the dimension as it is now, not as it was before.           |\\n| Type 3 | No         | Backfilling makes it impossible to determine whether to pick \\"original\\" or \\"current.\\"           |\\n\\n### Loading SCD2\\n\\n- Load the entire history in one query\\n    - Inefficient but nimble\\n- Incremnetally load the data after the previoud SCD is generated\\n    - Has the same `depends_on_past`. its efficient but cumbersome.\\n\\nI hope you enjoyed reading this blog on Slowly Changing Dimensions. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-dimensional-modelling","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling","source":"@site/blog/2024-11-29-de-bootcamp-dimensional-modelling/index.md","title":"Dimensional Modelling - Cumulative Table","description":"This page is about Dimensional Modelling - Cumulative Table.","date":"2024-11-29T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"cumulative-table","permalink":"/blog/tags/cumulativetable","description":"Cumulative Table tag description"}],"readingTime":8.975,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling","title":"Dimensional Modelling - Cumulative Table","authors":["me"],"tags":["de","dim","de-bootcamp","cumulativetable"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","cumulative table"],"hide_table_of_contents":false,"image":"assets/img/dimension_modelling.png","description":"This page is about Dimensional Modelling - Cumulative Table."},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome-to-my-blog"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Cumulative Table.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-1: Dimensional Modelling (Theory)\\n\\n## What is a Dimension?\\n\\nDimension are `attributes` of an entity ( example: user\'s name, age etc)\\n    - Dimensions may be `IDENTIFY` an entity, which would be a unique identifier for the entity (example: user_id, device_id etc)\\n    - Dimensions may be `DESCRIPTIVE` which would be the attributes of the entity (example: user\'s name,age etc) but wont be unique.\\n\\n\x3c!-- truncate --\x3e\\n\\nDimensions comes in two flavors:\\n    - `Slowly Changing Dimensions` : Dimensions that change over time like user\'s address, phone number etc. This attribute is time dependent. Can be painful deal with these attributes.\\n    - `Fixed`: Dimensions that do not change over time like user\'s birthdate. These are easy to deal with.\\n\\n:::info\\nKnowing whether the attributes are slowly changing or fixed is important to design the data model.\\n:::\\n## Knowing your data consumer\\n\\n`Who is using your data?` is the most important question to ask before designing the data model. The data model should be designed in such a way that it is easy for the data consumer to understand the data.\\n\\nWhether the data is getting delivered to:\\n1. Data Analyst / Data Scientist: \\n    - They would be interested in the data that is `easy to understand and analyze`.\\n    - They wouldnt be interested in dealing with `complex data types` like JSON, XML etc.\\n2. Data Engineers:\\n    - Sometimes downstream systems would be handled by Data Engineers, to take your data sets and join with other data to achieve the buisness logic, so the data model be compact and probably harder to query.\\n    - Nested types are fine to model the data.\\n    - Usually, as we get further up in ladder of data engineering, there would be `Master Data` which would be used by other data engineers or teams, to join the master data with other data sets.\\n3. ML models:\\n    - Generally machine learning models would need `identifier` and bunch of flattened decimal columns like `feature value columns` etc.\\n    - Sometimes it depends on the model, the model can sometime handle the complex data structures, similiar to what Data analyst and data scientist would need.\\n4. Customers\\n    - You would never give the data directly ( like JSON, CSVs etc) to the customers, you would always give them the `dashboards` or `reports` which would be easy to understand and analyze.\\n\\n> Not knowing your data consumer, there is a high chance that the data model would be designed in a wrong way, can lead to business loss.\\n\\n## OLTP Vs OLAP Vs Master Data\\n\\n1. OLTP(Online Transaction Processing):\\n    - OLTP is the system that is used to record the transactions that are happening in the system.\\n    - The data model is optimized for `low latency` and `low-volume` queries, usually dealing with `single record` at a time.\\n    - Usually Software Engineers model the data in OLTP.\\n    - The data model in OLTP is `3NF` (Third Normal Form) which means the data is broken down into multiple tables and the data is not repeated.\\n    - Databases: MySQL, Postgres etc.\\n2. OLAP(Online Analytical Processing):\\n    - OLAP is the system that is used to analyze the data that is stored in the OLTP system.\\n    - The data model is optimized for `high volume` and `complex queries`, using `aggregations` and `joins`.\\n    - Usually `Data Engineers` model the data in OLAP.\\n    - Analytical Databases: Redshift, BigQuery etc.\\n3. Master Data:\\n    - Master Data is optimied for completeness of entity definition and deduped.\\n    - Master Data is usually a middle layer between OLTP and OLAP.\\n\\n## Impacts of Mismatched Data Model\\n:::info\\n Mismatching needs of the data consumer == Less business value\\n:::\\n\\n- When you model transaction system as analytical system, like transactional system usually need only one record at a time, but if you model it as analytical system, you would retrieve all the unnecessary data which would be a performance hit.\\n- When you model analytical system as transactional system, like analytical system usually need to aggregate the data, but if you model it as transactional system, you would have to do the aggregation/joins on the fly which would be a performance hit.\\n- Having the Master Data as middle layer, would give you the flexibility whichever data model you want to use.\\n\\n## OLTP and OLAP is a continuum\\n\\n![data-flow](data-flow.png)\\n\\n- `Production Database Snapshot`: Usually the data in Production databases are transactional data, which is used to record the transactions that are happening in the system/web application/mobile application etc.\\n- `Master Data`: Taking the production datasets and creating the master data, for easy to understanding. Instead of querying \\"many\\" transactional databases, merging them into master data would be much efficient.\\n- `OLAP Cubes`: \\n    - You would flatten the data, like you might have multiple rows per entity, where you can aggregate the data.\\n    - OLAP Cubes space is also create the slice and dice the data.\\n- `Metrics`: You would aggregate even further to get the one number which would be like `KPIs` of the business.\\n\\n## Cumulative Table Design\\n\\n- Some days not every user would be active, but still master data should hold the details of the user.\\n- Cumulative table is the table that holds the `history of the data`.\\n\\n*The way cumulative table is designed is*:\\n- You would have 2 datasets like `today` and `yesterday`.\\n- `Full Outer Join` is done on the datasets, to get all the data, pulling the history data every day.\\n- `Coalesce` is used to get the latest data.\\n\\n*Where is it used?*\\n- `Growth Analysis`: To see how the data is growing over time.\\n- `State transition tracking`: To see how the data is changing over time.\\n\\n:::warning\\n- Cumulative table gets bigger and bigger over time, so applying the filtering on the data is important.\\n:::\\n\\n\\n*Cumulative Table: Strengths and Drawbacks*\\n\\n| **Category**  | **Strengths**                                                                 | **Drawbacks**                                                                                           |\\n|----------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\\n| **Analytics** | - Enables historical analysis without shuffling or reprocessing the data.    | - Sequential backfilling can limit flexibility when correcting or reprocessing older data.             |\\n| **Tracking**  | - Easy to track changes and transitions over time.                           | - Incremental growth of data size over time, leading to storage and performance challenges.             |\\n| **Data**      | - Provides a complete snapshot of trends and transitions.                   | - Retains potentially outdated or deleted PII data, complicating compliance with data privacy laws.     |\\n\\n## The compactness vs usability tradeoff\\n\\n- The most usable tables usually are\\n    - They are straightforward, they would have identifier for dimensional model.\\n    - Have no complex data types.\\n    - Easily can be manipulated with `WHERE` and `GROUP BY` clauses.\\n- The most compact tables usually are\\n    - Are compressed to be as small as possible, which would be harder to query directly until they\'re decoded.\\n\\n- The middle-ground table:\\n    - Where you would use complex data types (e.g. ARRAY, STRUCT, MAP) to model the data, which would be harder to query directly but can be used to join with other data sets.\\n\\n*When would you each type of table?*\\n- Most compact:\\n    - Online systems where latency and data volume are critical. \\n    - Consumers are usually hihgly technical and can handle complex data types.\\n- Most usable:\\n    - When analytics is the main consumer\\n    - The majority of the consumer are less technical and need the data to be easy to understand and analyze.\\n- Middle-ground:\\n    - Upstream staging/master data are used by majority of consumers who are other data engineers.\\n\\n## Struct Vs Array Vs Map\\n\\n| **Feature**           | **Struct**                                        | **Map**                                      | **Array**                                    |\\n|------------------------|--------------------------------------------------|---------------------------------------------|---------------------------------------------|\\n| **Definition**         | A table within a table.                          | A key-value pair structure.                 | A list of values.                           |\\n| **Keys**              | Rigidly defined, improving compression.          | Flexible, less compression efficiency.      | Not applicable (index-based).              |\\n| **Values**            | Can be of any type.                              | Must all be of the same type.               | Must all be of the same type.               |\\n| **Usage**             | Suitable for structured data with predefined keys.| Best for flexible key-value relationships.  | Ideal for ordered collections of data.     |\\n| **Complexity**         | Higher due to nested structure.                  | Simpler, dynamic key-value management.      | Simplest among the three; index-based access. |\\n| **Nesting**           | Supports nesting within itself or arrays.         | Can be nested inside structs or arrays.     | Can contain structs or maps as elements.   |\\n\\n## Impact of adding a temporal (time-based) dimension\\n\\n1. **Cardinality Explosion**:\\n   - Adding a time dimension (e.g., days, nights, or timestamps) increases the number of unique combinations in the dataset. This often grows by at least **one order of magnitude** (10x or more).\\n   - Example: Airbnb has ~6 million listings. If we want to track the nightly availability and pricing for each listing for a year, it results in:\\n     - **365 days \xd7 6 million listings = ~2 billion rows** of data.\\n\\n2. **Challenges with Dataset Design**:\\n   - **Option 1**: You could store this data as a single listing with an array (list) of 365 nights. This approach is compact but may be harder to query efficiently.\\n   - **Option 2**: Alternatively, store it as 2 billion rows, where each row represents one listing-night combination. While this structure is easier to query, it increases the dataset size significantly.\\n\\n3. **Using the Parquet**:\\n   - Using efficient file formats like **Parquet**, which compresses data well, helps to reduce the size of both approaches. With proper sorting and organization, you can minimize storage overhead.\\n\\n:::warning\\nBadness of denormolized temporal dimension: if you have to join the data with other dimensions, spark shuffle will ruin the compression.\\n:::\\n\\n## Run length encoding compression\\n\\n- [Run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)  is a form of lossless data compression in which runs of data (consecutive occurrences of the same data value) are stored as a single occurrence of that data value and a count of its consecutive occurrences, rather than as the original run. <sup>[1]</sup>\\n- RLE is one of reasons why Parquet is so efficient for storing data.\\n- However, **shuffle can ruin this efficiency**. Since shuffle happens in a distributed environment during JOIN and GROUP BY operations, data is redistributed across nodes, and compression benefits like RLE are often lost.\\n\\nI hope you enjoyed reading this blog on Dimensional Modelling. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />\\n---\\n[1]: source from wikipedia"},{"id":"welcome-to-my-blog","metadata":{"permalink":"/blog/welcome-to-my-blog","source":"@site/blog/2024-11-26-welcome-to-my-blog/index.md","title":"Welcome","description":"Welcome dear readers...","date":"2024-11-26T00:00:00.000Z","tags":[{"inline":false,"label":"Welcome post","permalink":"/blog/tags/welcome","description":"Welcome post"}],"readingTime":1.06,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"welcome-to-my-blog","title":"Welcome","authors":["me"],"tags":["hola"],"keywords":["blog","welcome"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nWelcome dear readers...\\n\\nIm Vibhavari Bellutagi, from India, currently staying in France. I\'m a Data Engineer and I got sometime to channelise my thoughts and experiences in the form of blog. I\'m a big fan of `Learn by Doing`, so you will see most of the posts are practical oriented.\\n\\n\x3c!-- truncate --\x3e\\n\\nI always wanted to have a blog to share my thoughts and experiences with the data engineering world. Im beginner in the world of blogging, so Im still learning how to write a good and effective content. I hope you enjoy my blog and find it useful.\\n\\nI have three different sections in my website:\\n\\n1. [Blogs](index.md): Where I would long format technical blogs that I have experienced or learned recently.\\n2. [Tech Bytes](/TechBytes/intro): Where I would post small technical tips and tricks that I have learned.\\n4. [From First Principles](/FirstPrinciples/thoughts): Where I would post the concepts that I have learned from the scratch going to the depth of it.\\n\\nHope I will be able to keep up with the content and make it useful for you, im striving to make it better everyday.\\n\\nThanks for visiting my blog. Hope you enjoy it. Feel free to reach out to me on \\n<SocialLinks />.\\n\\nCheers,\\nVibhavari"}]}}')}}]);