"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"spark-basics","metadata":{"permalink":"/blog/spark-basics","source":"@site/blog/2025-01-01-spark-basics/index.md","title":"Introduction to Apache Spark","description":"Welcome to my Apache Spark series! I\u2019ll dive deep into Apache Spark, from basics to advanced concepts. This series is about learning, exploring, and sharing\u2014documenting my journey to mastering Apache Spark ( again ) while sharing insights, challenges, and tips.","date":"2025-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"}],"readingTime":4.42,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"spark-basics","title":"Introduction to Apache Spark","authors":["me"],"tags":["de","apache-spark","de-bootcamp"],"keywords":["data engineering","apache spark"],"hide_table_of_contents":false,"image":"assets/spark-basics/spark-basics.png"},"unlisted":false,"nextItem":{"title":"Data Modelling - Fact vs Dimension","permalink":"/blog/de-bootcamp-fact-vs-dimension"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nWelcome to my **Apache Spark** series! I\u2019ll dive deep into Apache Spark, from basics to advanced concepts. This series is about learning, exploring, and sharing\u2014documenting my journey to mastering Apache Spark ( again ) while sharing insights, challenges, and tips.\\n\\nIn this first post, we\u2019ll cover the fundamentals of Apache Spark, its history, and why it\u2019s a game-changer in data engineering.\\n\\nFind all the blogs in the series [here](/blog/tags/apache-spark).\\n\x3c!-- truncate --\x3e\\n\\n## Spark History\\nApache Spark started as a research project by [Matei Zaharia](https://www.linkedin.com/in/mateizaharia/) at the University of California, Berkeley\'s AMPLab in 2009 which focuses on big data analytics. The project was open-sourced in 2010, initially focused on MapReduce computing with a faster API.\\n\\nThe aim was to design a programming model that supports a much wider class of applications than MapReduce, while maintaining its automatic fault tolerance.\\n\\n### Motivation for Apache Spark\\nAt the time, MapReduce was the de facto standard for big data processing, but it had notable limitations:\\n\\n- **Inefficiency for Complex Applications**: MapReduce required multiple disk I/O operations, as each intermediate computation step wrote data to disk, significantly slowing down iterative tasks like machine learning algorithms or graph processing.\\n- **Rigid Programming Model**: Developers were constrained to writing programs in the Map and Reduce paradigm, making it less intuitive for complex workflows or real-time analytics.\\n- **Lack of In-Memory Processing**: MapReduce did not leverage in-memory computation effectively, leading to unnecessary overhead in cases where intermediate data could be reused.\\n\\nThese shortcomings paved the way for **Apache Spark**, designed to address these inefficiencies with its distributed in-memory computation framework. Spark introduced a more flexible programming model and faster execution capabilities, making it ideal for iterative algorithms and large-scale data processing.\\n\\n## What is Apache Spark?\\n\\nApache Spark is an <Highlight color=\\"#3e6980\\">open-source, unified computing engine and libraries for distributed data processing</Highlight>.\\n\\n- **Unified Computing Engine**\\n    - Spark provides a variety of data processing tasks like data loading, SQL queries, machine learning, and graph processing in a single framework etc.\\n    - It is consistent and composable API in multiple languages like Scala, Java, Python, and R.\\n    - It can optimizations across different libraries and workloads.\\n    - Computing Engine: Spark is completely detached from where the data resides and data is being fetched\\n\\n- **Distributed Data Processing**: \\n    - Spark is designed to process large volumes of data in parallel across a cluster of machines.\\n\\n:::warning\\n\\nSpark is not a database, it is a distributed computing engine that can process large volumes of data in parallel across a cluster of machines.\\n\\n:::\\n\\n## Why Apache Spark?\\n\\nSpark is designed to enhance the capabilities of Hadoop, particularly for applications involving <Highlight color=\\"#3e6980\\">iterative jobs and interactive analytics </Highlight> where data reuse is crucial. \\n\\n* **Superior Performance for Iterative Tasks:** Spark excels in iterative jobs common in machine learning. It <Highlight color=\\"#3e6980\\">caches data in memory across iterations</Highlight>, eliminating the need to reload from disk for each cycle, resulting in substantial performance gains over Hadoop.\\n\\n* **Enabling Interactive Data Exploration:** Spark empowers users with <Highlight color=\\"#3e6980\\">sub-second response times for interactive queries on large datasets</Highlight>. Unlike Hadoop, which incurs significant latency for each query, Spark provides an experience comparable to working with data locally. \\n\\n* **Resilience and Fault Tolerance:** Spark leverages the concept of Resilient Distributed Datasets (RDDs). RDDs track data lineage, allowing for <Highlight color=\\"#3e6980\\">efficient reconstruction of lost partitions in case of node failures</Highlight> without relying on resource-intensive checkpointing.\\n\\n* **Efficient Data Sharing:** Spark offers broadcast variables, enabling the distribution of large read-only datasets across worker nodes only once. This contrasts with Hadoop\'s distributed cache, which is limited to single jobs, resulting in reduced data transfer overheads. \\n\\n* **Simplified Programming Model:** Spark\'s abstractions like RDDs, shared variables, and parallel operations offer a <Highlight color=\\"#3e6980\\">higher-level, more user-friendly programming model compared to Hadoop</Highlight>. Its integration with languages like Scala promotes interactive development and simplifies the expression of complex computations. \\n\\n:::info\\n\\nSpark is not a part of the Hadoop ecosystem, but it can run on top of Hadoop YARN, providing a more efficient and flexible alternative to MapReduce for data processing.\\n\\n:::\\n\\n## Spark Ecosystem\\n\\n| **Component**       | **Purpose**                                | **Key Features**                                                                 | **Use Cases**                                   |\\n|----------------------|--------------------------------------------|----------------------------------------------------------------------------------|------------------------------------------------|\\n| **Spark Core**       | General-purpose distributed computing.     | Task scheduling, memory management, fault recovery, and interaction with storage.| Foundation for all other Spark components.     |\\n| **Spark SQL**        | Structured data processing.                | SQL queries, DataFrames/Datasets, Hive integration, and schema management.       | Batch processing, data warehousing, ETL tasks. |\\n| **Spark Streaming**  | Real-time data processing.                 | Processes data in micro-batches, supports Kafka, Flume, and file streams.        | Real-time dashboards, log monitoring.          |\\n| **MLlib**            | Machine learning library.                  | Algorithms for classification, regression, clustering, and recommendation.       | Predictive analytics, recommendation engines.  |\\n| **GraphX**           | Graph processing and computation.          | API for graph-parallel computations, pre-built graph algorithms.                 | Social network analysis, PageRank.             |\\n| **SparkR**           | R-based analytics in Spark.                | R frontend for statistical analysis and machine learning using Spark.            | Data exploration, statistical modeling.        |\\n\\n![Spark Ecosystem](assets/spark-ecosystem.png)\\n\\n## References\\n\\n1. [Spark + AI Summit 2020 - Matei Zaharia Keynote](https://youtu.be/OLJKIogf2nU?si=m0l_73WADiwmL4I0)\\n2. [Apache Spark Documentation](https://spark.apache.org/docs/latest/index.html)\\n3. [Wiki - Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark)\\n\\nI hope you enjoyed reading this blog on Apache Spark introduction . If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-fact-vs-dimension","metadata":{"permalink":"/blog/de-bootcamp-fact-vs-dimension","source":"@site/blog/2024-12-19-de-bootcamp-fact-vs-dimension/index.md","title":"Data Modelling - Fact vs Dimension","description":"I\'m sharing my learnings from the Data Engineering Bootcamp, where we are currently focusing on Fact vs Dimension.","date":"2024-12-19T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"fact-dimensional-modeling","permalink":"/blog/tags/fact","description":"Fact Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"}],"readingTime":2.595,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-fact-vs-dimension","title":"Data Modelling - Fact vs Dimension","authors":["me"],"tags":["de","fact","de-bootcamp"],"keywords":["data engineering","data engineering bootcamp","data modelling","fact data modelling"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Introduction to Apache Spark","permalink":"/blog/spark-basics"},"nextItem":{"title":"Data Modelling - Fact Modelling","permalink":"/blog/de-bootcamp-fact-modelling"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nI\'m sharing my learnings from the Data Engineering Bootcamp, where we are currently focusing on Fact vs Dimension.\\n\\n| Resource | Link |\\n|----------|------|\\n| DataExpert.io | [DataExpert.io](https://bootcamp.techcreator.io/lessons) |\\n| Zach Wilson on LinkedIn | [LinkedIn](https://www.linkedin.com/in/eczachly/) |\\n\\nThank you, Zach, for your invaluable guidance and this comprehensive bootcamp!\\n\\n---\\n\\nWeek-2, Day-2: Fact vs Dimension\\n\\n\x3c!-- truncate --\x3e\\n## Fact vs Dimension\\n\\n- The examples of `dim is active` and `dim is activated`, both user dimensions on Facebook. \\n- `Dim is active` is based on whether a user has engaged with the app in a given timeframe. This could be considered an aggregation of facts (e.g. likes, comments, shares), potentially making it difficult to categorise as purely a dimension. \\n- In contrast, `dim is activated` indicates whether a user has explicitly deactivated their account. This is a pure dimension, as it\'s an attribute of the user object, independent of their actions within the app.\\n\\n### Bucketisation\\n*   **Necessity for Meaningful Analysis:** When creating dimensions based on user activity (e.g., number of likes), bucketisation becomes crucial to avoid excessively high cardinality that would lead to groups of one, rendering analysis less meaningful.\\n*   **Data Distribution Awareness:** The choice of buckets shouldn\'t be arbitrary. Instead, it should be informed by the data\'s statistical distribution. Examining percentiles, quartiles, or quintiles can help define meaningful bucket ranges.\\n*   **Impact on Compression and Flexibility:** Bucketisation can improve data compression by reducing the number of unique values. However, it comes at the expense of flexibility, as pre-defined buckets may limit the types of analyses possible.\\n*   **Avoiding Arbitrary Buckets:** The author cautions against arbitrarily choosing bucket ranges without considering the underlying data distribution, as it can lead to misleading analyses and weaken the credibility of the derived insights.\\n*   **Stakeholder Involvement:** When defining buckets for dimensions that could impact business decisions, it\'s crucial to involve relevant stakeholders to ensure alignment and minimise the need for future changes, which can be costly and time-consuming.\\n\\nZach also provides examples where bucketisation plays a significant role:\\n\\n*   **Facebook\'s \\"Dim is active\\":** While not explicitly bucketised, this dimension demonstrates the aggregation of user actions (facts) into a broader category. A more refined approach could involve bucketising users into activity levels (e.g., low, medium, high) based on their engagement metrics.\\n*   **Airbnb superhosts:** Determining superhost status involves evaluating multiple criteria and potentially bucketising hosts based on their performance across these dimensions. This illustrates how bucketisation can create a meaningful dimension that reflects a collection of behaviours.\\n\\n## Properties of Fact and Dimension tables\\n\\n### Dimensions\\n- Usually show up in `GROUP BY` clause when doing analytics.\\n- Can be \\"high cardinality\\" (e.g., user_id) or \\"low cardinality\\" (e.g., region).\\n- Generally come from a snapshot of state.\\n\\n### Facts\\n- Usually aggregated (e.g., SUM, COUNT, AVG) in analytics.\\n- Almost always higher volume that dimensions, although some fact sources are low-volume, think \\"rare events\\".\\n- Generally are events and logs.\\n\\nI hope you enjoyed reading this blog on Facts vs Dimensions. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-fact-modelling","metadata":{"permalink":"/blog/de-bootcamp-fact-modelling","source":"@site/blog/2024-12-14-de-bootcamp-fact-modelling/index.md","title":"Data Modelling - Fact Modelling","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering.","date":"2024-12-14T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"fact-dimensional-modeling","permalink":"/blog/tags/fact","description":"Fact Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"}],"readingTime":6,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-fact-modelling","title":"Data Modelling - Fact Modelling","authors":["me"],"tags":["de","fact","de-bootcamp"],"keywords":["data engineering","data engineering bootcamp","data modelling","fact data modelling"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Fact vs Dimension","permalink":"/blog/de-bootcamp-fact-vs-dimension"},"nextItem":{"title":"Data Modelling - Graph Databases and Additve Dimensions","permalink":"/blog/de-bootcamp-graph-databases"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. \\nToday we are learning about Fact Modelling.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nWeek-2, Day-1: Fact Data Modeling\\n\\n\x3c!-- truncate --\x3e\\n:::caution\\nFact data is the biggest data you work as data engineer. Zach shares, he worked 2PB of data in a day at Netflix !!\\n:::\\n\\n## Fact Data Modelling\\n\\n### What is a fact?\\n\\nFact can be thought as a record of an event that happened or occured. \\n- A user logs in to an app ( an action )\\n- A transaction is made\\n- You run a mile with your smartwatch - ( a mile can be a aggregated, considering each step in that mile as a granular)\\n\\n:::info\\nFact cannot be broken down further. It is the most granular data you have. ( this is way you think about fact )\\n:::\\n\\n<Highlight color=\\"#3e6980\\">Facts are not slowly changing</Highlight>, which makes them easier to model than dimensions.\\n\\n### Hardest part of modelling facts\\n\\n- Fact data is usually 10-100x bigger than dimension data.\\n- Fact data can need a lot of context for effective analysis like which dimension is it related to etc.\\n- Facts would have duplicate data which is way more common than dimensions.\\n\\n### How does fact data work?\\n\\n- Normalized facts dont have any dimesnional attributes, just IDs to join to get that information.\\n- Denormalized facts bring in some dimension attributes to make it quicket analysis at the cost of storage.\\n\\n:::info\\nNormalised facts works well for a small scale, you would remove the duplicate facts and data integrity is achieved.\\n:::\\n\\n### How does fact modelling work?\\n\\nHere\u2019s a concise table highlighting the differences between **fact data** and **raw log data**, as described in the slide:\\n\\n| **Aspect**           | **Raw Logs**                                                                                  | **Fact Data**                                   |\\n|-----------------------|-----------------------------------------------------------------------------------------------|------------------------------------------------|\\n| **Schema Design**     | Ugly schemas designed for online systems, making data analysis difficult.                     | Well-structured with nice column names.        |\\n| **Data Quality**      | May contain duplicates and other quality issues.                                              | Includes quality guarantees like uniqueness, non-null constraints, etc. |\\n| **Retention**         | Usually has shorter retention.                                                                | Retained for longer periods.                   |\\n\\nThough Fact data and Raw logs are dependent, they are not the same. Fact data is a subset of raw logs, and it is the data that is used for analysis.\\n\\nFact can be identified as `Who`, `What`, `When`, `Where`, `How` of the data.\\n\\n- **Who** - User ID ( user who did the action )\\n- **Where** - Location ID ( where the action happened ), most likely modeled out like who with \\"IDs\\" to join, but more likely to bring in dimensions.\\n- **How** - Similiar to where, \\"He used an iphone to make this click\\"\\n- **When** - Timestamp ( when the action happened ). fundamentally part of the nature of the fact.\\n    - Mostly an \\"event_timestamp\\" field or \\"event_date\\" field. ( always to be not null )\\n- **What** - The action that happened. ( what is the fact ) ( always to be not null )\\n    - In notification world, it could be \\"notification_sent\\", \\"notification_delivered\\", \\"notification_clicked\\"\\n\\n- Fact datasets should have quality guarantees like uniqueness, non-null constraints, etc.\\n- Fact data < raw logs\\n- Fact data should parse out hard-to-understand columns. \\n- Expected to have the simple data types, in some cases, there can be complex data types \\n\\n### How logging fit into fact data?\\n\\n- It brings in all the crtical context for your fact data\\n- Do not log everything, log what you need\\n- Logging should conform to values specified by the online teams, define the standard schema for logging\\n    - Thrift was used at Netflix and Airbnb\\n\\n### Potential options when working with high volume fact data\\n\\n- **Sampling**: This involves analysing a subset of the data, which can be significantly faster and require less storage, especially for gauging trends or directionality. However, sampling is unsuitable for situations like security analysis, where capturing rare events is crucial.\\n\\n- **Bucketing**: This involves dividing the data into smaller partitions based on a key, like user ID. Bucketing can speed up joins, especially when employing techniques like bucket joins or sorted merge bucket joins (SMB joins) that minimise or eliminate shuffle.\\n\\n### Retention of Fact Data\\n\\n- High volumes make fact data much more costly to store.\\n- Any fact tables < 10 TBs, Retention is not a big deal.\\n    - Anonymisation of facts usually happens after 60-90 days, the data would be moved to a new table the PII data would be removed.\\n- Fact tables > 100 TBs, very short retention is common. (~ 14 days)\\n\\n### Deduplication of Fact Data\\n\\nAs duplicate records are much more common in fact datasets compared to dimensional data. These duplicates can arise from various sources, such as:\\n\\n*   **Data quality errors:** Software bugs in logging systems can lead to duplicate entries every time an event occurs.\\n*   **Genuine duplicate actions:** Users might perform the same action multiple times within a given timeframe, resulting in multiple legitimate entries that need to be accounted for without inflating metrics. For example, a user might click on a notification multiple times, or a step-tracking app might record multiple steps in quick succession. \\n\\n**Deduplication is crucial for accurate analysis**, as failing to address duplicates can distort metrics like click-through rates or user engagement. For example, if duplicates aren\'t removed from notification click data, the click-through rate might appear artificially inflated. \\n\\nThe suggestion here is to consider the **timeframe** for deduplication.  While it\'s essential to remove duplicates within a specific period where they significantly impact analysis, duplicates occurring over a longer timeframe might be less critical. For instance, a user clicking on a notification a year after initially clicking on it might not be relevant for measuring short-term engagement. \\n\\nTwo approaches to efficiently handle deduplication for high-volume fact data:\\n\\n*   **Streaming:** \\n    - This method processes data in real time, deduplicating records as they arrive.\\n    - Windowing matters in streaming, you need to have a window to deduplicate the data.\\n    - Entire day deduplication is not possible in streaming, because it needs to hold onto such a big window of memory.\\n    - Large number of duplicates happens within a short time of first event.\\n    - **Deduplication window** - 15 minutes, a sweet spot\\n*   **Microbatch processing:** \\n    - This technique involves processing data in small batches, such as hourly, to deduplicate records within each batch and subsequently merge the deduplicated batches. \\n    - There is a specific microbatch deduplication pattern involving hourly aggregation followed by a series of full outer joins to merge deduplicated data from different hours.\\n\\nThe choice between streaming and microbatch processing depends on factors like latency requirements and the complexity of the deduplication logic. \\n\\nI hope you enjoyed reading this blog on Fact Data Modelling. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-graph-databases","metadata":{"permalink":"/blog/de-bootcamp-graph-databases","source":"@site/blog/2024-12-05-de-bootcamp-graph-databases/index.md","title":"Data Modelling - Graph Databases and Additve Dimensions","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering.","date":"2024-12-05T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"graph-database","permalink":"/blog/tags/graph-database","description":"Graph Database tag description"}],"readingTime":4.955,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-graph-databases","title":"Data Modelling - Graph Databases and Additve Dimensions","authors":["me"],"tags":["de","dim","de-bootcamp","graph-database"],"keywords":["data engineering","data engineering bootcamp","data modelling","graph database"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Fact Modelling","permalink":"/blog/de-bootcamp-fact-modelling"},"nextItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. \\nToday we are learning about Data Modelling - Graph Databases\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay - 3: Data Modeling: Graph Databases\\n\\n\x3c!-- truncate --\x3e\\n\\n## Additive vs non-additive dimensions\\n\\n### What makes a dimension additive\\n\\n- A additive dimension is a dimension that can be summed up across all the dimensions in the fact table.\\nFor Example: You manage a sales database that tracks daily transactions. \\nYou want to analyze the <Highlight color=\\"#3e6980\\">total revenue generated</Highlight> by your business.\\nThe Revenue measure is an additive dimension because it can be summed across all levels of the data (e.g., by day, month, region, product, etc.).\\n\\n- Non-additive dimensions are dimensions that cannot be summed up across all the dimensions in the fact table.\\nFor Example: Application interface is not additive.\\nThe number of active users != # of active users on web + # of active users on mobile (Android + iOS).\\n\\n    Why is this?\\n    Overlapping Users: A single user can be active on multiple platforms (e.g., logging in on both web and mobile during the same time period). Simply summing active users across platforms would `double-count` those users.\\n\\n:::important\\nA deminsion is additive over a specific window of time, if and only if, the grain of data over that window can\\nonly ever be one value at a time.\\n:::\\n\\n### How additivity help?\\n\\n1. **No need for COUNT(DISTINCT)**: When a measure is additive, you can aggregate it (e.g., SUM) across dimensions without needing a COUNT(DISTINCT) to avoid duplicates.\\n\\n2. **Additivity Applies Mostly to SUM:**\\n   - Non-additive dimensions (like active users) are usually problematic only when using `COUNT` because you might double-count. However, **SUM** aggregations typically remain additive.\\n\\n:::note\\nCan the dimension be 2 things at once? If yes, it\'s non-additive.\\n:::\\n\\n## All about ENUMS\\n\\n### What is an ENUM type?\\nENUM is a data type that contains a fixed set of values.\\n\\n### When should you use ENUMs?\\n- Enums are great for low-to-medium cardinality columns.\\n- Country is a great example of where Enums start to struggle.\\n\\n:::info\\nEnums are great if the set of value count is < 50. \\n:::\\n\\n### Why use ENUMs?\\n\\n- Built in data quality:\\n    - IF you model a column as ENUM, you can be sure that the data in that column is one of the values in the ENUM. if the data is not in the ENUM, the pipeline will fail.\\n- Built in static fields:\\n    - If you have a column that is not going to change, you can use ENUMs.\\n    - Example: Employee Type (Full Time, Part Time, Contractor)\\n- Built in documentation:\\n    - You would know already what the values are, it is self documented.\\n\\nIn Postgres you can create ENUMs like this:\\n```sql\\nCREATE TYPE employee_type AS ENUM (\'Full Time\', \'Part Time\', \'Contractor\');\\n```\\n\\n### Enumerations and sup partitions\\n\\n- Enumerations make it easy to create sup partitions, because:\\n    - You can be sure that the data in the column is one of the values in the ENUM.\\n    - They chunk up the big data problem into manageable pieces.\\n\\n**For example:** Deduping the Notification channel (Email, SMS, Push) is defined as ENUM. You can process the data with date and notification channel in parallel.\\n\\n- The [little book of pipelines](https://github.com/EcZachly/little-book-of-pipelines/tree/master) by Zach:\\n    - The **Little Book of Enums** is a design pattern for organizing and managing complex data pipelines with multiple sources. It groups similar data sources together, defines a shared schema for consistency, and uses enums to document metadata, data quality rules, and constants for each group. \\n    - How is this little book generated?\\n        - The **ENUMs** are typically defined in Python or Scala to represent the metadata.  \\n        - A dedicated job is used to convert the enumerated list into a table.  \\n        - The resulting table is small, as it only contains as many entries as there are ENUMs.  \\n        - This table can then be used to share metadata between data quality (DQ) checks (passed as a table) and source functions (passed as Python objects) by joining them as needed.\\n    - What type of use cases is this ENUM pattern useful for?\\n        - Whenever you have many sources mapping to a shared schema.\\n        - Airbnb\\n            - Unit Economics (fees, coupons, credits, insurance, infrastructure cost, taxes etc)\\n    - How do you model data from disparate sources into a shared schema?\\n        - Flexible schema: Using map\\n\\n### Flexible schema\\n\\n- Benefits\\n    - No need to run ALTER TABLE each time a new column is added.\\n    - Easier management of numerous columns.\\n    - Schemas are not cluttered with many \\"NULL\\" columns.\\n    - \\"Other_properties\\" columns are excellent for \\"rarely-used-but-needed\\" data.\\n- Drawbacks\\n    - Typically results in poorer compression.\\n    - Can affect readability and queryability.\\n\\n## Graph data modeling\\n\\n### What is a graph database?\\nGraph databases are a type of NoSQL database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data.\\n\\nGraph modelling is RELATIONSHIP focused, not entity focused. There would be chances of poor job at modelling the entities.\\n\\n\\nUsually the model look like this for entities(vertices):\\n\\n```\\n- Identifier: STRING\\n- Type: STRING\\n- Properties: MAP<STRING,STRING>\\n```\\n:::warning\\nWe dont care about the entities, we care about the relationships.\\n:::\\n\\nThe schema for edges would look like this:\\n```\\n- subject_identifier: STRING\\n- subject_type: VertexType\\n- object_identifier: STRING\\n- object_type: VertexType\\n- edge_type: EdgeType (always a verb: is_a, place_with, has_a etc)\\n- Properties: MAP<STRING,STRING>\\n``` \\n\\nI hope you enjoyed reading this blog on Graph Data Modelling. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-dimensional-modelling-scd","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling-scd","source":"@site/blog/2024-12-01-de-bootcamp-dimensional-modelling-scd/index.md","title":"Dimensional Modelling - SCD","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.","date":"2024-12-01T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"slowly-changing-dimension","permalink":"/blog/tags/scd","description":"Slowly Changing Dimension tag description"}],"readingTime":5.245,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling-scd","title":"Dimensional Modelling - SCD","authors":["me"],"tags":["de","dim","de-bootcamp","scd"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","slowly changing dimensions"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Graph Databases and Additve Dimensions","permalink":"/blog/de-bootcamp-graph-databases"},"nextItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-2: Dimensional Modelling - Slowly Changing Dimensions (Theory)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is an Idempotent Pipeline?\\n> Idempotency: An idempotent pipeline ensures that processing the same data multiple times (whether it\'s live production data or backfill data) yields the same result without any unintended side effects. This consistency is crucial for reliability and simplicity in data workflows.\\n\\nPipelines should produce the same results:\\n1. The day they are run\\n2. How many times they are run\\n3. What hour they are run\\n\\n### Why is hard to troubleshoot non-idempotent pipelines?\\n1. Silent Failure: The non-idempotent pipelines can produce different results every time they are run, it doesnt fail. This results in <Highlight color=\\"#3e6980\\">non reproduceable results</Highlight>.\\n2. This causes inconsistency in the data to the downstream systems too.\\n\\n### What can make a pipline non-idempotent?\\n1. `INSERT INTO` without `TRUNCATE`:\\n    1. If you dont `TRUNCATE` data before INSERT, it will keep on adding the data to the table, creating duplicates. \\n    2. Thus your pipeline will not be idempotent, as it will keep on adding the data to the table.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">MERGE</Highlight> or <Highlight color=\\"#3e6980\\">INSERT OVERWRITE</Highlight> always.\\n2. Using `start_date >` without a corresponding `end_date <`:\\n    1. If you dont have `end_date` in your SCD, you will not be able to track the changes in the data.\\n    2. This can create a out of memory exceptions, when you backfill the data.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">end_date</Highlight> in your SCD always.\\n3. Not using a full set of partition sensors:\\n    1. Your pipeline may run with partial set of inputs.\\n    2. pipeline might run when there is no/partial data\\n4. Not using `depends_on_past` for cumulative pipelines:\\n    1. When you process the data in parallel ( lets say processing parallel days ), pipeline may end up processing yesterday\'s data, which hasnt been created yet.\\n\\n:::info\\nThe beauty of an idempotent pipeline lies in its ability to produce consistent results for both production and backfill data.\\n:::\\n\\n**Example:**\\nImagine a pipeline that:\\n1. Reads data (e.g., transactions).\\n2. Cleans and deduplicates it.\\n3. Stores the cleaned data in a database.\\n\\n- **Production:** As new transactions come in, they\u2019re cleaned, deduplicated, and stored.\\n- **Backfill:** When historical transactions are added, they go through the same cleaning and deduplication process, ensuring no duplicate records and consistent results.\\n\\nBecause the pipeline is idempotent:\\n- Processing the same data again won\u2019t alter the results or create issues.\\n- Whether it\'s production or backfill data, the behavior remains identical.\\n\\n### Few more problems that can make a pipeline non-idempotent\\n\\n1. Relying on the **latest** partition of a not properly modeled SCD table.\\n    - Cumulative table design AMPLIFIES this bug.\\n2. One exception relying on latest partition is when you have properly modeled SCD table and <Highlight color=\\"#3e8045\\">you are backfilling not in production</Highlight>.\\n\\n### The pains of not having an idempotent pipeline\\n\\n- Backfilling causes inconsistencies between the old and restated data.\\n- Hard to troubleshoot bugs and fix the issues.\\n- Unit testing cannot replicate the production behavior.\\n- The pipeline doesnt fail, but the data is inconsistent, which is silent failures.\\n\\n## Slowly Changing Dimensions (SCD)\\n\\nThe Slowly Changing Dimensions (SCD) are dimensions that change slowly over time, which would have time frame with them. For example,\\n    1. Customer Address\\n    2. Product Description\\n    3. Employee Details\\n    4. etc.\\n\\n:::warning\\nNot modelling the SCDs properly can impact the idempotency.\\n:::\\n\\n:::tip\\nThe slower the changes in a Slowly Changing Dimension (SCD), the more effectively it can be modeled.\\n:::\\n\\n### How can you model dimensions that change over time?\\n\\n- Singular snapshot: The dimension table only contains the most recent data.\\n    - Be Careful: This pipelines are not idempotent.\\n- Daily partitioned snapshots: The dimension table contains a snapshot of the data for each day.\\n    - Very simple way of implementing SCD.\\n- SCD Types 1,2,3\\n\\n### Types of Slowly Changing Dimensions\\n\\nHere\u2019s a Markdown table summarizing the content provided:\\n\\n| **Type**  | **Description**                                                                                             | **Key Features**                                                                                      | **Limitations**                                                                                   |\\n|-----------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\\n| **Type 0** | Static dimensions, not actually slowly changing.                                                           | - No temporal columns. <br /> - Table contains only identifier and value.                              | - No changes tracked or maintained.                                                              |\\n| **Type 1** | Overwrites old data with new data, only latest data is retained.                                           | - Simple and efficient. <br /> - No history maintained.                                                | - Cannot retrieve historical data.                                                               |\\n| **Type 2** | Gold standard of Slowly Changing Dimensions, maintains history using temporal columns.                     | - Uses `start_date` and `end_date`. <br /> - Current rows have `end_date = NULL` or `9999-12-31`. <br /> - Optionally uses `is_current` column. | - More than one row per dimension can complicate filtering and querying.                         |\\n| **Type 3** | Maintains only two versions of the data: `original` and `current`.                                         | - One row per dimension.                                                                             | - Loses history of changes between `original` and `current`. <br /> - Partially idempotent.        |\\n\\n\\n### Which types are idempotent?\\n\\n| Type      | Idempotent | Reason                                                                                           |\\n|-----------|------------|--------------------------------------------------------------------------------------------------|\\n| Type 0 | Yes        | The values are unchanging.                                                                       |\\n| Type 2 | Yes        | It is idempotent, but you need to be careful with how you use the `start_date` and `end_date`.   |\\n| Type 1 | No         | Backfilling with this dataset gives the dimension as it is now, not as it was before.           |\\n| Type 3 | No         | Backfilling makes it impossible to determine whether to pick \\"original\\" or \\"current.\\"           |\\n\\n### Loading SCD2\\n\\n- Load the entire history in one query\\n    - Inefficient but nimble\\n- Incremnetally load the data after the previoud SCD is generated\\n    - Has the same `depends_on_past`. its efficient but cumbersome.\\n\\nI hope you enjoyed reading this blog on Slowly Changing Dimensions. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />"},{"id":"de-bootcamp-dimensional-modelling","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling","source":"@site/blog/2024-11-29-de-bootcamp-dimensional-modelling/index.md","title":"Dimensional Modelling - Cumulative Table","description":"This page is about Dimensional Modelling - Cumulative Table.","date":"2024-11-29T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"cumulative-table","permalink":"/blog/tags/cumulativetable","description":"Cumulative Table tag description"}],"readingTime":8.975,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling","title":"Dimensional Modelling - Cumulative Table","authors":["me"],"tags":["de","dim","de-bootcamp","cumulativetable"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","cumulative table"],"hide_table_of_contents":false,"image":"assets/img/dimension_modelling.png","description":"This page is about Dimensional Modelling - Cumulative Table."},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome-to-my-blog"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nIm sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Cumulative Table.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\n\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-1: Dimensional Modelling (Theory)\\n\\n## What is a Dimension?\\n\\nDimension are `attributes` of an entity ( example: user\'s name, age etc)\\n    - Dimensions may be `IDENTIFY` an entity, which would be a unique identifier for the entity (example: user_id, device_id etc)\\n    - Dimensions may be `DESCRIPTIVE` which would be the attributes of the entity (example: user\'s name,age etc) but wont be unique.\\n\\n\x3c!-- truncate --\x3e\\n\\nDimensions comes in two flavors:\\n    - `Slowly Changing Dimensions` : Dimensions that change over time like user\'s address, phone number etc. This attribute is time dependent. Can be painful deal with these attributes.\\n    - `Fixed`: Dimensions that do not change over time like user\'s birthdate. These are easy to deal with.\\n\\n:::info\\nKnowing whether the attributes are slowly changing or fixed is important to design the data model.\\n:::\\n## Knowing your data consumer\\n\\n`Who is using your data?` is the most important question to ask before designing the data model. The data model should be designed in such a way that it is easy for the data consumer to understand the data.\\n\\nWhether the data is getting delivered to:\\n1. Data Analyst / Data Scientist: \\n    - They would be interested in the data that is `easy to understand and analyze`.\\n    - They wouldnt be interested in dealing with `complex data types` like JSON, XML etc.\\n2. Data Engineers:\\n    - Sometimes downstream systems would be handled by Data Engineers, to take your data sets and join with other data to achieve the buisness logic, so the data model be compact and probably harder to query.\\n    - Nested types are fine to model the data.\\n    - Usually, as we get further up in ladder of data engineering, there would be `Master Data` which would be used by other data engineers or teams, to join the master data with other data sets.\\n3. ML models:\\n    - Generally machine learning models would need `identifier` and bunch of flattened decimal columns like `feature value columns` etc.\\n    - Sometimes it depends on the model, the model can sometime handle the complex data structures, similiar to what Data analyst and data scientist would need.\\n4. Customers\\n    - You would never give the data directly ( like JSON, CSVs etc) to the customers, you would always give them the `dashboards` or `reports` which would be easy to understand and analyze.\\n\\n> Not knowing your data consumer, there is a high chance that the data model would be designed in a wrong way, can lead to business loss.\\n\\n## OLTP Vs OLAP Vs Master Data\\n\\n1. OLTP(Online Transaction Processing):\\n    - OLTP is the system that is used to record the transactions that are happening in the system.\\n    - The data model is optimized for `low latency` and `low-volume` queries, usually dealing with `single record` at a time.\\n    - Usually Software Engineers model the data in OLTP.\\n    - The data model in OLTP is `3NF` (Third Normal Form) which means the data is broken down into multiple tables and the data is not repeated.\\n    - Databases: MySQL, Postgres etc.\\n2. OLAP(Online Analytical Processing):\\n    - OLAP is the system that is used to analyze the data that is stored in the OLTP system.\\n    - The data model is optimized for `high volume` and `complex queries`, using `aggregations` and `joins`.\\n    - Usually `Data Engineers` model the data in OLAP.\\n    - Analytical Databases: Redshift, BigQuery etc.\\n3. Master Data:\\n    - Master Data is optimied for completeness of entity definition and deduped.\\n    - Master Data is usually a middle layer between OLTP and OLAP.\\n\\n## Impacts of Mismatched Data Model\\n:::info\\n Mismatching needs of the data consumer == Less business value\\n:::\\n\\n- When you model transaction system as analytical system, like transactional system usually need only one record at a time, but if you model it as analytical system, you would retrieve all the unnecessary data which would be a performance hit.\\n- When you model analytical system as transactional system, like analytical system usually need to aggregate the data, but if you model it as transactional system, you would have to do the aggregation/joins on the fly which would be a performance hit.\\n- Having the Master Data as middle layer, would give you the flexibility whichever data model you want to use.\\n\\n## OLTP and OLAP is a continuum\\n\\n![data-flow](data-flow.png)\\n\\n- `Production Database Snapshot`: Usually the data in Production databases are transactional data, which is used to record the transactions that are happening in the system/web application/mobile application etc.\\n- `Master Data`: Taking the production datasets and creating the master data, for easy to understanding. Instead of querying \\"many\\" transactional databases, merging them into master data would be much efficient.\\n- `OLAP Cubes`: \\n    - You would flatten the data, like you might have multiple rows per entity, where you can aggregate the data.\\n    - OLAP Cubes space is also create the slice and dice the data.\\n- `Metrics`: You would aggregate even further to get the one number which would be like `KPIs` of the business.\\n\\n## Cumulative Table Design\\n\\n- Some days not every user would be active, but still master data should hold the details of the user.\\n- Cumulative table is the table that holds the `history of the data`.\\n\\n*The way cumulative table is designed is*:\\n- You would have 2 datasets like `today` and `yesterday`.\\n- `Full Outer Join` is done on the datasets, to get all the data, pulling the history data every day.\\n- `Coalesce` is used to get the latest data.\\n\\n*Where is it used?*\\n- `Growth Analysis`: To see how the data is growing over time.\\n- `State transition tracking`: To see how the data is changing over time.\\n\\n:::warning\\n- Cumulative table gets bigger and bigger over time, so applying the filtering on the data is important.\\n:::\\n\\n\\n*Cumulative Table: Strengths and Drawbacks*\\n\\n| **Category**  | **Strengths**                                                                 | **Drawbacks**                                                                                           |\\n|----------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\\n| **Analytics** | - Enables historical analysis without shuffling or reprocessing the data.    | - Sequential backfilling can limit flexibility when correcting or reprocessing older data.             |\\n| **Tracking**  | - Easy to track changes and transitions over time.                           | - Incremental growth of data size over time, leading to storage and performance challenges.             |\\n| **Data**      | - Provides a complete snapshot of trends and transitions.                   | - Retains potentially outdated or deleted PII data, complicating compliance with data privacy laws.     |\\n\\n## The compactness vs usability tradeoff\\n\\n- The most usable tables usually are\\n    - They are straightforward, they would have identifier for dimensional model.\\n    - Have no complex data types.\\n    - Easily can be manipulated with `WHERE` and `GROUP BY` clauses.\\n- The most compact tables usually are\\n    - Are compressed to be as small as possible, which would be harder to query directly until they\'re decoded.\\n\\n- The middle-ground table:\\n    - Where you would use complex data types (e.g. ARRAY, STRUCT, MAP) to model the data, which would be harder to query directly but can be used to join with other data sets.\\n\\n*When would you each type of table?*\\n- Most compact:\\n    - Online systems where latency and data volume are critical. \\n    - Consumers are usually hihgly technical and can handle complex data types.\\n- Most usable:\\n    - When analytics is the main consumer\\n    - The majority of the consumer are less technical and need the data to be easy to understand and analyze.\\n- Middle-ground:\\n    - Upstream staging/master data are used by majority of consumers who are other data engineers.\\n\\n## Struct Vs Array Vs Map\\n\\n| **Feature**           | **Struct**                                        | **Map**                                      | **Array**                                    |\\n|------------------------|--------------------------------------------------|---------------------------------------------|---------------------------------------------|\\n| **Definition**         | A table within a table.                          | A key-value pair structure.                 | A list of values.                           |\\n| **Keys**              | Rigidly defined, improving compression.          | Flexible, less compression efficiency.      | Not applicable (index-based).              |\\n| **Values**            | Can be of any type.                              | Must all be of the same type.               | Must all be of the same type.               |\\n| **Usage**             | Suitable for structured data with predefined keys.| Best for flexible key-value relationships.  | Ideal for ordered collections of data.     |\\n| **Complexity**         | Higher due to nested structure.                  | Simpler, dynamic key-value management.      | Simplest among the three; index-based access. |\\n| **Nesting**           | Supports nesting within itself or arrays.         | Can be nested inside structs or arrays.     | Can contain structs or maps as elements.   |\\n\\n## Impact of adding a temporal (time-based) dimension\\n\\n1. **Cardinality Explosion**:\\n   - Adding a time dimension (e.g., days, nights, or timestamps) increases the number of unique combinations in the dataset. This often grows by at least **one order of magnitude** (10x or more).\\n   - Example: Airbnb has ~6 million listings. If we want to track the nightly availability and pricing for each listing for a year, it results in:\\n     - **365 days \xd7 6 million listings = ~2 billion rows** of data.\\n\\n2. **Challenges with Dataset Design**:\\n   - **Option 1**: You could store this data as a single listing with an array (list) of 365 nights. This approach is compact but may be harder to query efficiently.\\n   - **Option 2**: Alternatively, store it as 2 billion rows, where each row represents one listing-night combination. While this structure is easier to query, it increases the dataset size significantly.\\n\\n3. **Using the Parquet**:\\n   - Using efficient file formats like **Parquet**, which compresses data well, helps to reduce the size of both approaches. With proper sorting and organization, you can minimize storage overhead.\\n\\n:::warning\\nBadness of denormolized temporal dimension: if you have to join the data with other dimensions, spark shuffle will ruin the compression.\\n:::\\n\\n## Run length encoding compression\\n\\n- [Run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)  is a form of lossless data compression in which runs of data (consecutive occurrences of the same data value) are stored as a single occurrence of that data value and a count of its consecutive occurrences, rather than as the original run. <sup>[1]</sup>\\n- RLE is one of reasons why Parquet is so efficient for storing data.\\n- However, **shuffle can ruin this efficiency**. Since shuffle happens in a distributed environment during JOIN and GROUP BY operations, data is redistributed across nodes, and compression benefits like RLE are often lost.\\n\\nI hope you enjoyed reading this blog on Dimensional Modelling. If you have any questions or feedback, feel free to reach out to me on <SocialLinks />\\n---\\n[1]: source from wikipedia"},{"id":"welcome-to-my-blog","metadata":{"permalink":"/blog/welcome-to-my-blog","source":"@site/blog/2024-11-26-welcome-to-my-blog/index.md","title":"Welcome","description":"Welcome dear readers...","date":"2024-11-26T00:00:00.000Z","tags":[{"inline":false,"label":"Welcome post","permalink":"/blog/tags/welcome","description":"Welcome post"}],"readingTime":1.165,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"welcome-to-my-blog","title":"Welcome","authors":["me"],"tags":["hola"],"keywords":["blog","welcome"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"import SocialLinks from \'@site/src/components/SocialLinks/socialLinks.js\'\\n\\nWelcome dear readers...\\n\\nIm Vibhavari Bellutagi, from India, currently staying in France. I\'m a Data Engineer and I got sometime to channelise my thoughts and experiences in the form of blog. I\'m a big fan of `Learn by Doing`, so you will see most of the posts are practical oriented.\\n\\n\x3c!-- truncate --\x3e\\n\\nI always wanted to have a blog to share my thoughts and experiences with the world. Im beginner in the world of blogging, so Im still learning how to write a good and effective content. I hope you enjoy my blog and find it useful.\\n\\nI have three different sections in my website:\\n\\n1. [Blogs](index.md): Where I would long format technical blogs that I have experienced or learned recently.\\n2. [Tech Bytes](/TechBytes/intro): Where I would post small technical tips and tricks that I have learned.\\n3. [Projects](/projects): Where I would post the projects that I have worked on, I would like to start with [CodeChallenge](https://codingchallenges.fyi/) by [John Crickett](https://www.linkedin.com/in/johncrickett/).\\n4. [From First Principles](/FirstPrinciples/thoughts): Where I would post the concepts that I have learned from the scratch going to the depth of it.\\n\\nHope I will be able to keep up with the content and make it useful for you, im striving to make it better everyday.\\n\\nThanks for visiting my blog. Hope you enjoy it. Feel free to reach out to me on \\n<SocialLinks />.\\n\\nCheers,\\nVibhavari"}]}}')}}]);