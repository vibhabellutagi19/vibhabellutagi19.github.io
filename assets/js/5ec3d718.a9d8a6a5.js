"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[151],{8579:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"GenAI/decode-gen-ai-jargons","title":"Decode Gen AI Jargons with me","description":"Introduction","source":"@site/content/GenAI/decode-gen-ai-jargons.md","sourceDirName":"GenAI","slug":"/GenAI/decode-gen-ai-jargons","permalink":"/GenAI/decode-gen-ai-jargons","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"slug":"decode-gen-ai-jargons","title":"Decode Gen AI Jargons with me","keywords":["data engineering","Gen AI","Generative AI","Generative Pre-trained Transformers","transformers","tokenizers","encoders","decoders"]},"sidebar":"tutorialSidebar","previous":{"title":"GenAI","permalink":"/category/genai"},"next":{"title":"Tech Bytes","permalink":"/category/tech-bytes"}}');var r=t(4848),a=t(8453),s=t(4512);const o={slug:"decode-gen-ai-jargons",title:"Decode Gen AI Jargons with me",keywords:["data engineering","Gen AI","Generative AI","Generative Pre-trained Transformers","transformers","tokenizers","encoders","decoders"]},d=void 0,l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"What is Generative AI?",id:"what-is-generative-ai",level:2},{value:"Understanding GPTs (Generative Pre-trained Transformers)",id:"understanding-gpts-generative-pre-trained-transformers",level:2},{value:"Why is GPT important?",id:"why-is-gpt-important",level:3},{value:"Breaking Down Transformers",id:"breaking-down-transformers",level:2},{value:"Tokenizers: Breaking Down Text",id:"tokenizers-breaking-down-text",level:3},{value:"Encoders and Decoders: The Building Blocks",id:"encoders-and-decoders-the-building-blocks",level:3},{value:"Encoders:",id:"encoders",level:4},{value:"Decoders:",id:"decoders",level:4},{value:"Self-Attention: How Transformers &quot;Think&quot;",id:"self-attention-how-transformers-think",level:3},{value:"Multi-Head Attention: AI&#39;s Multitasking Ability",id:"multi-head-attention-ais-multitasking-ability",level:3},{value:"Positional Encoding: Teaching AI About Order",id:"positional-encoding-teaching-ai-about-order",level:3},{value:"How Generative AI Understands Language?",id:"how-generative-ai-understands-language",level:2},{value:"Vector Embeddings: Turning Words into Numbers",id:"vector-embeddings-turning-words-into-numbers",level:3},{value:"Semantic Mapping: Finding Meaning in Context",id:"semantic-mapping-finding-meaning-in-context",level:3},{value:"How Generative AI Actually Works (Step-by-Step Interaction)",id:"how-generative-ai-actually-works-step-by-step-interaction",level:2},{value:"Step 1: User Query Submission",id:"step-1-user-query-submission",level:3},{value:"Step 2: Tokenization",id:"step-2-tokenization",level:3},{value:"Step 3: Creating Embeddings",id:"step-3-creating-embeddings",level:3},{value:"Step 4: Positional Encoding",id:"step-4-positional-encoding",level:3},{value:"Step 5: Transformer Layers and Self-Attention",id:"step-5-transformer-layers-and-self-attention",level:3},{value:"Step 6: Response Generation with Multi-Head Attention",id:"step-6-response-generation-with-multi-head-attention",level:3},{value:"Step 7: Adjusting AI Creativity with Temperature",id:"step-7-adjusting-ai-creativity-with-temperature",level:3},{value:"Step 8: Feedforward Neural Network",id:"step-8-feedforward-neural-network",level:3},{value:"Additional Concepts You Should Know",id:"additional-concepts-you-should-know",level:2},{value:"Knowledge Cutoff: Why AI Doesn\u2019t Know Everything",id:"knowledge-cutoff-why-ai-doesnt-know-everything",level:3}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",section:"section",strong:"strong",sup:"sup",ul:"ul",...(0,a.R)(),...e.components},{Highlight:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Highlight",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["As a Data Engineer, I have worked with the data science team for integrating LLM within our data pipelines, that's\nwhen I thought it would be interesting to learn more about the Generative AI world. So I started exploring the Generative AI\nworld with Hitesh Choudhary's ",(0,r.jsx)(n.a,{href:"https://hitesh.ai/cohort",children:"gen-ai cohort"}),", if the course excites you,\nyou can use this ",(0,r.jsx)(n.a,{href:"https://courses.chaicode.com/learn/fast-checkout/227321?priceId=0&code=VIBHAVAR51981&is_affiliate=true&tc=VIBHAVAR51981",children:"link"})," to get a discount.\nI will be sharing my learnings and experiences in this blog. Are you ready to decode the Gen AI jargons with me? Let's get started!"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"what-is-generative-ai",children:"What is Generative AI?"}),"\n",(0,r.jsxs)(n.p,{children:["According to the ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Generative_artificial_intelligence",children:"Wikipedia"}),", Generative artificial intelligence (Generative AI, GenAI or GAI)\nis a subset of artificial intelligence that uses ",(0,r.jsx)(i,{color:"#3e6980",children:"generative models"})," to produce text, images, videos, or other forms of data.\nThese models ",(0,r.jsx)(i,{color:"#3e6980",children:"learn the underlying patterns and structures of their training data"})," and use them to produce new data based on the input,\nwhich often comes in the form of natural language prompts."]}),"\n",(0,r.jsx)(n.p,{children:"Due to its ability to generate human-like text, images, videos and automating tasks using Agentic Workflows, Generative\nAI has gained significant attention and popularity in recent years. It has the potential to revolutionize various industries\nlike healthcare, finance, retail, education etc."}),"\n",(0,r.jsx)(n.p,{children:"Common examples of Gen AI are ChatGPT, DALL-E, Claude etc are some of the most popular generative AI tools."}),"\n",(0,r.jsx)(n.h2,{id:"understanding-gpts-generative-pre-trained-transformers",children:"Understanding GPTs (Generative Pre-trained Transformers)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generative"}),": Refers to the model's ability to generate new content, such as text, images, or other data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained"}),": Indicates that the model has been trained on a large dataset before being fine-tuned for specific tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transformers"}),": Refers to the underlying architecture of the model, which is designed to process and generate sequences of data efficiently."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["GPTs are first created by ",(0,r.jsx)(n.code,{children:"OpenAI"})," and are based on the Transformer architecture. They are designed to understand and generate\nhuman-like text by learning from vast amounts of data. The ",(0,r.jsx)(n.strong,{children:"pre-trained"})," aspect means that these models are initially trained on a broad dataset,\nallowing them to learn grammar, facts, and some reasoning abilities before being fine-tuned for specific tasks."]}),"\n",(0,r.jsx)(n.p,{children:"GPTs are a family of neural network models that uses the transformer architecture and is a key advancement in artificial intelligence (AI)\npowering generative AI applications such as ChatGPT."}),"\n",(0,r.jsx)(n.h3,{id:"why-is-gpt-important",children:"Why is GPT important?"}),"\n",(0,r.jsx)(n.p,{children:"GPT models, especially their transformer architecture, are a big step forward in AI.\nThey make tasks like translating languages, summarizing documents, writing blogs, creating websites, and designing visuals much faster and easier.\nFor example, writing and editing a complex article might take hours, but a GPT model can do it in seconds.\nThese models help organizations save time, boost productivity, and improve their applications and customer experiences."}),"\n",(0,r.jsx)(n.h2,{id:"breaking-down-transformers",children:"Breaking Down Transformers"}),"\n",(0,r.jsx)(n.p,{children:"Transformers are a type of neural network that learns contextual relationships between words in a sentence. Transformer models\napply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant\ndata elements in a series influence and depend on each other."}),"\n",(0,r.jsx)(n.h3,{id:"tokenizers-breaking-down-text",children:"Tokenizers: Breaking Down Text"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Role:"}),"\nConverts your text into smaller pieces (tokens). Tokens can be whole words, parts of words, or punctuation marks."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example:"}),'\nOriginal Text: "Hello, world!"\nTokens: [\u201cHello\u201d, \u201c,\u201d, \u201cworld\u201d, \u201c!\u201d]']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why It Matters:"}),"\nTokenization makes text manageable and standardized for the AI to process."]}),"\n",(0,r.jsx)(n.h3,{id:"encoders-and-decoders-the-building-blocks",children:"Encoders and Decoders: The Building Blocks"}),"\n",(0,r.jsx)(n.h4,{id:"encoders",children:"Encoders:"}),"\n",(0,r.jsx)(n.p,{children:"Encoders take these tokens (represented numerically after tokenization) and process them into a meaningful internal representation that captures context."}),"\n",(0,r.jsx)(n.p,{children:"After tokenization, tokens are converted into numbers (embeddings). The encoder then uses these embeddings to capture deeper meanings and relationships."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example:"}),"\nIf tokenization is about identifying puzzle pieces, encoding is like fitting these puzzle pieces together to understand what the picture represents."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"tokeniser",src:t(7973).A+"",width:"2412",height:"1436"})}),"\n",(0,r.jsx)(n.h4,{id:"decoders",children:"Decoders:"}),"\n",(0,r.jsx)(n.p,{children:"Decoders take the encoded internal representation from the encoder (or from itself, if it's generating text token-by-token) and produce new text tokens as output."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example:"}),"\nIf encoding puts the puzzle together, decoding is like explaining or describing the picture in simple language."]}),"\n",(0,r.jsx)(n.h3,{id:"self-attention-how-transformers-think",children:'Self-Attention: How Transformers "Think"'}),"\n",(0,r.jsx)(n.p,{children:"Self-attention is like reading a sentence and deciding how much attention to pay to each word based on their relevance to others."}),"\n",(0,r.jsx)(n.p,{children:'Imagine you\'re reading, "The cat sat on the mat because it was tired." Here, "it" refers to the "cat."\nSelf-attention helps the model understand this by relating "it" back to "cat."\nTransformers use self-attention to weigh each word differently, focusing more on relevant words to accurately grasp the context and meaning.'}),"\n",(0,r.jsxs)(n.p,{children:["An interesting article I found on self-attention is ",(0,r.jsx)(n.a,{href:"https://jalammar.github.io/illustrated-transformer/",children:"The Illustrated Transformer"}),". Take a look at it if you want to understand the self-attention mechanism in detail."]}),"\n",(0,r.jsx)(n.h3,{id:"multi-head-attention-ais-multitasking-ability",children:"Multi-Head Attention: AI's Multitasking Ability"}),"\n",(0,r.jsx)(n.p,{children:"Multi-head attention allows transformers to simultaneously focus on different aspects of the input sequence.\nImagine having multiple specialists each analyzing different parts of the same text at once\u2014this collective analysis results\nin a richer understanding and improved performance of the AI model."}),"\n",(0,r.jsxs)(n.p,{children:['For example, when analyzing a sentence like "She bought apples from the store after finishing her work,"\none attention head might focus on identifying the ',(0,r.jsx)(n.code,{children:"action (buying)"}),", another on the ",(0,r.jsx)(n.code,{children:"objects involved (apples, store)"}),",\nand another on the ",(0,r.jsx)(n.code,{children:"sequence of events (after finishing work)"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"positional-encoding-teaching-ai-about-order",children:"Positional Encoding: Teaching AI About Order"}),"\n",(0,r.jsxs)(n.p,{children:["Positional encoding helps transformers understand the ",(0,r.jsx)(i,{color:"#3e6980",children:"order of words in a sentence"}),'.\nSince transformers don\'t inherently understand sequences, positional encoding adds information about word positions.\nFor example, "The dog chased the cat" versus "',(0,r.jsx)(n.code,{children:"The cat chased the dog"}),'" conveys entirely different meanings due to word order.\nPositional encoding ensures that the transformer recognizes these differences.']}),"\n",(0,r.jsxs)(n.p,{children:["Another example is ",(0,r.jsx)(n.code,{children:"He arrived before the meeting"})," versus ",(0,r.jsx)(n.code,{children:"Before the meeting, he arrived,"})," where positional encoding helps\nthe model correctly interpret timing and sequence relationships."]}),"\n",(0,r.jsx)(n.h2,{id:"how-generative-ai-understands-language",children:"How Generative AI Understands Language?"}),"\n",(0,r.jsx)(n.h3,{id:"vector-embeddings-turning-words-into-numbers",children:"Vector Embeddings: Turning Words into Numbers"}),"\n",(0,r.jsx)(n.p,{children:"Embeddings convert tokens into numeric vectors, capturing semantic meaning and relationships.\nWords with similar meanings or contexts are placed closer together in numerical space, enhancing the AI's understanding\nof language nuances."}),"\n",(0,r.jsxs)(n.p,{children:['For instance, words like "happy" and "love" have similar embeddings and are located closer together compared to words\nlike "sad" or "angry," which would be positioned further away. ',(0,r.jsx)(n.sup,{children:(0,r.jsx)(n.a,{href:"#user-content-fn-1",id:"user-content-fnref-1","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})})]}),"\n",(0,r.jsx)(n.h3,{id:"semantic-mapping-finding-meaning-in-context",children:"Semantic Mapping: Finding Meaning in Context"}),"\n",(0,r.jsx)(n.p,{children:'Semantic mapping organizes embeddings into meaningful structures based on context, helping AI models grasp subtle differences in meanings.\nFor example, "bank" in the context of finance ("She deposited money at the bank.") versus "bank" referring to a riverbank ("He sat on the bank of the river.")\nare clearly distinguished through semantic mapping. Another example is distinguishing "apple" as a fruit from "Apple" as\na technology company, based on the context provided.'}),"\n",(0,r.jsx)(n.h2,{id:"how-generative-ai-actually-works-step-by-step-interaction",children:"How Generative AI Actually Works (Step-by-Step Interaction)"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-user-query-submission",children:"Step 1: User Query Submission"}),"\n",(0,r.jsx)(n.p,{children:"A user submits a prompt or query to the AI model. For example:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"what is artificial intelligence?\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-tokenization",children:"Step 2: Tokenization"}),"\n",(0,r.jsx)(n.p,{children:'The input text is broken down into tokens.\nFor example, the sentence "what is artificial intelligence?" might be tokenized into ["what", "is", "artificial", "intelligence","?"] to simplify further processing.'}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:'"what is artificial intelligence?" -> ["what", "is", "artificial", "intelligence","?"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-creating-embeddings",children:"Step 3: Creating Embeddings"}),"\n",(0,r.jsx)(n.p,{children:'Each token is converted into numeric embeddings capturing semantic meanings, such as placing "artificial" and "intelligence"\nclose together due to their frequent contextual relationship.'}),"\n",(0,r.jsx)(n.h3,{id:"step-4-positional-encoding",children:"Step 4: Positional Encoding"}),"\n",(0,r.jsxs)(n.p,{children:["The model incorporates positional information to understand word order and context.\nFor example, in the sentence ",(0,r.jsx)(n.code,{children:"The cat chased the mouse,"})," positional encoding ensures the model understands the sequence\nand can differentiate it from ",(0,r.jsx)(n.code,{children:"The mouse chased the cat."})]}),"\n",(0,r.jsx)(n.h3,{id:"step-5-transformer-layers-and-self-attention",children:"Step 5: Transformer Layers and Self-Attention"}),"\n",(0,r.jsx)(n.p,{children:"Transformer layers apply self-attention to evaluate and weigh the importance of each word concerning others in the sentence,\nhelping the model grasp the overall sentiment and context."}),"\n",(0,r.jsx)(n.h3,{id:"step-6-response-generation-with-multi-head-attention",children:"Step 6: Response Generation with Multi-Head Attention"}),"\n",(0,r.jsx)(n.p,{children:"Multiple attention heads simultaneously analyze different aspects of the context\u2014such as emotional tone (love), the subject matter (artificial intelligence)\u2014to create a nuanced response."}),"\n",(0,r.jsx)(n.h3,{id:"step-7-adjusting-ai-creativity-with-temperature",children:"Step 7: Adjusting AI Creativity with Temperature"}),"\n",(0,r.jsx)(n.p,{children:'The "temperature" parameter controls the randomness or creativity of the AI\'s responses.\nLower temperatures yield predictable results, while higher temperatures produce more creative outputs.'}),"\n",(0,r.jsx)(n.p,{children:"Think like a chef adjusting the spice level in a dish. A low temperature (less spice) results in a safe, familiar flavor,\nwhile a high temperature (more spice) leads to unexpected and exciting combinations."}),"\n",(0,r.jsx)(n.h3,{id:"step-8-feedforward-neural-network",children:"Step 8: Feedforward Neural Network"}),"\n",(0,r.jsx)(n.p,{children:"The model processes the output from the transformer layers through a feedforward neural network, refining the response further.\nThis network applies non-linear transformations to the data, enhancing the model's ability to understand complex relationships."}),"\n",(0,r.jsx)(n.h2,{id:"additional-concepts-you-should-know",children:"Additional Concepts You Should Know"}),"\n",(0,r.jsx)(n.h3,{id:"knowledge-cutoff-why-ai-doesnt-know-everything",children:"Knowledge Cutoff: Why AI Doesn\u2019t Know Everything"}),"\n",(0,r.jsx)(n.p,{children:"Knowledge cutoff represents the date up to which the AI model was trained.\nInformation after this cutoff isn't available to the model, limiting its current knowledge."}),"\n",(0,r.jsx)(n.p,{children:"If you liked reading this blog, I would love to hear your thoughts, feel free to contact me on below social platform :)"}),"\n",(0,r.jsx)(s.A,{}),"\n",(0,r.jsx)(n.hr,{}),"\n","\n",(0,r.jsxs)(n.section,{"data-footnotes":!0,className:"footnotes",children:[(0,r.jsx)(n.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{id:"user-content-fn-1",children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://projector.tensorflow.org/",children:"Vector-Space"})," ",(0,r.jsx)(n.a,{href:"#user-content-fnref-1","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},7973:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/tiktokeniser-ab43287fe8c43ec3fbdbbcb0794c11a9.png"}}]);