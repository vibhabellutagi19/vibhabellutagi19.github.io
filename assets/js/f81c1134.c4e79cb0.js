"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"de-bootcamp-graph-databases","metadata":{"permalink":"/blog/de-bootcamp-graph-databases","source":"@site/blog/2024-12-05-de-bootcamp-graph-databases/index.md","title":"Data Modelling - Graph Databases and Additve Dimensions","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering.","date":"2024-12-05T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"graph-database","permalink":"/blog/tags/graph-database","description":"Graph Database tag description"}],"readingTime":4.795,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-graph-databases","title":"Data Modelling - Graph Databases and Additve Dimensions","authors":["me"],"tags":["de","dim","de-bootcamp","graph-database"],"keywords":["data engineering","data engineering bootcamp","data modelling","graph database"],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"}},"content":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. \\nToday we are learning about Data Modelling - Graph Databases\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay - 3: Data Modeling: Graph Databases\\n\\n\x3c!-- truncate --\x3e\\n\\n## Additive vs non-additive dimensions\\n\\n### What makes a dimension additive\\n\\n- A additive dimension is a dimension that can be summed up across all the dimensions in the fact table.\\nFor Example: You manage a sales database that tracks daily transactions. \\nYou want to analyze the <Highlight color=\\"#3e6980\\">total revenue generated</Highlight> by your business.\\nThe Revenue measure is an additive dimension because it can be summed across all levels of the data (e.g., by day, month, region, product, etc.).\\n\\n- Non-additive dimensions are dimensions that cannot be summed up across all the dimensions in the fact table.\\nFor Example: Application interface is not additive.\\nThe number of active users != # of active users on web + # of active users on mobile (Android + iOS).\\n\\n    Why is this?\\n    Overlapping Users: A single user can be active on multiple platforms (e.g., logging in on both web and mobile during the same time period). Simply summing active users across platforms would `double-count` those users.\\n\\n:::important\\nA deminsion is additive over a specific window of time, if and only if, the grain of data over that window can\\nonly ever be one value at a time.\\n:::\\n\\n### How additivity help?\\n\\n1. **No need for COUNT(DISTINCT)**: When a measure is additive, you can aggregate it (e.g., SUM) across dimensions without needing a COUNT(DISTINCT) to avoid duplicates.\\n\\n2. **Additivity Applies Mostly to SUM:**\\n   - Non-additive dimensions (like active users) are usually problematic only when using `COUNT` because you might double-count. However, **SUM** aggregations typically remain additive.\\n\\n:::note\\nCan the dimension be 2 things at once? If yes, it\'s non-additive.\\n:::\\n\\n## All about ENUMS\\n\\n### What is an ENUM type?\\nENUM is a data type that contains a fixed set of values.\\n\\n### When should you use ENUMs?\\n- Enums are great for low-to-medium cardinality columns.\\n- Country is a great example of where Enums start to struggle.\\n\\n:::info\\nEnums are great if the set of value count is < 50. \\n:::\\n\\n### Why use ENUMs?\\n\\n- Built in data quality:\\n    - IF you model a column as ENUM, you can be sure that the data in that column is one of the values in the ENUM. if the data is not in the ENUM, the pipeline will fail.\\n- Built in static fields:\\n    - If you have a column that is not going to change, you can use ENUMs.\\n    - Example: Employee Type (Full Time, Part Time, Contractor)\\n- Built in documentation:\\n    - You would know already what the values are, it is self documented.\\n\\nIn Postgres you can create ENUMs like this:\\n```sql\\nCREATE TYPE employee_type AS ENUM (\'Full Time\', \'Part Time\', \'Contractor\');\\n```\\n\\n### Enumerations and sup partitions\\n\\n- Enumerations make it easy to create sup partitions, because:\\n    - You can be sure that the data in the column is one of the values in the ENUM.\\n    - They chunk up the big data problem into manageable pieces.\\n\\n**For example:** Deduping the Notification channel (Email, SMS, Push) is defined as ENUM. You can process the data with date and notification channel in parallel.\\n\\n- The [little book of pipelines](https://github.com/EcZachly/little-book-of-pipelines/tree/master) by Zach:\\n    - The **Little Book of Enums** is a design pattern for organizing and managing complex data pipelines with multiple sources. It groups similar data sources together, defines a shared schema for consistency, and uses enums to document metadata, data quality rules, and constants for each group. \\n    - How is this little book generated?\\n        - The **ENUMs** are typically defined in Python or Scala to represent the metadata.  \\n        - A dedicated job is used to convert the enumerated list into a table.  \\n        - The resulting table is small, as it only contains as many entries as there are ENUMs.  \\n        - This table can then be used to share metadata between data quality (DQ) checks (passed as a table) and source functions (passed as Python objects) by joining them as needed.\\n    - What type of use cases is this ENUM pattern useful for?\\n        - Whenever you have many sources mapping to a shared schema.\\n        - Airbnb\\n            - Unit Economics (fees, coupons, credits, insurance, infrastructure cost, taxes etc)\\n    - How do you model data from disparate sources into a shared schema?\\n        - Flexible schema: Using map\\n\\n### Flexible schema\\n\\n- Benefits\\n    - No need to run ALTER TABLE each time a new column is added.\\n    - Easier management of numerous columns.\\n    - Schemas are not cluttered with many \\"NULL\\" columns.\\n    - \\"Other_properties\\" columns are excellent for \\"rarely-used-but-needed\\" data.\\n- Drawbacks\\n    - Typically results in poorer compression.\\n    - Can affect readability and queryability.\\n\\n## Graph data modeling\\n\\n### What is a graph database?\\nGraph databases are a type of NoSQL database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data.\\n\\nGraph modelling is RELATIONSHIP focused, not entity focused. There would be chances of poor job at modelling the entities.\\n\\n\\nUsually the model look like this for entities(vertices):\\n\\n```\\n- Identifier: STRING\\n- Type: STRING\\n- Properties: MAP<STRING,STRING>\\n```\\n:::warning\\nWe dont care about the entities, we care about the relationships.\\n:::\\n\\nThe schema for edges would look like this:\\n```\\n- subject_identifier: STRING\\n- subject_type: VertexType\\n- object_identifier: STRING\\n- object_type: VertexType\\n- edge_type: EdgeType (always a verb: is_a, place_with, has_a etc)\\n- Properties: MAP<STRING,STRING>\\n```"},{"id":"de-bootcamp-dimensional-modelling-scd","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling-scd","source":"@site/blog/2024-12-01-de-bootcamp-dimensional-modelling-scd/index.md","title":"Dimensional Modelling - SCD","description":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.","date":"2024-12-01T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"slowly-changing-dimension","permalink":"/blog/tags/scd","description":"Slowly Changing Dimension tag description"}],"readingTime":5.085,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling-scd","title":"Dimensional Modelling - SCD","authors":["me"],"tags":["de","dim","de-bootcamp","scd"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","slowly changing dimensions"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Data Modelling - Graph Databases and Additve Dimensions","permalink":"/blog/de-bootcamp-graph-databases"},"nextItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Idempotency and SCDs.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-2: Dimensional Modelling - Slowly Changing Dimensions (Theory)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is an Idempotent Pipeline?\\n> Idempotency: An idempotent pipeline ensures that processing the same data multiple times (whether it\'s live production data or backfill data) yields the same result without any unintended side effects. This consistency is crucial for reliability and simplicity in data workflows.\\n\\nPipelines should produce the same results:\\n1. The day they are run\\n2. How many times they are run\\n3. What hour they are run\\n\\n### Why is hard to troubleshoot non-idempotent pipelines?\\n1. Silent Failure: The non-idempotent pipelines can produce different results every time they are run, it doesnt fail. This results in <Highlight color=\\"#3e6980\\">non reproduceable results</Highlight>.\\n2. This causes inconsistency in the data to the downstream systems too.\\n\\n### What can make a pipline non-idempotent?\\n1. `INSERT INTO` without `TRUNCATE`:\\n    1. If you dont `TRUNCATE` data before INSERT, it will keep on adding the data to the table, creating duplicates. \\n    2. Thus your pipeline will not be idempotent, as it will keep on adding the data to the table.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">MERGE</Highlight> or <Highlight color=\\"#3e6980\\">INSERT OVERWRITE</Highlight> always.\\n2. Using `start_date >` without a corresponding `end_date <`:\\n    1. If you dont have `end_date` in your SCD, you will not be able to track the changes in the data.\\n    2. This can create a out of memory exceptions, when you backfill the data.\\n    3. Highly recommended to use <Highlight color=\\"#3e6980\\">end_date</Highlight> in your SCD always.\\n3. Not using a full set of partition sensors:\\n    1. Your pipeline may run with partial set of inputs.\\n    2. pipeline might run when there is no/partial data\\n4. Not using `depends_on_past` for cumulative pipelines:\\n    1. When you process the data in parallel ( lets say processing parallel days ), pipeline may end up processing yesterday\'s data, which hasnt been created yet.\\n\\n:::info\\nThe beauty of an idempotent pipeline lies in its ability to produce consistent results for both production and backfill data.\\n:::\\n\\n**Example:**\\nImagine a pipeline that:\\n1. Reads data (e.g., transactions).\\n2. Cleans and deduplicates it.\\n3. Stores the cleaned data in a database.\\n\\n- **Production:** As new transactions come in, they\u2019re cleaned, deduplicated, and stored.\\n- **Backfill:** When historical transactions are added, they go through the same cleaning and deduplication process, ensuring no duplicate records and consistent results.\\n\\nBecause the pipeline is idempotent:\\n- Processing the same data again won\u2019t alter the results or create issues.\\n- Whether it\'s production or backfill data, the behavior remains identical.\\n\\n### Few more problems that can make a pipeline non-idempotent\\n\\n1. Relying on the **latest** partition of a not properly modeled SCD table.\\n    - Cumulative table design AMPLIFIES this bug.\\n2. One exception relying on latest partition is when you have properly modeled SCD table and <Highlight color=\\"#3e8045\\">you are backfilling not in production</Highlight>.\\n\\n### The pains of not having an idempotent pipeline\\n\\n- Backfilling causes inconsistencies between the old and restated data.\\n- Hard to troubleshoot bugs and fix the issues.\\n- Unit testing cannot replicate the production behavior.\\n- The pipeline doesnt fail, but the data is inconsistent, which is silent failures.\\n\\n## Slowly Changing Dimensions (SCD)\\n\\nThe Slowly Changing Dimensions (SCD) are dimensions that change slowly over time, which would have time frame with them. For example,\\n    1. Customer Address\\n    2. Product Description\\n    3. Employee Details\\n    4. etc.\\n\\n:::warning\\nNot modelling the SCDs properly can impact the idempotency.\\n:::\\n\\n:::tip\\nThe slower the changes in a Slowly Changing Dimension (SCD), the more effectively it can be modeled.\\n:::\\n\\n### How can you model dimensions that change over time?\\n\\n- Singular snapshot: The dimension table only contains the most recent data.\\n    - Be Careful: This pipelines are not idempotent.\\n- Daily partitioned snapshots: The dimension table contains a snapshot of the data for each day.\\n    - Very simple way of implementing SCD.\\n- SCD Types 1,2,3\\n\\n### Types of Slowly Changing Dimensions\\n\\nHere\u2019s a Markdown table summarizing the content provided:\\n\\n| **Type**  | **Description**                                                                                             | **Key Features**                                                                                      | **Limitations**                                                                                   |\\n|-----------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\\n| **Type 0** | Static dimensions, not actually slowly changing.                                                           | - No temporal columns. <br /> - Table contains only identifier and value.                              | - No changes tracked or maintained.                                                              |\\n| **Type 1** | Overwrites old data with new data, only latest data is retained.                                           | - Simple and efficient. <br /> - No history maintained.                                                | - Cannot retrieve historical data.                                                               |\\n| **Type 2** | Gold standard of Slowly Changing Dimensions, maintains history using temporal columns.                     | - Uses `start_date` and `end_date`. <br /> - Current rows have `end_date = NULL` or `9999-12-31`. <br /> - Optionally uses `is_current` column. | - More than one row per dimension can complicate filtering and querying.                         |\\n| **Type 3** | Maintains only two versions of the data: `original` and `current`.                                         | - One row per dimension.                                                                             | - Loses history of changes between `original` and `current`. <br /> - Partially idempotent.        |\\n\\n\\n### Which types are idempotent?\\n\\n| Type      | Idempotent | Reason                                                                                           |\\n|-----------|------------|--------------------------------------------------------------------------------------------------|\\n| Type 0 | Yes        | The values are unchanging.                                                                       |\\n| Type 2 | Yes        | It is idempotent, but you need to be careful with how you use the `start_date` and `end_date`.   |\\n| Type 1 | No         | Backfilling with this dataset gives the dimension as it is now, not as it was before.           |\\n| Type 3 | No         | Backfilling makes it impossible to determine whether to pick \\"original\\" or \\"current.\\"           |\\n\\n### Loading SCD2\\n\\n- Load the entire history in one query\\n    - Inefficient but nimble\\n- Incremnetally load the data after the previoud SCD is generated\\n    - Has the same `depends_on_past`. its efficient but cumbersome."},{"id":"de-bootcamp-dimensional-modelling","metadata":{"permalink":"/blog/de-bootcamp-dimensional-modelling","source":"@site/blog/2024-11-29-de-bootcamp-dimensional-modelling/index.md","title":"Dimensional Modelling - Cumulative Table","description":"This page is about Dimensional Modelling - Cumulative Table.","date":"2024-11-29T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"dimensional-modeling","permalink":"/blog/tags/dim","description":"Dimensional Modeling tag description"},{"inline":false,"label":"data-engineering-bootcamp","permalink":"/blog/tags/de-bootcamp","description":"Tags on Data Engineering Bootcamp by Zach Wilson"},{"inline":false,"label":"cumulative-table","permalink":"/blog/tags/cumulativetable","description":"Cumulative Table tag description"}],"readingTime":8.82,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"de-bootcamp-dimensional-modelling","title":"Dimensional Modelling - Cumulative Table","authors":["me"],"tags":["de","dim","de-bootcamp","cumulativetable"],"keywords":["data engineering","data engineering bootcamp","dimensional modeling","cumulative table"],"hide_table_of_contents":false,"image":"assets/img/dimension_modelling.png","description":"This page is about Dimensional Modelling - Cumulative Table."},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - SCD","permalink":"/blog/de-bootcamp-dimensional-modelling-scd"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome-to-my-blog"}},"content":"Im sharing my learning from the Data Engineering Bootcamp, where we are learning about Data Engeering. Today we are learning about Dimensional modelling - Cumulative Table.\\n\\nI would like to extend my gratitude to Zach Wilson, the founder of [DataExpert.io](https://bootcamp.techcreator.io/lessons), for his invaluable guidance and the comprehensive Data Engineering Bootcamp.\\nConnect with Zach Wilson on [LinkedIn](https://www.linkedin.com/in/eczachly/).\\nThank you, Zach, for this amazing intense bootcamp on Data engineering!\\n\\n---\\n\\nDay-1: Dimensional Modelling (Theory)\\n\\n## What is a Dimension?\\n\\nDimension are `attributes` of an entity ( example: user\'s name, age etc)\\n    - Dimensions may be `IDENTIFY` an entity, which would be a unique identifier for the entity (example: user_id, device_id etc)\\n    - Dimensions may be `DESCRIPTIVE` which would be the attributes of the entity (example: user\'s name,age etc) but wont be unique.\\n\\n\x3c!-- truncate --\x3e\\n\\nDimensions comes in two flavors:\\n    - `Slowly Changing Dimensions` : Dimensions that change over time like user\'s address, phone number etc. This attribute is time dependent. Can be painful deal with these attributes.\\n    - `Fixed`: Dimensions that do not change over time like user\'s birthdate. These are easy to deal with.\\n\\n:::info\\nKnowing whether the attributes are slowly changing or fixed is important to design the data model.\\n:::\\n## Knowing your data consumer\\n\\n`Who is using your data?` is the most important question to ask before designing the data model. The data model should be designed in such a way that it is easy for the data consumer to understand the data.\\n\\nWhether the data is getting delivered to:\\n1. Data Analyst / Data Scientist: \\n    - They would be interested in the data that is `easy to understand and analyze`.\\n    - They wouldnt be interested in dealing with `complex data types` like JSON, XML etc.\\n2. Data Engineers:\\n    - Sometimes downstream systems would be handled by Data Engineers, to take your data sets and join with other data to achieve the buisness logic, so the data model be compact and probably harder to query.\\n    - Nested types are fine to model the data.\\n    - Usually, as we get further up in ladder of data engineering, there would be `Master Data` which would be used by other data engineers or teams, to join the master data with other data sets.\\n3. ML models:\\n    - Generally machine learning models would need `identifier` and bunch of flattened decimal columns like `feature value columns` etc.\\n    - Sometimes it depends on the model, the model can sometime handle the complex data structures, similiar to what Data analyst and data scientist would need.\\n4. Customers\\n    - You would never give the data directly ( like JSON, CSVs etc) to the customers, you would always give them the `dashboards` or `reports` which would be easy to understand and analyze.\\n\\n> Not knowing your data consumer, there is a high chance that the data model would be designed in a wrong way, can lead to business loss.\\n\\n## OLTP Vs OLAP Vs Master Data\\n\\n1. OLTP(Online Transaction Processing):\\n    - OLTP is the system that is used to record the transactions that are happening in the system.\\n    - The data model is optimized for `low latency` and `low-volume` queries, usually dealing with `single record` at a time.\\n    - Usually Software Engineers model the data in OLTP.\\n    - The data model in OLTP is `3NF` (Third Normal Form) which means the data is broken down into multiple tables and the data is not repeated.\\n    - Databases: MySQL, Postgres etc.\\n2. OLAP(Online Analytical Processing):\\n    - OLAP is the system that is used to analyze the data that is stored in the OLTP system.\\n    - The data model is optimized for `high volume` and `complex queries`, using `aggregations` and `joins`.\\n    - Usually `Data Engineers` model the data in OLAP.\\n    - Analytical Databases: Redshift, BigQuery etc.\\n3. Master Data:\\n    - Master Data is optimied for completeness of entity definition and deduped.\\n    - Master Data is usually a middle layer between OLTP and OLAP.\\n\\n## Impacts of Mismatched Data Model\\n:::info\\n Mismatching needs of the data consumer == Less business value\\n:::\\n\\n- When you model transaction system as analytical system, like transactional system usually need only one record at a time, but if you model it as analytical system, you would retrieve all the unnecessary data which would be a performance hit.\\n- When you model analytical system as transactional system, like analytical system usually need to aggregate the data, but if you model it as transactional system, you would have to do the aggregation/joins on the fly which would be a performance hit.\\n- Having the Master Data as middle layer, would give you the flexibility whichever data model you want to use.\\n\\n## OLTP and OLAP is a continuum\\n\\n![data-flow](data-flow.png)\\n\\n- `Production Database Snapshot`: Usually the data in Production databases are transactional data, which is used to record the transactions that are happening in the system/web application/mobile application etc.\\n- `Master Data`: Taking the production datasets and creating the master data, for easy to understanding. Instead of querying \\"many\\" transactional databases, merging them into master data would be much efficient.\\n- `OLAP Cubes`: \\n    - You would flatten the data, like you might have multiple rows per entity, where you can aggregate the data.\\n    - OLAP Cubes space is also create the slice and dice the data.\\n- `Metrics`: You would aggregate even further to get the one number which would be like `KPIs` of the business.\\n\\n## Cumulative Table Design\\n\\n- Some days not every user would be active, but still master data should hold the details of the user.\\n- Cumulative table is the table that holds the `history of the data`.\\n\\n*The way cumulative table is designed is*:\\n- You would have 2 datasets like `today` and `yesterday`.\\n- `Full Outer Join` is done on the datasets, to get all the data, pulling the history data every day.\\n- `Coalesce` is used to get the latest data.\\n\\n*Where is it used?*\\n- `Growth Analysis`: To see how the data is growing over time.\\n- `State transition tracking`: To see how the data is changing over time.\\n\\n:::warning\\n- Cumulative table gets bigger and bigger over time, so applying the filtering on the data is important.\\n:::\\n\\n\\n*Cumulative Table: Strengths and Drawbacks*\\n\\n| **Category**  | **Strengths**                                                                 | **Drawbacks**                                                                                           |\\n|----------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\\n| **Analytics** | - Enables historical analysis without shuffling or reprocessing the data.    | - Sequential backfilling can limit flexibility when correcting or reprocessing older data.             |\\n| **Tracking**  | - Easy to track changes and transitions over time.                           | - Incremental growth of data size over time, leading to storage and performance challenges.             |\\n| **Data**      | - Provides a complete snapshot of trends and transitions.                   | - Retains potentially outdated or deleted PII data, complicating compliance with data privacy laws.     |\\n\\n## The compactness vs usability tradeoff\\n\\n- The most usable tables usually are\\n    - They are straightforward, they would have identifier for dimensional model.\\n    - Have no complex data types.\\n    - Easily can be manipulated with `WHERE` and `GROUP BY` clauses.\\n- The most compact tables usually are\\n    - Are compressed to be as small as possible, which would be harder to query directly until they\'re decoded.\\n\\n- The middle-ground table:\\n    - Where you would use complex data types (e.g. ARRAY, STRUCT, MAP) to model the data, which would be harder to query directly but can be used to join with other data sets.\\n\\n*When would you each type of table?*\\n- Most compact:\\n    - Online systems where latency and data volume are critical. \\n    - Consumers are usually hihgly technical and can handle complex data types.\\n- Most usable:\\n    - When analytics is the main consumer\\n    - The majority of the consumer are less technical and need the data to be easy to understand and analyze.\\n- Middle-ground:\\n    - Upstream staging/master data are used by majority of consumers who are other data engineers.\\n\\n## Struct Vs Array Vs Map\\n\\n| **Feature**           | **Struct**                                        | **Map**                                      | **Array**                                    |\\n|------------------------|--------------------------------------------------|---------------------------------------------|---------------------------------------------|\\n| **Definition**         | A table within a table.                          | A key-value pair structure.                 | A list of values.                           |\\n| **Keys**              | Rigidly defined, improving compression.          | Flexible, less compression efficiency.      | Not applicable (index-based).              |\\n| **Values**            | Can be of any type.                              | Must all be of the same type.               | Must all be of the same type.               |\\n| **Usage**             | Suitable for structured data with predefined keys.| Best for flexible key-value relationships.  | Ideal for ordered collections of data.     |\\n| **Complexity**         | Higher due to nested structure.                  | Simpler, dynamic key-value management.      | Simplest among the three; index-based access. |\\n| **Nesting**           | Supports nesting within itself or arrays.         | Can be nested inside structs or arrays.     | Can contain structs or maps as elements.   |\\n\\n## Impact of adding a temporal (time-based) dimension\\n\\n1. **Cardinality Explosion**:\\n   - Adding a time dimension (e.g., days, nights, or timestamps) increases the number of unique combinations in the dataset. This often grows by at least **one order of magnitude** (10x or more).\\n   - Example: Airbnb has ~6 million listings. If we want to track the nightly availability and pricing for each listing for a year, it results in:\\n     - **365 days \xd7 6 million listings = ~2 billion rows** of data.\\n\\n2. **Challenges with Dataset Design**:\\n   - **Option 1**: You could store this data as a single listing with an array (list) of 365 nights. This approach is compact but may be harder to query efficiently.\\n   - **Option 2**: Alternatively, store it as 2 billion rows, where each row represents one listing-night combination. While this structure is easier to query, it increases the dataset size significantly.\\n\\n3. **Using the Parquet**:\\n   - Using efficient file formats like **Parquet**, which compresses data well, helps to reduce the size of both approaches. With proper sorting and organization, you can minimize storage overhead.\\n\\n:::warning\\nBadness of denormolized temporal dimension: if you have to join the data with other dimensions, spark shuffle will ruin the compression.\\n:::\\n\\n## Run length encoding compression\\n\\n- [Run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)  is a form of lossless data compression in which runs of data (consecutive occurrences of the same data value) are stored as a single occurrence of that data value and a count of its consecutive occurrences, rather than as the original run. <sup>[1]</sup>\\n- RLE is one of reasons why Parquet is so efficient for storing data.\\n- However, **shuffle can ruin this efficiency**. Since shuffle happens in a distributed environment during JOIN and GROUP BY operations, data is redistributed across nodes, and compression benefits like RLE are often lost.\\n\\n---\\n[1]: source from wikipedia"},{"id":"welcome-to-my-blog","metadata":{"permalink":"/blog/welcome-to-my-blog","source":"@site/blog/2024-11-26-welcome-to-my-blog/index.md","title":"Welcome","description":"Welcome dear readers...","date":"2024-11-26T00:00:00.000Z","tags":[{"inline":false,"label":"Welcome post","permalink":"/blog/tags/welcome","description":"Welcome post"}],"readingTime":1.095,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://github.com/vibhabellutagi19","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"welcome-to-my-blog","title":"Welcome","authors":["me"],"tags":["hola"],"keywords":["blog","welcome"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Dimensional Modelling - Cumulative Table","permalink":"/blog/de-bootcamp-dimensional-modelling"}},"content":"Welcome dear readers...\\n\\nIm Vibhavari Bellutagi, from India, currently staying in France. I\'m a Data Engineer and I got sometime to channelise my thoughts and experiences in the form of blog. I\'m a big fan of `Learn by Doing`, so you will see most of the posts are practical oriented.\\n\\n\x3c!-- truncate --\x3e\\n\\nI always wanted to have a blog to share my thoughts and experiences with the world. Im beginner in the world of blogging, so Im still learning how to write a good and effective content. I hope you enjoy my blog and find it useful.\\n\\nI have three different sections in my website:\\n\\n1. [Blogs](index.md): Where I would long format technical blogs that I have experienced or learned recently.\\n2. [Tech Bytes](/TechBytes/intro): Where I would post small technical tips and tricks that I have learned.\\n3. [Projects](/projects): Where I would post the projects that I have worked on, I would like to start with [CodeChallenge](https://codingchallenges.fyi/) by [John Crickett](https://www.linkedin.com/in/johncrickett/).\\n4. [From First Principles](/FirstPrinciples/thoughts): Where I would post the concepts that I have learned from the scratch going to the depth of it.\\n\\nHope I will be able to keep up with the content and make it useful for you, im striving to make it better everyday.\\n\\nThanks for visiting my blog. Hope you enjoy it.\\n\\nCheers,\\nVibhavari"}]}}')}}]);