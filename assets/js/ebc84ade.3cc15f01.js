"use strict";(self.webpackChunkvibhavari_bellutagi=self.webpackChunkvibhavari_bellutagi||[]).push([[6539],{7932:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>d});var o=s(3493),i=s(4848),a=s(8453),r=s(4512);const l={slug:"columns-and-expressions",title:"Columns and Expressions",authors:["me"],tags:["de","apache-spark"],keywords:["data engineering","apache spark","columns and expressions"],hide_table_of_contents:!1,image:"assets/col_n_exps.png"},t=void 0,c={authorsImageUrls:[void 0]},d=[{value:"Columns and Experssions",id:"columns-and-experssions",level:2},{value:"Ways of Constructing the Column",id:"ways-of-constructing-the-column",level:3},{value:"Selecting Columns",id:"selecting-columns",level:3},{value:"Setting up the Spark Session and dummy data",id:"setting-up-the-spark-session-and-dummy-data",level:4},{value:"Selecting Columns using <code>col</code> and <code>select</code>",id:"selecting-columns-using-col-and-select",level:4},{value:"Performing Calculations with Columns",id:"performing-calculations-with-columns",level:4},{value:"Using <code>expr</code> to perform calculations",id:"using-expr-to-perform-calculations",level:4},{value:"Using <code>selectExpr</code>",id:"using-selectexpr",level:4},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components},{Highlight:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Highlight",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Apache Spark's ",(0,i.jsx)(n.code,{children:"Column and Expression"})," play a big\u2002role in making your pipeline more efficient. In this\u2002Tech Byte we will look into ALL the possible ways to select columns, use built-in functions and perform calculations with column objects and expressions in PySpark. So, whether you build an ETL pipeline or doing exploratory data analysis, these techniques methods will come in handy."]}),"\n",(0,i.jsx)(n.h2,{id:"columns-and-experssions",children:"Columns and Experssions"}),"\n",(0,i.jsx)(n.p,{children:"In Spark, columns are not actual data; they are logical constructs\u2014formulas, basically\u2014that say how to calculate a value for every row in\u2002a table. These formulas by themselves\u2002don\u2019t contain any real data, just return real values when they are applied to rows in a DataFrame. For this reason,\u2002columns can\u2019t exist in isolation \u2014 you need a DataFrame and some of its rows for a column\u2019s formula to be evaluated. Therefore, the creation or manipulation\u2002of columns should always occur by transforming on a DataFrame instead of on the column alone."}),"\n",(0,i.jsx)(n.h3,{id:"ways-of-constructing-the-column",children:"Ways of Constructing the Column"}),"\n",(0,i.jsxs)(n.p,{children:["There are many ways to construct and refer to columns but the simplest ways are by using the ",(0,i.jsx)(n.code,{children:"col, column functions or expr"}),". ",(0,i.jsx)(n.code,{children:"col(), column() or expr"})," are from package ",(0,i.jsx)(n.code,{children:"pyspark.sql.functions"}),". Find the syntax below:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import col, column, expr\ncol("columnName")\ncolumn("columnName")\nexpr("columnName")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"selecting-columns",children:"Selecting Columns"}),"\n",(0,i.jsxs)(n.p,{children:["You can select columns from a DataFrame using the ",(0,i.jsx)(n.code,{children:"select()"})," method. the ",(0,i.jsx)(n.code,{children:"select()"})," takes a list of column objects or expressions."]}),"\n",(0,i.jsx)(n.h4,{id:"setting-up-the-spark-session-and-dummy-data",children:"Setting up the Spark Session and dummy data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, column, expr\n\nspark_session = (\n    SparkSession\n    .builder\n    .appName("ColumnsAndExpressionsDemo")\n    .config("spark.master", "local")\n    .getOrCreate()\n)\n\ndata = [\n    {"Name": "car1", "Horsepower": 130, "Acceleration": 12.0, "Weight_in_lbs": 3504, "Year": 1970},\n    {"Name": "car2", "Horsepower": 165, "Acceleration": 11.5, "Weight_in_lbs": 3693, "Year": 1970},\n    {"Name": "car3", "Horsepower": 150, "Acceleration": 10.5, "Weight_in_lbs": 3436, "Year": 1970},\n    {"Name": "car4", "Horsepower": 150, "Acceleration": 10.0, "Weight_in_lbs": 3761, "Year": 1970},\n    {"Name": "car5", "Horsepower": 140, "Acceleration": 9.0,  "Weight_in_lbs": 3200, "Year": 1971},\n    {"Name": "car6", "Horsepower": 198, "Acceleration": 8.5,  "Weight_in_lbs": 4341, "Year": 1971},\n    {"Name": "car7", "Horsepower": 220, "Acceleration": 8.0,  "Weight_in_lbs": 4354, "Year": 1971},\n    {"Name": "car8", "Horsepower": 215, "Acceleration": 7.5,  "Weight_in_lbs": 4312, "Year": 1972},\n    {"Name": "car9", "Horsepower": 225, "Acceleration": 7.7,  "Weight_in_lbs": 4425, "Year": 1972},\n    {"Name": "car10","Horsepower": 190, "Acceleration": 9.5,  "Weight_in_lbs": 3850, "Year": 1972},\n]\n\ncars_df = spark_session.createDataFrame(data)\ncars_df.show()\n'})}),"\n",(0,i.jsxs)(n.h4,{id:"selecting-columns-using-col-and-select",children:["Selecting Columns using ",(0,i.jsx)(n.code,{children:"col"})," and ",(0,i.jsx)(n.code,{children:"select"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'first_column = col("Name")  # return Column object\n\ncars_df.select(\n    first_column, # using Column object\n    col("Acceleration"),\n    column("Weight_in_lbs"),\n    cars_df.Weight_in_lbs, # using dot (.) notation\n    \'Horsepower\',       # using string-based column reference\n    expr(\'Year\')        # using Spark SQL expression\n).show(10)\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(s,{color:"#3e6980",children:'col("columnName") and column("columnName")'})," are functionally equivalent. They create a Column object that you can pass to DataFrame transformations."]}),"\n",(0,i.jsxs)(n.li,{children:["You can also refer to columns directly by their ",(0,i.jsx)(s,{color:"#3e6980",children:"string name ('Horsepower')"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(s,{color:"#3e6980",children:'expr("Year")'})," showcases how you can mix SQL expressions right within the select statement."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"performing-calculations-with-columns",children:"Performing Calculations with Columns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.column import Column\n\nsimple_expression: Column = col("Weight_in_lbs")\nweight_in_kgs_expression: Column = col("Weight_in_lbs") / 2.2 # performing arithmetic on columns\n\ncars_df.select(\n    col("Name"),\n    col("Weight_in_lbs"),\n    weight_in_kgs_expression.alias("Weight_in_kgs")\n).show(5)\n\n'})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Spark allows you to perform arithmetic on columns just like regular Python variables."}),"\n",(0,i.jsx)(n.li,{children:'alias("Weight_in_kgs") labels your computed column for clarity in the output.'}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"using-expr-to-perform-calculations",children:["Using ",(0,i.jsx)(n.code,{children:"expr"})," to perform calculations"]}),"\n",(0,i.jsx)(n.p,{children:"An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset."}),"\n",(0,i.jsxs)(n.p,{children:["In the simplest case, an expression, created via the ",(0,i.jsx)(n.code,{children:"expr function"}),', is just a DataFrame column reference. In the simplest case, expr("someCol") is equivalent to col("someCol").']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\nprint("Select with expr...")\ncars_df.select(\n    col("Name"),\n    col("Weight_in_lbs"),\n    expr("Weight_in_lbs / 2.2") # using expr to perform arithmetic\n).show(5)\n\n'})}),"\n",(0,i.jsxs)(n.h4,{id:"using-selectexpr",children:["Using ",(0,i.jsx)(n.code,{children:"selectExpr"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"select method"})," when you\u2019re working with columns or expressions, and the ",(0,i.jsx)(n.code,{children:"selectExpr method"})," when you\u2019re working with expressions in strings."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"\ncars_df.selectExpr(\n    'Name',\n    'Weight_in_lbs',\n    'Weight_in_lbs / 2.2'\n).show(5)\n\n"})}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(n.p,{children:["Choosing between ",(0,i.jsx)(n.code,{children:"col()"}),", ",(0,i.jsx)(n.code,{children:"column()"}),", ",(0,i.jsx)(n.code,{children:"expr()"}),", or even raw string references comes down to personal preference, readability, and complexity of your transformations. For simple column references, col() is often the most straightforward. However, if you prefer writing SQL-like expressions directly in your code or need complex SQL functions, ",(0,i.jsx)(n.code,{children:"expr()"})," and ",(0,i.jsx)(n.code,{children:"selectExpr()"})," provide the flexibility you need."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:["I hope this blog helped you understand the use of ",(0,i.jsx)(n.code,{children:"Columns and Expressions"})," in Apache Spark. If you are interested in more such content, do check out ",(0,i.jsx)(n.a,{href:"/blog/tags/apache-spark",children:"Apache Spark"}),". series."]}),"\n",(0,i.jsx)(n.p,{children:"If you have any questions or feedback, feel free to reach out to me on"}),"\n",(0,i.jsx)(r.A,{})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},3493:e=>{e.exports=JSON.parse('{"permalink":"/blog/columns-and-expressions","source":"@site/blog/2025-01-10-columns-and-experssions/index.md","title":"Columns and Expressions","description":"Apache Spark\'s Column and Expression play a big\u2002role in making your pipeline more efficient. In this\u2002Tech Byte we will look into ALL the possible ways to select columns, use built-in functions and perform calculations with column objects and expressions in PySpark. So, whether you build an ETL pipeline or doing exploratory data analysis, these techniques methods will come in handy.","date":"2025-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"data-engineering","permalink":"/blog/tags/de","description":"DE tag description"},{"inline":false,"label":"apache-spark","permalink":"/blog/tags/apache-spark","description":"Spark tag description"}],"readingTime":3.805,"hasTruncateMarker":true,"authors":[{"name":"Vibhavari Bellutagi","title":"Data Engineer","url":"https://buildwithvibs.in/","socials":{"github":"https://github.com/vibhabellutagi19","linkedin":"https://www.linkedin.com/in/vibhavari-bellutagi-837871189/","twitter":"https://twitter.com/buildwith_vibs"},"imageURL":"https://avatars.githubusercontent.com/u/39341524?s=400&u=5d760c052fe0614d3af649de9e85474d1cafeba7&v=4","key":"me","page":null}],"frontMatter":{"slug":"columns-and-expressions","title":"Columns and Expressions","authors":["me"],"tags":["de","apache-spark"],"keywords":["data engineering","apache spark","columns and expressions"],"hide_table_of_contents":false,"image":"assets/col_n_exps.png"},"unlisted":false,"prevItem":{"title":"Handling Nulls in Spark","permalink":"/blog/handling-nulls-in-spark"},"nextItem":{"title":"Introduction to Apache Spark","permalink":"/blog/spark-basics"}}')}}]);