<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-GenAI/decode-gen-ai-jargons" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">Decode Gen AI Jargons with me | Vibhavari Bellutagi</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://buildwithvibs.in/img/vb-social-card.png"><meta data-rh="true" name="twitter:image" content="https://buildwithvibs.in/img/vb-social-card.png"><meta data-rh="true" property="og:url" content="https://buildwithvibs.in/GenAI/decode-gen-ai-jargons"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Decode Gen AI Jargons with me | Vibhavari Bellutagi"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><meta data-rh="true" name="keywords" content="data engineering,Gen AI,Generative AI,Generative Pre-trained Transformers,transformers,tokenizers,encoders,decoders"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://buildwithvibs.in/GenAI/decode-gen-ai-jargons"><link data-rh="true" rel="alternate" href="https://buildwithvibs.in/GenAI/decode-gen-ai-jargons" hreflang="en"><link data-rh="true" rel="alternate" href="https://buildwithvibs.in/GenAI/decode-gen-ai-jargons" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Vibhavari Bellutagi RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Vibhavari Bellutagi Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1F9P2Z6DLX"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1F9P2Z6DLX",{anonymize_ip:!0})</script>



<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="/assets/css/styles.af25fa78.css">
<script src="/assets/js/runtime~main.df5301c1.js" defer="defer"></script>
<script src="/assets/js/main.2b1efeb9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">buildwith_vibs</b></a><a class="navbar__item navbar__link" href="/content">Content</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/Resume/experience">Resume</a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/category/resume">Resume</a><button aria-label="Expand sidebar category &#x27;Resume&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/category/genai">GenAI</a><button aria-label="Collapse sidebar category &#x27;GenAI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/GenAI/decode-gen-ai-jargons">Decode Gen AI Jargons with me</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/category/tech-bytes">Tech Bytes</a><button aria-label="Expand sidebar category &#x27;Tech Bytes&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/content">Content</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/category/genai"><span itemprop="name">GenAI</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Decode Gen AI Jargons with me</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Decode Gen AI Jargons with me</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>As a Data Engineer, I have worked with the data science team for integrating LLM within our data pipelines, that&#x27;s
when I thought it would be interesting to learn more about the Generative AI world. So I started exploring the Generative AI
world with Hitesh Choudhary&#x27;s <a href="https://hitesh.ai/cohort" target="_blank" rel="noopener noreferrer">gen-ai cohort</a>, if the course excites you,
you can use this <a href="https://courses.chaicode.com/learn/fast-checkout/227321?priceId=0&amp;code=VIBHAVAR51981&amp;is_affiliate=true&amp;tc=VIBHAVAR51981" target="_blank" rel="noopener noreferrer">link</a> to get a discount.
I will be sharing my learnings and experiences in this blog. Are you ready to decode the Gen AI jargons with me? Let&#x27;s get started!</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-generative-ai">What is Generative AI?<a href="#what-is-generative-ai" class="hash-link" aria-label="Direct link to What is Generative AI?" title="Direct link to What is Generative AI?">​</a></h2>
<p>According to the <a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence" target="_blank" rel="noopener noreferrer">Wikipedia</a>, Generative artificial intelligence (Generative AI, GenAI or GAI)
is a subset of artificial intelligence that uses <span style="background-color:#3e6980;border-radius:2px;color:#fff;padding:0.2rem">generative models</span> to produce text, images, videos, or other forms of data.
These models <span style="background-color:#3e6980;border-radius:2px;color:#fff;padding:0.2rem">learn the underlying patterns and structures of their training data</span> and use them to produce new data based on the input,
which often comes in the form of natural language prompts.</p>
<p>Due to its ability to generate human-like text, images, videos and automating tasks using Agentic Workflows, Generative
AI has gained significant attention and popularity in recent years. It has the potential to revolutionize various industries
like healthcare, finance, retail, education etc.</p>
<p>Common examples of Gen AI are ChatGPT, DALL-E, Claude etc are some of the most popular generative AI tools.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-gpts-generative-pre-trained-transformers">Understanding GPTs (Generative Pre-trained Transformers)<a href="#understanding-gpts-generative-pre-trained-transformers" class="hash-link" aria-label="Direct link to Understanding GPTs (Generative Pre-trained Transformers)" title="Direct link to Understanding GPTs (Generative Pre-trained Transformers)">​</a></h2>
<ul>
<li><strong>Generative</strong>: Refers to the model&#x27;s ability to generate new content, such as text, images, or other data.</li>
<li><strong>Pre-trained</strong>: Indicates that the model has been trained on a large dataset before being fine-tuned for specific tasks.</li>
<li><strong>Transformers</strong>: Refers to the underlying architecture of the model, which is designed to process and generate sequences of data efficiently.</li>
</ul>
<p>GPTs are first created by <code>OpenAI</code> and are based on the Transformer architecture. They are designed to understand and generate
human-like text by learning from vast amounts of data. The <strong>pre-trained</strong> aspect means that these models are initially trained on a broad dataset,
allowing them to learn grammar, facts, and some reasoning abilities before being fine-tuned for specific tasks.</p>
<p>GPTs are a family of neural network models that uses the transformer architecture and is a key advancement in artificial intelligence (AI)
powering generative AI applications such as ChatGPT.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-gpt-important">Why is GPT important?<a href="#why-is-gpt-important" class="hash-link" aria-label="Direct link to Why is GPT important?" title="Direct link to Why is GPT important?">​</a></h3>
<p>GPT models, especially their transformer architecture, are a big step forward in AI.
They make tasks like translating languages, summarizing documents, writing blogs, creating websites, and designing visuals much faster and easier.
For example, writing and editing a complex article might take hours, but a GPT model can do it in seconds.
These models help organizations save time, boost productivity, and improve their applications and customer experiences.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="breaking-down-transformers">Breaking Down Transformers<a href="#breaking-down-transformers" class="hash-link" aria-label="Direct link to Breaking Down Transformers" title="Direct link to Breaking Down Transformers">​</a></h2>
<p>Transformers are a type of neural network that learns contextual relationships between words in a sentence. Transformer models
apply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant
data elements in a series influence and depend on each other.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tokenizers-breaking-down-text">Tokenizers: Breaking Down Text<a href="#tokenizers-breaking-down-text" class="hash-link" aria-label="Direct link to Tokenizers: Breaking Down Text" title="Direct link to Tokenizers: Breaking Down Text">​</a></h3>
<p><strong>Role:</strong>
Converts your text into smaller pieces (tokens). Tokens can be whole words, parts of words, or punctuation marks.</p>
<p><strong>Example:</strong>
Original Text: &quot;Hello, world!&quot;
Tokens: [“Hello”, “,”, “world”, “!”]</p>
<p><strong>Why It Matters:</strong>
Tokenization makes text manageable and standardized for the AI to process.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="encoders-and-decoders-the-building-blocks">Encoders and Decoders: The Building Blocks<a href="#encoders-and-decoders-the-building-blocks" class="hash-link" aria-label="Direct link to Encoders and Decoders: The Building Blocks" title="Direct link to Encoders and Decoders: The Building Blocks">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="encoders">Encoders:<a href="#encoders" class="hash-link" aria-label="Direct link to Encoders:" title="Direct link to Encoders:">​</a></h4>
<p>Encoders take these tokens (represented numerically after tokenization) and process them into a meaningful internal representation that captures context.</p>
<p>After tokenization, tokens are converted into numbers (embeddings). The encoder then uses these embeddings to capture deeper meanings and relationships.</p>
<p><strong>Example:</strong>
If tokenization is about identifying puzzle pieces, encoding is like fitting these puzzle pieces together to understand what the picture represents.</p>
<p><img decoding="async" loading="lazy" alt="tokeniser" src="/assets/images/tiktokeniser-ab43287fe8c43ec3fbdbbcb0794c11a9.png" width="2412" height="1436" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoders">Decoders:<a href="#decoders" class="hash-link" aria-label="Direct link to Decoders:" title="Direct link to Decoders:">​</a></h4>
<p>Decoders take the encoded internal representation from the encoder (or from itself, if it&#x27;s generating text token-by-token) and produce new text tokens as output.</p>
<p><strong>Example:</strong>
If encoding puts the puzzle together, decoding is like explaining or describing the picture in simple language.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-attention-how-transformers-think">Self-Attention: How Transformers &quot;Think&quot;<a href="#self-attention-how-transformers-think" class="hash-link" aria-label="Direct link to Self-Attention: How Transformers &quot;Think&quot;" title="Direct link to Self-Attention: How Transformers &quot;Think&quot;">​</a></h3>
<p>Self-attention is like reading a sentence and deciding how much attention to pay to each word based on their relevance to others.</p>
<p>Imagine you&#x27;re reading, &quot;The cat sat on the mat because it was tired.&quot; Here, &quot;it&quot; refers to the &quot;cat.&quot;
Self-attention helps the model understand this by relating &quot;it&quot; back to &quot;cat.&quot;
Transformers use self-attention to weigh each word differently, focusing more on relevant words to accurately grasp the context and meaning.</p>
<p>An interesting article I found on self-attention is <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a>. Take a look at it if you want to understand the self-attention mechanism in detail.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-head-attention-ais-multitasking-ability">Multi-Head Attention: AI&#x27;s Multitasking Ability<a href="#multi-head-attention-ais-multitasking-ability" class="hash-link" aria-label="Direct link to Multi-Head Attention: AI&#x27;s Multitasking Ability" title="Direct link to Multi-Head Attention: AI&#x27;s Multitasking Ability">​</a></h3>
<p>Multi-head attention allows transformers to simultaneously focus on different aspects of the input sequence.
Imagine having multiple specialists each analyzing different parts of the same text at once—this collective analysis results
in a richer understanding and improved performance of the AI model.</p>
<p>For example, when analyzing a sentence like &quot;She bought apples from the store after finishing her work,&quot;
one attention head might focus on identifying the <code>action (buying)</code>, another on the <code>objects involved (apples, store)</code>,
and another on the <code>sequence of events (after finishing work)</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="positional-encoding-teaching-ai-about-order">Positional Encoding: Teaching AI About Order<a href="#positional-encoding-teaching-ai-about-order" class="hash-link" aria-label="Direct link to Positional Encoding: Teaching AI About Order" title="Direct link to Positional Encoding: Teaching AI About Order">​</a></h3>
<p>Positional encoding helps transformers understand the <span style="background-color:#3e6980;border-radius:2px;color:#fff;padding:0.2rem">order of words in a sentence</span>.
Since transformers don&#x27;t inherently understand sequences, positional encoding adds information about word positions.
For example, &quot;The dog chased the cat&quot; versus &quot;<code>The cat chased the dog</code>&quot; conveys entirely different meanings due to word order.
Positional encoding ensures that the transformer recognizes these differences.</p>
<p>Another example is <code>He arrived before the meeting</code> versus <code>Before the meeting, he arrived,</code> where positional encoding helps
the model correctly interpret timing and sequence relationships.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-generative-ai-understands-language">How Generative AI Understands Language?<a href="#how-generative-ai-understands-language" class="hash-link" aria-label="Direct link to How Generative AI Understands Language?" title="Direct link to How Generative AI Understands Language?">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vector-embeddings-turning-words-into-numbers">Vector Embeddings: Turning Words into Numbers<a href="#vector-embeddings-turning-words-into-numbers" class="hash-link" aria-label="Direct link to Vector Embeddings: Turning Words into Numbers" title="Direct link to Vector Embeddings: Turning Words into Numbers">​</a></h3>
<p>Embeddings convert tokens into numeric vectors, capturing semantic meaning and relationships.
Words with similar meanings or contexts are placed closer together in numerical space, enhancing the AI&#x27;s understanding
of language nuances.</p>
<p>For instance, words like &quot;happy&quot; and &quot;love&quot; have similar embeddings and are located closer together compared to words
like &quot;sad&quot; or &quot;angry,&quot; which would be positioned further away. <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="semantic-mapping-finding-meaning-in-context">Semantic Mapping: Finding Meaning in Context<a href="#semantic-mapping-finding-meaning-in-context" class="hash-link" aria-label="Direct link to Semantic Mapping: Finding Meaning in Context" title="Direct link to Semantic Mapping: Finding Meaning in Context">​</a></h3>
<p>Semantic mapping organizes embeddings into meaningful structures based on context, helping AI models grasp subtle differences in meanings.
For example, &quot;bank&quot; in the context of finance (&quot;She deposited money at the bank.&quot;) versus &quot;bank&quot; referring to a riverbank (&quot;He sat on the bank of the river.&quot;)
are clearly distinguished through semantic mapping. Another example is distinguishing &quot;apple&quot; as a fruit from &quot;Apple&quot; as
a technology company, based on the context provided.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-generative-ai-actually-works-step-by-step-interaction">How Generative AI Actually Works (Step-by-Step Interaction)<a href="#how-generative-ai-actually-works-step-by-step-interaction" class="hash-link" aria-label="Direct link to How Generative AI Actually Works (Step-by-Step Interaction)" title="Direct link to How Generative AI Actually Works (Step-by-Step Interaction)">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-user-query-submission">Step 1: User Query Submission<a href="#step-1-user-query-submission" class="hash-link" aria-label="Direct link to Step 1: User Query Submission" title="Direct link to Step 1: User Query Submission">​</a></h3>
<p>A user submits a prompt or query to the AI model. For example:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">what is artificial intelligence?</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-tokenization">Step 2: Tokenization<a href="#step-2-tokenization" class="hash-link" aria-label="Direct link to Step 2: Tokenization" title="Direct link to Step 2: Tokenization">​</a></h3>
<p>The input text is broken down into tokens.
For example, the sentence &quot;what is artificial intelligence?&quot; might be tokenized into [&quot;what&quot;, &quot;is&quot;, &quot;artificial&quot;, &quot;intelligence&quot;,&quot;?&quot;] to simplify further processing.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;what is artificial intelligence?&quot; -&gt; [&quot;what&quot;, &quot;is&quot;, &quot;artificial&quot;, &quot;intelligence&quot;,&quot;?&quot;]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-creating-embeddings">Step 3: Creating Embeddings<a href="#step-3-creating-embeddings" class="hash-link" aria-label="Direct link to Step 3: Creating Embeddings" title="Direct link to Step 3: Creating Embeddings">​</a></h3>
<p>Each token is converted into numeric embeddings capturing semantic meanings, such as placing &quot;artificial&quot; and &quot;intelligence&quot;
close together due to their frequent contextual relationship.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-positional-encoding">Step 4: Positional Encoding<a href="#step-4-positional-encoding" class="hash-link" aria-label="Direct link to Step 4: Positional Encoding" title="Direct link to Step 4: Positional Encoding">​</a></h3>
<p>The model incorporates positional information to understand word order and context.
For example, in the sentence <code>The cat chased the mouse,</code> positional encoding ensures the model understands the sequence
and can differentiate it from <code>The mouse chased the cat.</code></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5-transformer-layers-and-self-attention">Step 5: Transformer Layers and Self-Attention<a href="#step-5-transformer-layers-and-self-attention" class="hash-link" aria-label="Direct link to Step 5: Transformer Layers and Self-Attention" title="Direct link to Step 5: Transformer Layers and Self-Attention">​</a></h3>
<p>Transformer layers apply self-attention to evaluate and weigh the importance of each word concerning others in the sentence,
helping the model grasp the overall sentiment and context.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-6-response-generation-with-multi-head-attention">Step 6: Response Generation with Multi-Head Attention<a href="#step-6-response-generation-with-multi-head-attention" class="hash-link" aria-label="Direct link to Step 6: Response Generation with Multi-Head Attention" title="Direct link to Step 6: Response Generation with Multi-Head Attention">​</a></h3>
<p>Multiple attention heads simultaneously analyze different aspects of the context—such as emotional tone (love), the subject matter (artificial intelligence)—to create a nuanced response.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-7-adjusting-ai-creativity-with-temperature">Step 7: Adjusting AI Creativity with Temperature<a href="#step-7-adjusting-ai-creativity-with-temperature" class="hash-link" aria-label="Direct link to Step 7: Adjusting AI Creativity with Temperature" title="Direct link to Step 7: Adjusting AI Creativity with Temperature">​</a></h3>
<p>The &quot;temperature&quot; parameter controls the randomness or creativity of the AI&#x27;s responses.
Lower temperatures yield predictable results, while higher temperatures produce more creative outputs.</p>
<p>Think like a chef adjusting the spice level in a dish. A low temperature (less spice) results in a safe, familiar flavor,
while a high temperature (more spice) leads to unexpected and exciting combinations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-8-feedforward-neural-network">Step 8: Feedforward Neural Network<a href="#step-8-feedforward-neural-network" class="hash-link" aria-label="Direct link to Step 8: Feedforward Neural Network" title="Direct link to Step 8: Feedforward Neural Network">​</a></h3>
<p>The model processes the output from the transformer layers through a feedforward neural network, refining the response further.
This network applies non-linear transformations to the data, enhancing the model&#x27;s ability to understand complex relationships.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="additional-concepts-you-should-know">Additional Concepts You Should Know<a href="#additional-concepts-you-should-know" class="hash-link" aria-label="Direct link to Additional Concepts You Should Know" title="Direct link to Additional Concepts You Should Know">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="knowledge-cutoff-why-ai-doesnt-know-everything">Knowledge Cutoff: Why AI Doesn’t Know Everything<a href="#knowledge-cutoff-why-ai-doesnt-know-everything" class="hash-link" aria-label="Direct link to Knowledge Cutoff: Why AI Doesn’t Know Everything" title="Direct link to Knowledge Cutoff: Why AI Doesn’t Know Everything">​</a></h3>
<p>Knowledge cutoff represents the date up to which the AI model was trained.
Information after this cutoff isn&#x27;t available to the model, limiting its current knowledge.</p>
<p>If you liked reading this blog, I would love to hear your thoughts, feel free to contact me on below social platform :)</p>
<div class="social-container"><a href="https://github.com/vibhabellutagi19" target="_blank" rel="noopener noreferrer" class="social-link"><span class="social-icon github"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></span></a><a href="https://x.com/buildwith_vibs" target="_blank" rel="noopener noreferrer" class="social-link"><span class="social-icon twitter"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></span></a><a href="https://www.linkedin.com/in/vibhavari-bellutagi-837871189/" target="_blank" rel="noopener noreferrer" class="social-link"><span class="social-icon linkedin"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></span></a><a href="https://www.instagram.com/buildwith_vibs/" target="_blank" rel="noopener noreferrer" class="social-link"><span class="social-icon instagram"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"></path></svg></span></a></div>
<hr>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_LWe7 sr-only" id="footnote-label">Footnotes<a href="#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1">
<p><a href="https://projector.tensorflow.org/" target="_blank" rel="noopener noreferrer">Vector-Space</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/category/genai"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">GenAI</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/category/tech-bytes"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Tech Bytes</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#what-is-generative-ai" class="table-of-contents__link toc-highlight">What is Generative AI?</a></li><li><a href="#understanding-gpts-generative-pre-trained-transformers" class="table-of-contents__link toc-highlight">Understanding GPTs (Generative Pre-trained Transformers)</a><ul><li><a href="#why-is-gpt-important" class="table-of-contents__link toc-highlight">Why is GPT important?</a></li></ul></li><li><a href="#breaking-down-transformers" class="table-of-contents__link toc-highlight">Breaking Down Transformers</a><ul><li><a href="#tokenizers-breaking-down-text" class="table-of-contents__link toc-highlight">Tokenizers: Breaking Down Text</a></li><li><a href="#encoders-and-decoders-the-building-blocks" class="table-of-contents__link toc-highlight">Encoders and Decoders: The Building Blocks</a></li><li><a href="#self-attention-how-transformers-think" class="table-of-contents__link toc-highlight">Self-Attention: How Transformers &quot;Think&quot;</a></li><li><a href="#multi-head-attention-ais-multitasking-ability" class="table-of-contents__link toc-highlight">Multi-Head Attention: AI&#39;s Multitasking Ability</a></li><li><a href="#positional-encoding-teaching-ai-about-order" class="table-of-contents__link toc-highlight">Positional Encoding: Teaching AI About Order</a></li></ul></li><li><a href="#how-generative-ai-understands-language" class="table-of-contents__link toc-highlight">How Generative AI Understands Language?</a><ul><li><a href="#vector-embeddings-turning-words-into-numbers" class="table-of-contents__link toc-highlight">Vector Embeddings: Turning Words into Numbers</a></li><li><a href="#semantic-mapping-finding-meaning-in-context" class="table-of-contents__link toc-highlight">Semantic Mapping: Finding Meaning in Context</a></li></ul></li><li><a href="#how-generative-ai-actually-works-step-by-step-interaction" class="table-of-contents__link toc-highlight">How Generative AI Actually Works (Step-by-Step Interaction)</a><ul><li><a href="#step-1-user-query-submission" class="table-of-contents__link toc-highlight">Step 1: User Query Submission</a></li><li><a href="#step-2-tokenization" class="table-of-contents__link toc-highlight">Step 2: Tokenization</a></li><li><a href="#step-3-creating-embeddings" class="table-of-contents__link toc-highlight">Step 3: Creating Embeddings</a></li><li><a href="#step-4-positional-encoding" class="table-of-contents__link toc-highlight">Step 4: Positional Encoding</a></li><li><a href="#step-5-transformer-layers-and-self-attention" class="table-of-contents__link toc-highlight">Step 5: Transformer Layers and Self-Attention</a></li><li><a href="#step-6-response-generation-with-multi-head-attention" class="table-of-contents__link toc-highlight">Step 6: Response Generation with Multi-Head Attention</a></li><li><a href="#step-7-adjusting-ai-creativity-with-temperature" class="table-of-contents__link toc-highlight">Step 7: Adjusting AI Creativity with Temperature</a></li><li><a href="#step-8-feedforward-neural-network" class="table-of-contents__link toc-highlight">Step 8: Feedforward Neural Network</a></li></ul></li><li><a href="#additional-concepts-you-should-know" class="table-of-contents__link toc-highlight">Additional Concepts You Should Know</a><ul><li><a href="#knowledge-cutoff-why-ai-doesnt-know-everything" class="table-of-contents__link toc-highlight">Knowledge Cutoff: Why AI Doesn’t Know Everything</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About Me</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Resume/experience">Resume</a></li></ul></div><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/content">Content</a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:vibhavari.bellutagi@gmail.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mail</a></li><li class="footer__item"><a href="https://github.com/vibhabellutagi19/vibhabellutagi19.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/vibhavari-bellutagi/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/buildwith_vibs" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Vibhavari Bellutagi, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>